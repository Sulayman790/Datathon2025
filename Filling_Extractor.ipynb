{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "install-packages",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T22:26:42.020575Z",
     "iopub.status.busy": "2025-11-01T22:26:42.020312Z",
     "iopub.status.idle": "2025-11-01T22:26:43.682746Z",
     "shell.execute_reply": "2025-11-01T22:26:43.681692Z",
     "shell.execute_reply.started": "2025-11-01T22:26:42.020557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sec-parser in /opt/conda/lib/python3.11/site-packages (0.58.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (4.14.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: cssutils<3.0.0,>=2.11.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (2.11.1)\n",
      "Requirement already satisfied: frozendict<3.0.0,>=2.4.4 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (2.4.6)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (0.7.3)\n",
      "Requirement already satisfied: lxml<6.0.0,>=5.2.2 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (5.4.0)\n",
      "Requirement already satisfied: sec-downloader<0.12.0,>=0.11.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (0.11.2)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (0.9.0)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.4.1 in /opt/conda/lib/python3.11/site-packages (from sec-parser) (3.6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.11/site-packages (from cssutils<3.0.0,>=2.11.1->sec-parser) (10.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sec-edgar-downloader in /opt/conda/lib/python3.11/site-packages (from sec-downloader<0.12.0,>=0.11.1->sec-parser) (5.0.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (2.32.5)\n",
      "Requirement already satisfied: pyrate-limiter>=3.6.0 in /opt/conda/lib/python3.11/site-packages (from sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (3.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sec-edgar-downloader->sec-downloader<0.12.0,>=0.11.1->sec-parser) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install sec-parser beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "execution_state": "idle",
   "id": "16e67c25-314a-442c-8874-3d7b2f7f325c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:41:48.903819Z",
     "iopub.status.busy": "2025-11-02T01:41:48.903505Z",
     "iopub.status.idle": "2025-11-02T01:41:48.956181Z",
     "shell.execute_reply": "2025-11-02T01:41:48.955351Z",
     "shell.execute_reply.started": "2025-11-02T01:41:48.903798Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json, re, time, hashlib, datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from slugify import slugify\n",
    "from rapidfuzz import fuzz, process as rprocess\n",
    "\n",
    "\n",
    "import sec_parser  \n",
    "\n",
    "# Dossiers\n",
    "FILLINGS_PATH = '/home/sagemaker-user/shared/fillings'\n",
    "CACHE_DIR     = '/home/sagemaker-user/shared/cache/sec_analysis'\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_COLUMNS = [\n",
    "    \"ticker\",\"company\",\"sector\",\"headquarters_country\",\"revenues_total_usd\",\n",
    "    \"revenue_by_region_notes\",\"region_exposure_US\",\"region_exposure_Europe\",\n",
    "    \"region_exposure_China\",\"region_exposure_India\",\"supply_chain_regions\",\n",
    "    \"key_suppliers\",\"key_customers\",\"critical_dependencies\",\"regulatory_dependencies\",\n",
    "    \"sanctions_exposure\",\"environmental_regulatory_risk\",\"labor_regulatory_risk\",\n",
    "    \"cybersecurity_regulatory_risk\",\"ai_governance_risk\",\"overall_regulatory_risk_score\",\n",
    "    \"confidence\",\"sources\"\n",
    "]\n",
    "\n",
    "COLUMN_ALIASES = {\n",
    "    \"hq_country\": \"headquarters_country\",\n",
    "    \"risk_score\": \"overall_regulatory_risk_score\",\n",
    "    \"confidence_score\": \"confidence\",\n",
    "    \"environment_risk\": \"environmental_regulatory_risk\",\n",
    "    \"labour_risk\": \"labor_regulatory_risk\",\n",
    "    \"it_cyber_risk\": \"cybersecurity_regulatory_risk\",\n",
    "    \"ai_risk\": \"ai_governance_risk\",\n",
    "    \"dependencies\": \"critical_dependencies\",\n",
    "    \"regulatory_exposure\": \"regulatory_dependencies\",\n",
    "    \"us_exposure\": \"region_exposure_US\",\n",
    "    \"eu_exposure\": \"region_exposure_Europe\",\n",
    "    \"china_exposure\": \"region_exposure_China\",\n",
    "    \"india_exposure\": \"region_exposure_India\",\n",
    "}\n",
    "\n",
    "def _normalize_keys(d: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        out[COLUMN_ALIASES.get(k, k)] = v\n",
    "    return out\n",
    "\n",
    "def read_html_text(fp: str) -> str:\n",
    "    with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    import hashlib\n",
    "    return hashlib.sha1(s.encode('utf-8', errors='ignore')).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "execution_state": "idle",
   "id": "ac6c5a1d-a3f8-41d1-8e34-709345be061f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:41:49.635692Z",
     "iopub.status.busy": "2025-11-02T01:41:49.635270Z",
     "iopub.status.idle": "2025-11-02T01:41:49.645307Z",
     "shell.execute_reply": "2025-11-02T01:41:49.644296Z",
     "shell.execute_reply.started": "2025-11-02T01:41:49.635658Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_companies(fillings_path: str) -> List[str]:\n",
    "    return sorted([d for d in os.listdir(fillings_path) if os.path.isdir(os.path.join(fillings_path, d))])\n",
    "\n",
    "def list_html_files(company_dir: str) -> List[str]:\n",
    "    files = [f for f in os.listdir(company_dir) if f.lower().endswith('.html')]\n",
    "    # Heuristique: traiter d’abord les fichiers contenant 10-K\n",
    "    files = sorted(files, key=lambda x: (('10-k' not in x.lower()), x))\n",
    "    return files\n",
    "\n",
    "def parse_10k_sections(html_content: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        elements = sec_parser.parse(html_content)\n",
    "        flat = \" \".join([getattr(e, \"text\", \"\") for e in elements if getattr(e, \"text\", \"\")])\n",
    "        return {\"full_text\": flat}\n",
    "    except Exception as e:\n",
    "        # Fallback to BeautifulSoup if sec_parser fails\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        return {\"full_text\": soup.get_text(separator=' ', strip=True)}\n",
    "\n",
    "import re\n",
    "def split_items_by_regex(text: str) -> Dict[str, str]:\n",
    "    keys = {\n",
    "        \"Item 1\":  re.compile(r'\\bitem\\s*1\\b(?!\\s*a)', re.I),\n",
    "        \"Item 1A\": re.compile(r'\\bitem\\s*1a\\b', re.I),\n",
    "        \"Item 7\": re.compile(r'\\bitem\\s*7\\b', re.I),\n",
    "    }\n",
    "    # ancrages\n",
    "    hits = []\n",
    "    for k, rx in keys.items():\n",
    "        for m in rx.finditer(text):\n",
    "            hits.append((m.start(), k))\n",
    "    if not hits:\n",
    "        return {\"FULL_TEXT_FALLBACK\": text}\n",
    "    hits.sort()\n",
    "    out = {k:\"\" for k in keys}\n",
    "    for i, (pos, label) in enumerate(hits):\n",
    "        end = hits[i+1][0] if i+1 < len(hits) else len(text)\n",
    "        chunk = text[pos:end].strip()\n",
    "        if len(chunk) > len(out[label]):\n",
    "            out[label] = chunk\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_company_primary_10k(filepath: str) -> Tuple[Dict[str, str], str]:\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        html = f.read()\n",
    "    sections = parse_10k_sections(html)\n",
    "    return sections, html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "execution_state": "idle",
   "id": "6a1abcaa-b252-4d30-b027-015e98d6163f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:41:50.122330Z",
     "iopub.status.busy": "2025-11-02T01:41:50.122075Z",
     "iopub.status.idle": "2025-11-02T01:41:50.140354Z",
     "shell.execute_reply": "2025-11-02T01:41:50.139626Z",
     "shell.execute_reply.started": "2025-11-02T01:41:50.122310Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import re, os\n",
    "from slugify import slugify\n",
    "\n",
    "def _extract_text(resp: dict) -> str:\n",
    "    msg = resp[\"output\"][\"message\"]\n",
    "    pieces = msg.get(\"content\", [])\n",
    "    text = \"\"\n",
    "    for p in pieces:\n",
    "        if \"text\" in p:\n",
    "            text += p[\"text\"]\n",
    "    return text.strip()\n",
    "\n",
    "def _strip_code_fences(s: str) -> str:\n",
    "    s = re.sub(r\"^\\s*```(?:json)?\\s*\", \"\", s)\n",
    "    s = re.sub(r\"\\s*```\\s*$\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _best_json_slice(s: str) -> str:\n",
    "    start = s.find(\"{\")\n",
    "    if start == -1:\n",
    "        return s\n",
    "    stack = 0\n",
    "    end = -1\n",
    "    for i, ch in enumerate(s[start:], start=start):\n",
    "        if ch == \"{\": stack += 1\n",
    "        elif ch == \"}\":\n",
    "            stack -= 1\n",
    "            if stack == 0:\n",
    "                end = i\n",
    "                break\n",
    "    return s[start:end+1] if end != -1 else s[start:]\n",
    "\n",
    "def _sanitize_json_text(s: str) -> str:\n",
    "    s = _strip_code_fences(s)\n",
    "    s = _best_json_slice(s)\n",
    "    s = s.replace(\"“\",\"\\\"\").replace(\"”\",\"\\\"\").replace(\"’\",\"'\")\n",
    "    s = s.replace(\"`\",\"\")\n",
    "    s = re.sub(r\",\\s*([\\]}])\", r\"\\1\", s)                       # virgules traînantes\n",
    "    s = s.replace(\"None\",\"null\").replace(\"True\",\"true\").replace(\"False\",\"false\")\n",
    "    s = re.sub(r\"\\bNaN\\b\",\"null\", s); s = re.sub(r\"\\bInfinity\\b\",\"null\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\b-?inf(?:inity)?\\b\",\"null\", s, flags=re.I)\n",
    "    return s.strip()\n",
    "\n",
    "# Configure si besoin (SageMaker a souvent les rôles configurés)\n",
    "BEDROCK_REGION = os.environ.get(\"AWS_REGION\", \"us-west-2\")\n",
    "MODEL_ID       = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"  # ajuste si nécessaire\n",
    "\n",
    "brt = boto3.client(\"bedrock-runtime\", region_name=BEDROCK_REGION)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a senior financial analyst.\n",
    "From 10-K sections (Item 1 Business, Item 1A Risk Factors, Item 7 MD&A),\n",
    "produce a compact JSON with this schema (text or numeric fields, use Unknown if unknown):\n",
    "\n",
    "{\n",
    "  \"ticker\": \"...\",\n",
    "  \"company\": \"...\",\n",
    "  \"sector\": \"GICS or inferred if present\",\n",
    "  \"headquarters_country\": \"...\",\n",
    "  \"revenues_total_usd\": 1234567890,\n",
    "  \"revenue_by_region_notes\": \"Brief text citing regions/shares if available\",\n",
    "  \"region_exposure_US\": \"low/medium/high\",\n",
    "  \"region_exposure_Europe\": \"...\",\n",
    "  \"region_exposure_China\": \"...\",\n",
    "  \"region_exposure_India\": \"...\",\n",
    "  \"supply_chain_regions\": [\"list of key regions/countries if mentioned\"],\n",
    "  \"key_suppliers\": [\"...\"],\n",
    "  \"key_customers\": [\"...\"],\n",
    "  \"critical_dependencies\": [\"e.g., semiconductors, cloud, rare earths, data centers, lithium...\"],\n",
    "  \"regulatory_dependencies\": [\"e.g., GDPR, CSRD, IRA, export controls, data localization...\"],\n",
    "  \"sanctions_exposure\": \"short text if mentioned (countries/sectors)\",\n",
    "  \"environmental_regulatory_risk\": \"low/medium/high\",\n",
    "  \"labor_regulatory_risk\": \"low/medium/high\",\n",
    "  \"cybersecurity_regulatory_risk\": \"low/medium/high\",\n",
    "  \"ai_governance_risk\": \"low/medium/high\",\n",
    "  \"overall_regulatory_risk_score\": 0-100,\n",
    "  \"confidence\": 0-1,\n",
    "  \"sources\": [\"Item 1\", \"Item 1A\", \"Item 7\", ...]\n",
    "}\n",
    "\n",
    "Remember: be factual; cite only what is discernible in the provided text.\n",
    "Respond STRICTLY with valid JSON—no prose.\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(tkr: str, company: str, snippets: Dict[str, str]) -> str:\n",
    "    parts = [f\"TICKER: {tkr}\", f\"COMPANY: {company}\", \"EXTRACTS:\"]\n",
    "    for k, v in snippets.items():\n",
    "        if not v: \n",
    "            continue\n",
    "        # Tronquer au besoin pour contenir le contexte\n",
    "        vv = v[:20000]  # garde large, ajuste si tu as des erreurs de tokens\n",
    "        parts.append(f\"\\n=== {k} ===\\n{vv}\\n\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "@retry(\n",
    "    reraise=True,\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=20),\n",
    "    retry=retry_if_exception_type(Exception)\n",
    ")\n",
    "\n",
    "@retry(reraise=True, stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=20),\n",
    "       retry=retry_if_exception_type(Exception))\n",
    "def call_bedrock_json(prompt: str, debug_tag: str = \"unknown\") -> Dict[str, Any]:\n",
    "    response = brt.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=[{\"role\":\"user\",\"content\":[{\"text\":prompt}]}],\n",
    "        system=[{\"text\": SYSTEM_PROMPT + \"\\n\\nRègle de sortie: réponds UNIQUEMENT par un JSON valide MINIFIÉ, sans markdown.\"}],\n",
    "        inferenceConfig={\"maxTokens\": 2000, \"temperature\": 0.2, \"topP\": 0.9}\n",
    "    )\n",
    "    txt = _extract_text(response)\n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except Exception:\n",
    "        sani = _sanitize_json_text(txt)\n",
    "        try:\n",
    "            return json.loads(sani)\n",
    "        except Exception:\n",
    "            # sauvegarde pour debug\n",
    "            raw1 = os.path.join(CACHE_DIR, f\"raw_{slugify(debug_tag)}.txt\")\n",
    "            with open(raw1, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(txt)\n",
    "            # 2e tour: on redemande la même réponse, au format JSON strict\n",
    "            response2 = brt.converse(\n",
    "                modelId=MODEL_ID,\n",
    "                messages=[\n",
    "                    {\"role\":\"user\",\"content\":[{\"text\":prompt}]},\n",
    "                    {\"role\":\"assistant\",\"content\":[{\"text\":txt[:4000]}]},\n",
    "                    {\"role\":\"user\",\"content\":[{\"text\":\"Reformate exactement la même réponse en JSON valide MINIFIÉ, une seule ligne, sans texte autour.\"}]},\n",
    "                ],\n",
    "                system=[{\"text\": SYSTEM_PROMPT}],\n",
    "                inferenceConfig={\"maxTokens\": 5000, \"temperature\": 0.1, \"topP\": 0.9}\n",
    "            )\n",
    "            txt2 = _extract_text(response2)\n",
    "            try:\n",
    "                return json.loads(txt2)\n",
    "            except Exception:\n",
    "                sani2 = _sanitize_json_text(txt2)\n",
    "                return json.loads(sani2)  # laisser lever si encore invalide\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "execution_state": "idle",
   "id": "d42def65-84ed-4951-bf33-614ec384a93c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:41:50.566803Z",
     "iopub.status.busy": "2025-11-02T01:41:50.566257Z",
     "iopub.status.idle": "2025-11-02T01:41:50.580588Z",
     "shell.execute_reply": "2025-11-02T01:41:50.579530Z",
     "shell.execute_reply.started": "2025-11-02T01:41:50.566774Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer_company_ticker_from_filename(company_dir: str, file: str) -> Tuple[str,str]:\n",
    "    \"\"\"\n",
    "    Essaie de déduire (ticker, company) à partir du chemin.\n",
    "    - company_dir est le nom de dossier (souvent le ticker, mais pas toujours).\n",
    "    - le nom de fichier peut contenir la date-form-...-ticker.html\n",
    "    \"\"\"\n",
    "    tkr = company_dir\n",
    "    comp = company_dir\n",
    "    base = file.replace('.html','')\n",
    "    parts = base.split('-')\n",
    "    if len(parts) >= 5:\n",
    "        tkr = parts[4] or tkr\n",
    "    return tkr, comp\n",
    "\n",
    "def cache_path_for(company: str, ticker: str, filename: str) -> str:\n",
    "    key = f\"{company}|{ticker}|{filename}\"\n",
    "    return os.path.join(CACHE_DIR, f\"{slugify(company)}_{slugify(ticker)}_{sha1(key)}.json\")\n",
    "\n",
    "def normalize_record(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    rec = _normalize_keys(rec)\n",
    "    # forcer toutes les colonnes attendues, remplir par NA\n",
    "    out = {c: rec.get(c, None) for c in OUTPUT_COLUMNS}\n",
    "    # normalisations simples\n",
    "    if out.get(\"overall_regulatory_risk_score\") is not None:\n",
    "        try:\n",
    "            out[\"overall_regulatory_risk_score\"] = float(out[\"overall_regulatory_risk_score\"])\n",
    "        except:\n",
    "            pass\n",
    "    if out.get(\"confidence\") is not None:\n",
    "        try:\n",
    "            out[\"confidence\"] = float(out[\"confidence\"])\n",
    "        except:\n",
    "            pass\n",
    "    # sources liste -> string join pour CSV\n",
    "    if isinstance(out.get(\"sources\"), list):\n",
    "        out[\"sources\"] = \"; \".join(out[\"sources\"])\n",
    "    # listes -> string\n",
    "    for listy in [\"supply_chain_regions\",\"key_suppliers\",\"key_customers\",\"critical_dependencies\",\"regulatory_dependencies\"]:\n",
    "        if isinstance(out.get(listy), list):\n",
    "            out[listy] = \"; \".join([str(x) for x in out[listy]])\n",
    "    return out\n",
    "\n",
    "def process_company_dir(base_dir: str, company: str) -> Optional[Dict[str, Any]]:\n",
    "    company_path = os.path.join(base_dir, company)\n",
    "    files = list_html_files(company_path)\n",
    "    if not files:\n",
    "        return None\n",
    "    # On traite le premier 10-K prioritaire\n",
    "    file = files[0]\n",
    "    fp   = os.path.join(company_path, file)\n",
    "    ticker, comp_guess = infer_company_ticker_from_filename(company, file)\n",
    "\n",
    "    cpath = cache_path_for(company, ticker, file)\n",
    "    if os.path.exists(cpath):\n",
    "        with open(cpath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # Charger et parser\n",
    "    with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        html_content = f.read()\n",
    "    sections = parse_10k_sections(html_content)\n",
    "    # Si pas de sections, fallback texte brut (déjà fait dans parse_10k_sections)\n",
    "\n",
    "    user_prompt = build_user_prompt(ticker, company, sections)\n",
    "    try:\n",
    "        result = call_bedrock_json(user_prompt, debug_tag=f\"{ticker}_{company}\")\n",
    "    except Exception:\n",
    "        # dernier recours: snippet business ou full\n",
    "        snippet = sections.get(\"Item 1 - Business\") or sections.get(\"FULL_TEXT_FALLBACK\", \"\")\n",
    "        try:\n",
    "            result = call_bedrock_json(\n",
    "                build_user_prompt(ticker, company, {\"Item 1 - Business\": snippet[:15000]}),\n",
    "                debug_tag=f\"{ticker}_{company}_fallback\"\n",
    "            )\n",
    "        except Exception:\n",
    "            # ⚠️ NE PAS PERDRE LA LIGNE — on écrit une sentinelle\n",
    "            fail_row = {c: None for c in OUTPUT_COLUMNS}\n",
    "            fail_row.update({\n",
    "                \"ticker\": ticker,\n",
    "                \"company\": company,\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": \"PARSE_OR_JSON_ERROR\"\n",
    "            })\n",
    "            with open(cpath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(fail_row, f, ensure_ascii=False, indent=2)\n",
    "            return fail_row\n",
    "\n",
    "    record = normalize_record(result)\n",
    "    # Remplissage minimal si Claude n'a pas renvoyé ticker/company\n",
    "    record[\"ticker\"]  = record.get(\"ticker\")  or ticker\n",
    "    record[\"company\"] = record.get(\"company\") or company\n",
    "\n",
    "    with open(cpath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(record, f, ensure_ascii=False, indent=2)\n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "execution_state": "idle",
   "id": "46af5f2c-9e53-42fd-baae-5e93988bafd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:41:51.072200Z",
     "iopub.status.busy": "2025-11-02T01:41:51.071939Z",
     "iopub.status.idle": "2025-11-02T03:00:44.641765Z",
     "shell.execute_reply": "2025-11-02T03:00:44.637504Z",
     "shell.execute_reply.started": "2025-11-02T01:41:51.072180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compagnies détectées: 500\n",
      "Exemples: ['A', 'AAPL', 'ABBV', 'ABNB', 'ABT', 'ACGL', 'ACN', 'ADBE', 'ADI', 'ADM']\n",
      "[1/500] A -> OK\n",
      "[2/500] AAPL -> OK\n",
      "[3/500] ABBV -> OK\n",
      "[4/500] ABNB -> OK\n",
      "[5/500] ABT -> OK\n",
      "[6/500] ACGL -> OK\n",
      "[7/500] ACN -> OK\n",
      "[8/500] ADBE -> OK\n",
      "[9/500] ADI -> OK\n",
      "[10/500] ADM -> OK\n",
      "[11/500] ADP -> OK\n",
      "[12/500] ADSK -> OK\n",
      "[13/500] AEE -> OK\n",
      "[14/500] AEP -> OK\n",
      "[15/500] AES -> OK\n",
      "[16/500] AFL -> OK\n",
      "[17/500] AIG -> OK\n",
      "[18/500] AIZ -> OK\n",
      "[19/500] AJG -> OK\n",
      "[20/500] AKAM -> OK\n",
      "[21/500] ALB -> OK\n",
      "[22/500] ALGN -> OK\n",
      "[23/500] ALL -> OK\n",
      "[24/500] ALLE -> OK\n",
      "[25/500] AMAT -> OK\n",
      "[26/500] AMCR -> OK\n",
      "[27/500] AMD -> OK\n",
      "[28/500] AME -> OK\n",
      "[29/500] AMGN -> OK\n",
      "[30/500] AMP -> OK\n",
      "[31/500] AMT -> OK\n",
      "[32/500] AMZN -> OK\n",
      "[33/500] ANET -> OK\n",
      "[34/500] AON -> OK\n",
      "[35/500] AOS -> OK\n",
      "[36/500] APA -> OK\n",
      "[37/500] APD -> OK\n",
      "[38/500] APH -> OK\n",
      "[39/500] APO -> OK\n",
      "[40/500] APTV -> OK\n",
      "[41/500] ARE -> OK\n",
      "[42/500] ATO -> OK\n",
      "[43/500] AVB -> OK\n",
      "[44/500] AVGO -> OK\n",
      "[45/500] AVY -> OK\n",
      "[46/500] AWK -> OK\n",
      "[47/500] AXON -> OK\n",
      "[48/500] AXP -> OK\n",
      "[49/500] AZO -> OK\n",
      "[50/500] BA -> OK\n",
      "[51/500] BAC -> OK\n",
      "[52/500] BALL -> OK\n",
      "[53/500] BAX -> OK\n",
      "[54/500] BBY -> OK\n",
      "[55/500] BDX -> OK\n",
      "[56/500] BEN -> OK\n",
      "[57/500] BG -> OK\n",
      "[58/500] BIIB -> OK\n",
      "[59/500] BK -> OK\n",
      "[60/500] BKNG -> OK\n",
      "[61/500] BKR -> OK\n",
      "[62/500] BLDR -> OK\n",
      "[63/500] BLK -> OK\n",
      "[64/500] BMY -> OK\n",
      "[65/500] BR -> OK\n",
      "[66/500] BRO -> OK\n",
      "[67/500] BSX -> OK\n",
      "[68/500] BX -> OK\n",
      "[69/500] BXP -> OK\n",
      "[70/500] C -> OK\n",
      "[71/500] CAG -> OK\n",
      "[72/500] CAH -> OK\n",
      "[73/500] CARR -> OK\n",
      "[74/500] CAT -> OK\n",
      "[75/500] CB -> OK\n",
      "[76/500] CBOE -> OK\n",
      "[77/500] CBRE -> OK\n",
      "[78/500] CCI -> OK\n",
      "[79/500] CCL -> OK\n",
      "[80/500] CDNS -> OK\n",
      "[81/500] CDW -> OK\n",
      "[82/500] CEG -> OK\n",
      "[83/500] CF -> OK\n",
      "[84/500] CFG -> OK\n",
      "[85/500] CHD -> OK\n",
      "[86/500] CHRW -> OK\n",
      "[87/500] CHTR -> OK\n",
      "[88/500] CI -> OK\n",
      "[89/500] CINF -> OK\n",
      "[90/500] CL -> OK\n",
      "[91/500] CLX -> OK\n",
      "[92/500] CMCSA -> OK\n",
      "[93/500] CME -> OK\n",
      "[94/500] CMG -> OK\n",
      "[95/500] CMI -> OK\n",
      "[96/500] CMS -> OK\n",
      "[97/500] CNC -> OK\n",
      "[98/500] CNP -> OK\n",
      "[99/500] COF -> OK\n",
      "[100/500] COIN -> OK\n",
      "[101/500] COO -> OK\n",
      "[102/500] COP -> OK\n",
      "[103/500] COR -> OK\n",
      "[104/500] COST -> OK\n",
      "[105/500] CPAY -> OK\n",
      "[106/500] CPB -> OK\n",
      "[107/500] CPRT -> OK\n",
      "[108/500] CPT -> OK\n",
      "[109/500] CRL -> OK\n",
      "[110/500] CRM -> OK\n",
      "[111/500] CRWD -> OK\n",
      "[112/500] CSCO -> OK\n",
      "[113/500] CSGP -> OK\n",
      "[114/500] CSX -> OK\n",
      "[115/500] CTAS -> OK\n",
      "[116/500] CTRA -> OK\n",
      "[117/500] CTSH -> OK\n",
      "[118/500] CTVA -> OK\n",
      "[119/500] CVS -> OK\n",
      "[120/500] CVX -> OK\n",
      "[121/500] CZR -> OK\n",
      "[122/500] D -> OK\n",
      "[123/500] DAL -> OK\n",
      "[124/500] DASH -> OK\n",
      "[125/500] DAY -> OK\n",
      "[126/500] DD -> OK\n",
      "[127/500] DDOG -> OK\n",
      "[128/500] DE -> OK\n",
      "[129/500] DECK -> OK\n",
      "[130/500] DELL -> OK\n",
      "[131/500] DG -> OK\n",
      "[132/500] DGX -> OK\n",
      "[133/500] DHI -> OK\n",
      "[134/500] DHR -> OK\n",
      "[135/500] DIS -> OK\n",
      "[136/500] DLR -> OK\n",
      "[137/500] DLTR -> OK\n",
      "[138/500] DOC -> OK\n",
      "[139/500] DOV -> OK\n",
      "[140/500] DOW -> OK\n",
      "[141/500] DPZ -> OK\n",
      "[142/500] DRI -> OK\n",
      "[143/500] DTE -> OK\n",
      "[144/500] DUK -> OK\n",
      "[145/500] DVA -> OK\n",
      "[146/500] DVN -> OK\n",
      "[147/500] DXCM -> OK\n",
      "[148/500] EA -> OK\n",
      "[149/500] EBAY -> OK\n",
      "[150/500] ECL -> OK\n",
      "[151/500] ED -> OK\n",
      "[152/500] EFX -> OK\n",
      "[153/500] EG -> OK\n",
      "[154/500] EIX -> OK\n",
      "[155/500] EL -> OK\n",
      "[156/500] ELV -> OK\n",
      "[157/500] EMN -> OK\n",
      "[158/500] EMR -> OK\n",
      "[159/500] ENPH -> OK\n",
      "[160/500] EOG -> OK\n",
      "[161/500] EPAM -> OK\n",
      "[162/500] EQIX -> OK\n",
      "[163/500] EQR -> OK\n",
      "[164/500] EQT -> OK\n",
      "[165/500] ERIE -> OK\n",
      "[166/500] ES -> OK\n",
      "[167/500] ESS -> OK\n",
      "[168/500] ETN -> OK\n",
      "[169/500] ETR -> OK\n",
      "[170/500] EVRG -> OK\n",
      "[171/500] EW -> OK\n",
      "[172/500] EXC -> OK\n",
      "[173/500] EXE -> OK\n",
      "[174/500] EXPD -> OK\n",
      "[175/500] EXPE -> OK\n",
      "[176/500] EXR -> OK\n",
      "[177/500] F -> OK\n",
      "[178/500] FANG -> OK\n",
      "[179/500] FAST -> OK\n",
      "[180/500] FCX -> OK\n",
      "[181/500] FDS -> OK\n",
      "[182/500] FDX -> OK\n",
      "[183/500] FE -> OK\n",
      "[184/500] FFIV -> OK\n",
      "[185/500] FI -> OK\n",
      "[186/500] FICO -> OK\n",
      "[187/500] FIS -> OK\n",
      "[188/500] FITB -> OK\n",
      "[189/500] FOX -> OK\n",
      "[190/500] FOXA -> OK\n",
      "[191/500] FRT -> OK\n",
      "[192/500] FSLR -> OK\n",
      "[193/500] FTNT -> OK\n",
      "[194/500] FTV -> OK\n",
      "[195/500] GD -> OK\n",
      "[196/500] GDDY -> OK\n",
      "[197/500] GE -> OK\n",
      "[198/500] GEHC -> OK\n",
      "[199/500] GEN -> OK\n",
      "[200/500] GEV -> OK\n",
      "[201/500] GILD -> OK\n",
      "[202/500] GIS -> OK\n",
      "[203/500] GL -> OK\n",
      "[204/500] GLW -> OK\n",
      "[205/500] GM -> OK\n",
      "[206/500] GNRC -> OK\n",
      "[207/500] GOOG -> OK\n",
      "[208/500] GOOGL -> OK\n",
      "[209/500] GPC -> OK\n",
      "[210/500] GPN -> OK\n",
      "[211/500] GRMN -> OK\n",
      "[212/500] GS -> OK\n",
      "[213/500] GWW -> OK\n",
      "[214/500] HAL -> OK\n",
      "[215/500] HAS -> OK\n",
      "[216/500] HBAN -> OK\n",
      "[217/500] HCA -> OK\n",
      "[218/500] HD -> OK\n",
      "[219/500] HIG -> OK\n",
      "[220/500] HII -> OK\n",
      "[221/500] HLT -> OK\n",
      "[222/500] HOLX -> OK\n",
      "[223/500] HON -> OK\n",
      "[224/500] HPE -> OK\n",
      "[225/500] HPQ -> OK\n",
      "[226/500] HRL -> OK\n",
      "[227/500] HSIC -> OK\n",
      "[228/500] HST -> OK\n",
      "[229/500] HSY -> OK\n",
      "[230/500] HUBB -> OK\n",
      "[231/500] HUM -> OK\n",
      "[232/500] HWM -> OK\n",
      "[233/500] IBM -> OK\n",
      "[234/500] ICE -> OK\n",
      "[235/500] IDXX -> OK\n",
      "[236/500] IEX -> OK\n",
      "[237/500] IFF -> OK\n",
      "[238/500] INCY -> OK\n",
      "[239/500] INTC -> OK\n",
      "[240/500] INTU -> OK\n",
      "[241/500] INVH -> OK\n",
      "[242/500] IP -> OK\n",
      "[243/500] IPG -> OK\n",
      "[244/500] IQV -> OK\n",
      "[245/500] IR -> OK\n",
      "[246/500] IRM -> OK\n",
      "[247/500] ISRG -> OK\n",
      "[248/500] IT -> OK\n",
      "[249/500] ITW -> OK\n",
      "[250/500] IVZ -> OK\n",
      "[251/500] J -> OK\n",
      "[252/500] JBHT -> OK\n",
      "[253/500] JBL -> OK\n",
      "[254/500] JCI -> OK\n",
      "[255/500] JKHY -> OK\n",
      "[256/500] JNJ -> OK\n",
      "[257/500] JPM -> OK\n",
      "[258/500] K -> OK\n",
      "[259/500] KDP -> OK\n",
      "[260/500] KEY -> OK\n",
      "[261/500] KEYS -> OK\n",
      "[262/500] KHC -> OK\n",
      "[263/500] KIM -> OK\n",
      "[264/500] KKR -> OK\n",
      "[265/500] KLAC -> OK\n",
      "[266/500] KMB -> OK\n",
      "[267/500] KMI -> OK\n",
      "[268/500] KMX -> OK\n",
      "[269/500] KO -> OK\n",
      "[270/500] KR -> OK\n",
      "[271/500] KVUE -> OK\n",
      "[272/500] L -> OK\n",
      "[273/500] LDOS -> OK\n",
      "[274/500] LEN -> OK\n",
      "[275/500] LH -> OK\n",
      "[276/500] LHX -> OK\n",
      "[277/500] LII -> OK\n",
      "[278/500] LIN -> OK\n",
      "[279/500] LKQ -> OK\n",
      "[280/500] LLY -> OK\n",
      "[281/500] LMT -> OK\n",
      "[282/500] LNT -> OK\n",
      "[283/500] LOW -> OK\n",
      "[284/500] LRCX -> OK\n",
      "[285/500] LULU -> OK\n",
      "[286/500] LUV -> OK\n",
      "[287/500] LVS -> OK\n",
      "[288/500] LW -> OK\n",
      "[289/500] LYB -> OK\n",
      "[290/500] LYV -> OK\n",
      "[291/500] MA -> OK\n",
      "[292/500] MAA -> OK\n",
      "[293/500] MAR -> OK\n",
      "[294/500] MAS -> OK\n",
      "[295/500] MCD -> OK\n",
      "[296/500] MCHP -> OK\n",
      "[297/500] MCK -> OK\n",
      "[298/500] MCO -> OK\n",
      "[299/500] MDLZ -> OK\n",
      "[300/500] MDT -> OK\n",
      "[301/500] MET -> OK\n",
      "[302/500] META -> OK\n",
      "[303/500] MGM -> OK\n",
      "[304/500] MHK -> OK\n",
      "[305/500] MKC -> OK\n",
      "[306/500] MKTX -> OK\n",
      "[307/500] MLM -> OK\n",
      "[308/500] MMC -> OK\n",
      "[309/500] MMM -> OK\n",
      "[310/500] MNST -> OK\n",
      "[311/500] MO -> OK\n",
      "[312/500] MOH -> OK\n",
      "[313/500] MOS -> OK\n",
      "[314/500] MPC -> OK\n",
      "[315/500] MPWR -> OK\n",
      "[316/500] MRK -> OK\n",
      "[317/500] MRNA -> OK\n",
      "[318/500] MS -> OK\n",
      "[319/500] MSCI -> OK\n",
      "[320/500] MSFT -> OK\n",
      "[321/500] MSI -> OK\n",
      "[322/500] MTB -> OK\n",
      "[323/500] MTCH -> OK\n",
      "[324/500] MTD -> OK\n",
      "[325/500] MU -> OK\n",
      "[326/500] NCLH -> OK\n",
      "[327/500] NDAQ -> OK\n",
      "[328/500] NDSN -> OK\n",
      "[329/500] NEE -> OK\n",
      "[330/500] NEM -> OK\n",
      "[331/500] NFLX -> OK\n",
      "[332/500] NI -> OK\n",
      "[333/500] NKE -> OK\n",
      "[334/500] NOC -> OK\n",
      "[335/500] NOW -> OK\n",
      "[336/500] NRG -> OK\n",
      "[337/500] NSC -> OK\n",
      "[338/500] NTAP -> OK\n",
      "[339/500] NTRS -> OK\n",
      "[340/500] NUE -> OK\n",
      "[341/500] NVDA -> OK\n",
      "[342/500] NVR -> OK\n",
      "[343/500] NWS -> OK\n",
      "[344/500] NWSA -> OK\n",
      "[345/500] NXPI -> OK\n",
      "[346/500] O -> OK\n",
      "[347/500] ODFL -> OK\n",
      "[348/500] OKE -> OK\n",
      "[349/500] OMC -> OK\n",
      "[350/500] ON -> OK\n",
      "[351/500] ORCL -> OK\n",
      "[352/500] ORLY -> OK\n",
      "[353/500] OTIS -> OK\n",
      "[354/500] OXY -> OK\n",
      "[355/500] PANW -> OK\n",
      "[356/500] PAYC -> OK\n",
      "[357/500] PAYX -> OK\n",
      "[358/500] PCAR -> OK\n",
      "[359/500] PCG -> OK\n",
      "[360/500] PEG -> OK\n",
      "[361/500] PEP -> OK\n",
      "[362/500] PFE -> OK\n",
      "[363/500] PFG -> OK\n",
      "[364/500] PG -> OK\n",
      "[365/500] PGR -> OK\n",
      "[366/500] PH -> OK\n",
      "[367/500] PHM -> OK\n",
      "[368/500] PKG -> OK\n",
      "[369/500] PLD -> OK\n",
      "[370/500] PLTR -> OK\n",
      "[371/500] PM -> OK\n",
      "[372/500] PNC -> OK\n",
      "[373/500] PNR -> OK\n",
      "[374/500] PNW -> OK\n",
      "[375/500] PODD -> OK\n",
      "[376/500] POOL -> OK\n",
      "[377/500] PPG -> OK\n",
      "[378/500] PPL -> OK\n",
      "[379/500] PRU -> OK\n",
      "[380/500] PSA -> OK\n",
      "[381/500] PSX -> OK\n",
      "[382/500] PTC -> OK\n",
      "[383/500] PWR -> OK\n",
      "[384/500] PYPL -> OK\n",
      "[385/500] QCOM -> OK\n",
      "[386/500] RCL -> OK\n",
      "[387/500] REG -> OK\n",
      "[388/500] REGN -> OK\n",
      "[389/500] RF -> OK\n",
      "[390/500] RJF -> OK\n",
      "[391/500] RL -> OK\n",
      "[392/500] RMD -> OK\n",
      "[393/500] ROK -> OK\n",
      "[394/500] ROL -> OK\n",
      "[395/500] ROP -> OK\n",
      "[396/500] ROST -> OK\n",
      "[397/500] RSG -> OK\n",
      "[398/500] RTX -> OK\n",
      "[399/500] RVTY -> OK\n",
      "[400/500] SBAC -> OK\n",
      "[401/500] SBUX -> OK\n",
      "[402/500] SCHW -> OK\n",
      "[403/500] SHW -> OK\n",
      "[404/500] SJM -> OK\n",
      "[405/500] SLB -> OK\n",
      "[406/500] SMCI -> OK\n",
      "[407/500] SNA -> OK\n",
      "[408/500] SNPS -> OK\n",
      "[409/500] SO -> OK\n",
      "[410/500] SOLV -> OK\n",
      "[411/500] SPG -> OK\n",
      "[412/500] SPGI -> OK\n",
      "[413/500] SRE -> OK\n",
      "[414/500] STE -> OK\n",
      "[415/500] STLD -> OK\n",
      "[416/500] STT -> OK\n",
      "[417/500] STX -> OK\n",
      "[418/500] STZ -> OK\n",
      "[419/500] SW -> OK\n",
      "[420/500] SWK -> OK\n",
      "[421/500] SWKS -> OK\n",
      "[422/500] SYF -> OK\n",
      "[423/500] SYK -> OK\n",
      "[424/500] SYY -> OK\n",
      "[425/500] T -> OK\n",
      "[426/500] TAP -> OK\n",
      "[427/500] TDG -> OK\n",
      "[428/500] TDY -> OK\n",
      "[429/500] TECH -> OK\n",
      "[430/500] TEL -> OK\n",
      "[431/500] TER -> OK\n",
      "[432/500] TFC -> OK\n",
      "[433/500] TGT -> OK\n",
      "[434/500] TJX -> OK\n",
      "[435/500] TKO -> OK\n",
      "[436/500] TMO -> OK\n",
      "[437/500] TMUS -> OK\n",
      "[438/500] TPL -> OK\n",
      "[439/500] TPR -> OK\n",
      "[440/500] TRGP -> OK\n",
      "[441/500] TRMB -> OK\n",
      "[442/500] TROW -> OK\n",
      "[443/500] TRV -> OK\n",
      "[444/500] TSCO -> OK\n",
      "[445/500] TSLA -> OK\n",
      "[446/500] TSN -> OK\n",
      "[447/500] TT -> OK\n",
      "[448/500] TTD -> OK\n",
      "[449/500] TTWO -> OK\n",
      "[450/500] TXN -> OK\n",
      "[451/500] TXT -> OK\n",
      "[452/500] TYL -> OK\n",
      "[453/500] UAL -> OK\n",
      "[454/500] UBER -> OK\n",
      "[455/500] UDR -> OK\n",
      "[456/500] UHS -> OK\n",
      "[457/500] ULTA -> OK\n",
      "[458/500] UNH -> OK\n",
      "[459/500] UNP -> OK\n",
      "[460/500] UPS -> OK\n",
      "[461/500] URI -> OK\n",
      "[462/500] USB -> OK\n",
      "[463/500] V -> OK\n",
      "[464/500] VICI -> OK\n",
      "[465/500] VLO -> OK\n",
      "[466/500] VLTO -> OK\n",
      "[467/500] VMC -> OK\n",
      "[468/500] VRSK -> OK\n",
      "[469/500] VRSN -> OK\n",
      "[470/500] VRTX -> OK\n",
      "[471/500] VST -> OK\n",
      "[472/500] VTR -> OK\n",
      "[473/500] VTRS -> OK\n",
      "[474/500] VZ -> OK\n",
      "[475/500] WAB -> OK\n",
      "[476/500] WAT -> OK\n",
      "[477/500] WBA -> OK\n",
      "[478/500] WBD -> OK\n",
      "[479/500] WDAY -> OK\n",
      "[480/500] WDC -> OK\n",
      "[481/500] WEC -> OK\n",
      "[482/500] WELL -> OK\n",
      "[483/500] WFC -> OK\n",
      "[484/500] WM -> OK\n",
      "[485/500] WMB -> OK\n",
      "[486/500] WMT -> OK\n",
      "[487/500] WRB -> OK\n",
      "[488/500] WSM -> OK\n",
      "[489/500] WST -> OK\n",
      "[490/500] WTW -> OK\n",
      "[491/500] WY -> OK\n",
      "[492/500] WYNN -> OK\n",
      "[493/500] XEL -> OK\n",
      "[494/500] XOM -> OK\n",
      "[495/500] XYL -> OK\n",
      "[496/500] XYZ -> OK\n",
      "[497/500] YUM -> OK\n",
      "[498/500] ZBH -> OK\n",
      "[499/500] ZBRA -> OK\n",
      "[500/500] ZTS -> OK\n",
      "Failed companies: []\n",
      "Total lignes: 500\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "(\"Could not convert 'Unknown' with type str: tried to convert to int64\", 'Conversion failed for column revenues_total_usd with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sagemaker-user/shared/outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(out_csv, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_parquet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3124\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3045\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parquet.py:482\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    480\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 482\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parquet.py:191\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m--> 191\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m    194\u001b[0m     df_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPANDAS_ATTRS\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(df\u001b[38;5;241m.\u001b[39mattrs)}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/table.pxi:4751\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/pandas_compat.py:639\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    635\u001b[0m             arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 639\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mconvert_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns_to_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_fields\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/pandas_compat.py:639\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    635\u001b[0m             arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 639\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m c, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(columns_to_convert, convert_fields)]\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/pandas_compat.py:626\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    622\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    623\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    624\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnull_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was non-nullable but pandas column \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m null values\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(field),\n\u001b[1;32m    630\u001b[0m                                                  result\u001b[38;5;241m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/pandas_compat.py:620\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    617\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 620\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    622\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    623\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    624\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/array.pxi:362\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/array.pxi:87\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: (\"Could not convert 'Unknown' with type str: tried to convert to int64\", 'Conversion failed for column revenues_total_usd with type object')"
     ]
    },
    {
     "data": {
      "application/sagemaker-interactive-debugging": {
       "cell_id": "46af5f2c-9e53-42fd-baae-5e93988bafd7",
       "debugging_info_folder": "/home/sagemaker-user/shared/.temp_sagemaker_unified_studio_debugging_info/46af5f2c-9e53-42fd-baae-5e93988bafd7",
       "instruction_file": "/home/sagemaker-user/shared/.temp_sagemaker_unified_studio_debugging_info/ipython_debugging_sop.txt",
       "magic_command": "no_magic",
       "session_type": "python_3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "companies = list_companies(FILLINGS_PATH)\n",
    "print(f\"Compagnies détectées: {len(companies)}\")\n",
    "print(\"Exemples:\", companies[:10])\n",
    "\n",
    "# ⚠️ Pendant le dev, commence par un petit batch pour valider\n",
    "SAMPLE = companies[:500]   # élargis ensuite\n",
    "\n",
    "rows = []\n",
    "failed = []\n",
    "for i, company in enumerate(SAMPLE, 1):\n",
    "    try:\n",
    "        rec = process_company_dir(FILLINGS_PATH, company)\n",
    "        rows.append(rec)\n",
    "        status = \"OK\" if rec.get(\"sources\") != \"PARSE_OR_JSON_ERROR\" else \"ERROR\"\n",
    "        print(f\"[{i}/{len(SAMPLE)}] {company} -> {status}\")\n",
    "        if status == \"ERROR\":\n",
    "            failed.append(company)\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}/{len(SAMPLE)}] {company} -> EXC: {e}\")\n",
    "        failed.append(company)\n",
    "\n",
    "print(\"Failed companies:\", failed)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=OUTPUT_COLUMNS)\n",
    "print(f\"Total lignes: {len(df)}\")\n",
    "\n",
    "# Sauvegardes\n",
    "out_csv = \"/home/sagemaker-user/shared/outputs/sec_matrix.csv\"\n",
    "out_parquet = \"/home/sagemaker-user/shared/outputs/sec_matrix.parquet\"\n",
    "Path(\"/home/sagemaker-user/shared/outputs\").mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_csv, index=False)\n",
    "df.to_parquet(out_parquet, index=False)\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
