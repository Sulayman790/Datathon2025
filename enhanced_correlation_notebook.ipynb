{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Enhanced Correlation Analysis\n",
    "\n",
    "This notebook performs enhanced correlation analysis between companies and regulations using:\n",
    "- AWS Bedrock Titan embeddings for semantic similarity\n",
    "- AWS Comprehend for entity extraction\n",
    "- Weighted scoring based on field types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T01:43:48.505183Z",
     "iopub.status.busy": "2025-11-03T01:43:48.504875Z",
     "iopub.status.idle": "2025-11-03T01:43:49.597576Z",
     "shell.execute_reply": "2025-11-03T01:43:49.596764Z",
     "shell.execute_reply.started": "2025-11-03T01:43:48.505113Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T01:43:49.599189Z",
     "iopub.status.busy": "2025-11-03T01:43:49.598789Z",
     "iopub.status.idle": "2025-11-03T01:43:49.713887Z",
     "shell.execute_reply": "2025-11-03T01:43:49.712797Z",
     "shell.execute_reply.started": "2025-11-03T01:43:49.599167Z"
    }
   },
   "outputs": [],
   "source": [
    "# AWS clients\n",
    "bedrock = boto3.client('bedrock-runtime', region_name='us-west-2')\n",
    "comprehend = boto3.client('comprehend', region_name='us-west-2')\n",
    "\n",
    "# File paths\n",
    "COMP_PATH = Path(\"/home/sagemaker-user/shared/outputs/sec_matrix.csv\")\n",
    "REGS_PATH = Path(\"/home/sagemaker-user/shared/regulations_example.csv\")\n",
    "OUT_PATH = Path(\"/home/sagemaker-user/shared/outputs/enhanced_correlations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T01:43:49.715016Z",
     "iopub.status.busy": "2025-11-03T01:43:49.714777Z",
     "iopub.status.idle": "2025-11-03T01:43:49.720803Z",
     "shell.execute_reply": "2025-11-03T01:43:49.719859Z",
     "shell.execute_reply.started": "2025-11-03T01:43:49.714996Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Get text embedding using Bedrock Titan\"\"\"\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "            modelId='amazon.titan-embed-text-v2:0',\n",
    "            body=json.dumps({\"inputText\": str(text)[:8000]})\n",
    "        )\n",
    "        result = json.loads(response['body'].read())\n",
    "        return np.array(result['embedding'])\n",
    "    except:\n",
    "        return np.zeros(1024)\n",
    "\n",
    "def extract_entities(text: str) -> list:\n",
    "    \"\"\"Extract entities using Comprehend\"\"\"\n",
    "    try:\n",
    "        response = comprehend.detect_entities(\n",
    "            Text=str(text)[:5000],\n",
    "            LanguageCode='en'\n",
    "        )\n",
    "        return [entity['Text'].lower() for entity in response['Entities'] \n",
    "                if entity['Score'] > 0.8]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T01:43:49.874819Z",
     "iopub.status.busy": "2025-11-03T01:43:49.874469Z",
     "iopub.status.idle": "2025-11-03T01:43:49.879469Z",
     "shell.execute_reply": "2025-11-03T01:43:49.878962Z",
     "shell.execute_reply.started": "2025-11-03T01:43:49.874796Z"
    }
   },
   "outputs": [],
   "source": [
    "def semantic_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate semantic similarity using embeddings\"\"\"\n",
    "    emb1 = get_embedding(text1)\n",
    "    emb2 = get_embedding(text2)\n",
    "    return cosine_similarity([emb1], [emb2])[0][0]\n",
    "\n",
    "def entity_overlap(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate entity overlap using Comprehend\"\"\"\n",
    "    entities1 = set(extract_entities(text1))\n",
    "    entities2 = set(extract_entities(text2))\n",
    "    if not entities1 and not entities2:\n",
    "        return 0.0\n",
    "    intersection = len(entities1 & entities2)\n",
    "    union = len(entities1 | entities2)\n",
    "    return intersection / union if union > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T01:43:50.310411Z",
     "iopub.status.busy": "2025-11-03T01:43:50.310118Z",
     "iopub.status.idle": "2025-11-03T01:43:50.317832Z",
     "shell.execute_reply": "2025-11-03T01:43:50.317071Z",
     "shell.execute_reply.started": "2025-11-03T01:43:50.310387Z"
    }
   },
   "outputs": [],
   "source": [
    "def enhanced_similarity(text1: str, text2: str, field_type: str) -> float:\n",
    "    \"\"\"Combined similarity using embeddings + entities\"\"\"\n",
    "    semantic_score = semantic_similarity(text1, text2)\n",
    "    entity_score = entity_overlap(text1, text2)\n",
    "    \n",
    "    # Weight based on field type\n",
    "    weights = {\n",
    "        'country': (0.8, 0.2),  # High semantic, low entity\n",
    "        'sector': (0.6, 0.4),   # Balanced\n",
    "        'activities': (0.5, 0.5), # Balanced\n",
    "        'regulatory_domain': (0.7, 0.3)\n",
    "    }\n",
    "    \n",
    "    w_semantic, w_entity = weights.get(field_type, (0.6, 0.4))\n",
    "    return w_semantic * semantic_score + w_entity * entity_score\n",
    "\n",
    "def make_geo_context(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Compose a short text that summarizes where the company actually operates\n",
    "    based on region_exposure_* columns + revenue_by_region_notes.\n",
    "    Example output: \"US:high Europe:medium China:low ... (plus notes)\"\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    # Common regions you mentioned; extend if your CSV has more\n",
    "    for region in [\"US\", \"Europe\", \"China\", \"India\"]:\n",
    "        col = f\"region_exposure_{region}\"\n",
    "        if col in row and pd.notna(row[col]):\n",
    "            val = str(row[col]).strip().lower()\n",
    "            # keep only meaningful labels\n",
    "            if val in {\"high\", \"medium\", \"low\"}:\n",
    "                parts.append(f\"{region}:{val}\")\n",
    "    # Add any free-text notes about revenue distribution by region\n",
    "    notes = str(row.get(\"revenue_by_region_notes\", \"\") or \"\").strip()\n",
    "    if notes:\n",
    "        parts.append(notes)\n",
    "    return \" \".join(parts).strip()\n",
    "\n",
    "def make_activity_context(row):\n",
    "    base = str(row.get(\"activities\", \"\")).strip()\n",
    "    if base:\n",
    "        return base\n",
    "    deps = str(row.get(\"critical_dependencies\", \"\"))\n",
    "    sector = str(row.get(\"sector\", \"\"))\n",
    "    return f\"{sector} {deps}\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "execution_state": "idle",
   "id": "359ffa66-0f61-4234-97f1-4e36fe9bc0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T01:47:35.095817Z",
     "iopub.status.busy": "2025-11-03T01:47:35.095071Z",
     "iopub.status.idle": "2025-11-03T01:47:35.118678Z",
     "shell.execute_reply": "2025-11-03T01:47:35.117780Z",
     "shell.execute_reply.started": "2025-11-03T01:47:35.095781Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_correlation(args):\n",
    "    i, crow, regulations, comp_cols, get_field, WEIGHTS = args\n",
    "\n",
    "    c_ticker = get_field(crow, \"ticker\")\n",
    "    c_name   = get_field(crow, \"company_name\")\n",
    "    c_country= get_field(crow, \"jurisdiction_country\")\n",
    "    c_sector = get_field(crow, \"sector\")\n",
    "    c_acts   = get_field(crow, \"activities\")\n",
    "    c_theme  = get_field(crow, \"regulatory_domain\")\n",
    "    c_geo_ctx = make_geo_context(crow)\n",
    "\n",
    "    rows = []\n",
    "    for j, rrow in regulations.iterrows():\n",
    "        m_country = enhanced_similarity(c_country, str(rrow.get(\"jurisdiction_country\", \"\")), \"country\")\n",
    "        m_sector  = enhanced_similarity(c_sector,  str(rrow.get(\"sector\", \"\")),                 \"sector\")\n",
    "        m_acts = enhanced_similarity(make_activity_context(crow), str(rrow.get(\"activity\", \"\")), \"activities\")\n",
    "        m_theme   = enhanced_similarity(c_theme,   str(rrow.get(\"regulatory_domain\", \"\")),      \"regulatory_domain\")\n",
    "        m_geo_ctx = enhanced_similarity(c_geo_ctx, str(rrow.get(\"jurisdiction_country\", \"\")), \"country\")\n",
    "        score = (\n",
    "            WEIGHTS[\"jurisdiction_country\"] * m_country +\n",
    "            WEIGHTS[\"geo_context\"]          * m_geo_ctx   +\n",
    "            WEIGHTS[\"sector\"]               * m_sector    +\n",
    "            WEIGHTS[\"activities\"]           * m_acts      +\n",
    "            WEIGHTS[\"regulatory_domain\"]    * m_theme\n",
    "        )\n",
    "        rows.append({\n",
    "            \"company_ticker\": c_ticker,\n",
    "            \"company_name\": c_name,\n",
    "            \"law_id\": rrow.get(\"law_id\", \"\"),\n",
    "            \"date\": rrow.get(\"date\", \"\"),   \n",
    "            \"country_match\": round(m_country, 3),\n",
    "            \"geo_context_match\": round(m_geo_ctx, 3),\n",
    "            \"sector_match\": round(m_sector, 3),\n",
    "            \"activities_match\": round(m_acts, 3),\n",
    "            \"domain_match\": round(m_theme, 3),\n",
    "            \"score_total\": round(score, 4)\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def run_enhanced_correlation(max_company_rows: int = 500):\n",
    "    # ... préparation CSV inchangée ...\n",
    "    companies = pd.read_csv(COMP_PATH, nrows=max_company_rows)\n",
    "    regulations = pd.read_csv(REGS_PATH)\n",
    "    comp_cols = {c.lower(): c for c in companies.columns}\n",
    "\n",
    "    def get_field(row, logical):\n",
    "        mapping = {\n",
    "            \"ticker\": [\"ticker\"],\n",
    "            \"company_name\": [\"company\", \"company_name\"],\n",
    "            \"jurisdiction_country\": [\"headquarters_country\", \"country\"],\n",
    "            \"sector\": [\"sector\"],\n",
    "            \"activities\": [\"activities\", \"business_function\"],\n",
    "            \"regulatory_domain\": [\"regulatory_dependencies\"]\n",
    "        }\n",
    "        for cand in mapping.get(logical, []):\n",
    "            if cand in comp_cols and pd.notna(row[comp_cols[cand]]):\n",
    "                return str(row[comp_cols[cand]])\n",
    "        return \"\"\n",
    "\n",
    "    WEIGHTS = {\n",
    "        \"jurisdiction_country\": 0.8,\n",
    "        \"geo_context\": 0.8,\n",
    "        \"sector\": 0.7,\n",
    "        \"activities\": 0.8,\n",
    "        \"regulatory_domain\": 0.2\n",
    "    }\n",
    "\n",
    "    # Construction de la liste d’arguments\n",
    "    args_list = []\n",
    "    for i, crow in companies.iterrows():\n",
    "        args_list.append((i, crow, regulations, comp_cols, get_field, WEIGHTS))\n",
    "\n",
    "    print(\"Launching parallel correlation computations ...\")\n",
    "    rows = []\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        for res in executor.map(compute_correlation, args_list):\n",
    "            rows.extend(res)\n",
    "\n",
    "    # ... reste inchangé (DataFrame tri, export CSV, print) ...\n",
    "    matches = pd.DataFrame(rows)\n",
    "    matches = matches.sort_values([\"company_ticker\", \"score_total\"], ascending=[True, False])\n",
    "    matches.to_csv(OUT_PATH, index=False)\n",
    "    print(f\"\\nEnhanced correlations saved to: {OUT_PATH}\")\n",
    "    print(\"Top 10 matches:\")\n",
    "    print(matches.head(10))\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "execution_state": "idle",
   "id": "8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T01:47:36.559596Z",
     "iopub.status.busy": "2025-11-03T01:47:36.559225Z",
     "iopub.status.idle": "2025-11-03T01:48:55.982692Z",
     "shell.execute_reply": "2025-11-03T01:48:55.980306Z",
     "shell.execute_reply.started": "2025-11-03T01:47:36.559564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching parallel correlation computations ...\n",
      "\n",
      "Enhanced correlations saved to: /home/sagemaker-user/shared/outputs/enhanced_correlations.csv\n",
      "Top 10 matches:\n",
      "  company_ticker                company_name  \\\n",
      "0              A  Agilent Technologies, Inc.   \n",
      "1           AAPL                  Apple Inc.   \n",
      "2           ABBV                 AbbVie Inc.   \n",
      "3           ABNB                Airbnb, Inc.   \n",
      "4            ABT         Abbott Laboratories   \n",
      "5           ACGL     Arch Capital Group Ltd.   \n",
      "6            ACN                   Accenture   \n",
      "7           ADBE                  Adobe Inc.   \n",
      "8            ADI        Analog Devices, Inc.   \n",
      "9            ADM                         ADM   \n",
      "\n",
      "                                 law_id        date  country_match  \\\n",
      "0  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.800   \n",
      "1  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.496   \n",
      "2  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.800   \n",
      "3  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.800   \n",
      "4  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.800   \n",
      "5  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.121   \n",
      "6  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.125   \n",
      "7  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.800   \n",
      "8  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.496   \n",
      "9  2.H.R.1 - One Big Beautiful Bill Act  2025-07-04          0.800   \n",
      "\n",
      "   geo_context_match  sector_match  activities_match  domain_match  \\\n",
      "0              0.040         0.071             0.021         0.130   \n",
      "1              0.065         0.062             0.029         0.126   \n",
      "2              0.088         0.092             0.072         0.098   \n",
      "3              0.083         0.074             0.088         0.139   \n",
      "4              0.065         0.092             0.046         0.099   \n",
      "5              0.070         0.087             0.042         0.078   \n",
      "6              0.074         0.044             0.055         0.079   \n",
      "7              0.071         0.044             0.035         0.086   \n",
      "8              0.067         0.054             0.027         0.103   \n",
      "9              0.067         0.095             0.107         0.176   \n",
      "\n",
      "   score_total  \n",
      "0       0.7644  \n",
      "1       0.5410  \n",
      "2       0.8520  \n",
      "3       0.8566  \n",
      "4       0.8133  \n",
      "5       0.2627  \n",
      "6       0.2498  \n",
      "7       0.7727  \n",
      "8       0.5301  \n",
      "9       0.8809  \n"
     ]
    }
   ],
   "source": [
    "# Run the enhanced correlation analysis\n",
    "matches = run_enhanced_correlation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4d4b5-a2ce-4b8b-9e1e-f97abc843e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
