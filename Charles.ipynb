{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b10ac-bb92-46b4-948b-bbd978527a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect spacy beautifulsoup4 lxml instructor pydantic\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "execution_state": "idle",
   "id": "ebdd40db-5423-4486-b1d9-238cf173b5b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:11:32.348665Z",
     "iopub.status.busy": "2025-11-02T01:11:32.348383Z",
     "iopub.status.idle": "2025-11-02T01:16:02.785043Z",
     "shell.execute_reply": "2025-11-02T01:16:02.784146Z",
     "shell.execute_reply.started": "2025-11-02T01:11:32.348644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INPUT] 5 files in directives (recursive=True)\n",
      " - 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL.html\n",
      " - 3.H.R.5376 - Inflation Reduction Act of 2022.xml\n",
      " - 4.REGULATION (EU) 20241689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL.html\n",
      " - 5.中华人民共和国能源法__中国政府网.html\n",
      " - 6.人工知能関連技術の研究開発及び活用の推進に関する法律.html\n",
      "[SAVE] out/1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL.csv\n",
      "[FILE] 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL.html: 5 rows; next_id=5\n",
      "[SAVE] out/3.H.R.5376 - Inflation Reduction Act of 2022.csv\n",
      "[FILE] 3.H.R.5376 - Inflation Reduction Act of 2022.xml: 144 rows; next_id=149\n",
      "[SAVE] out/4.REGULATION (EU) 20241689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL.csv\n",
      "[FILE] 4.REGULATION (EU) 20241689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL.html: 74 rows; next_id=223\n",
      "[SAVE] out/5.中华人民共和国能源法__中国政府网.csv\n",
      "[FILE] 5.中华人民共和国能源法__中国政府网.html: 29 rows; next_id=252\n",
      "[SAVE] out/6.人工知能関連技術の研究開発及び活用の推進に関する法律.csv\n",
      "[FILE] 6.人工知能関連技術の研究開発及び活用の推進に関する法律.html: 6 rows; next_id=258\n",
      "[SAVE] out/Regulatory_Article_Extraction_ALL.csv\n",
      "[TOTAL] 258 rows\n",
      "   article_id    jurisdiction sector                 activity  \\\n",
      "0           0  European Union         customer onboarding kyc   \n",
      "1           1  European Union         customer onboarding kyc   \n",
      "2           2  European Union                                   \n",
      "3           3  European Union                                   \n",
      "4           4  European Union                                   \n",
      "\n",
      "  regulatory_theme impact_type effective_date          regulator  \\\n",
      "0                   obligation     2019-12-18               NIST   \n",
      "1                   obligation     2019-12-18  General Regulator   \n",
      "2                   obligation     2019-12-18  General Regulator   \n",
      "3                   obligation     2019-12-18  General Regulator   \n",
      "4                   obligation     2019-12-18  General Regulator   \n",
      "\n",
      "                                            keywords company_country  \\\n",
      "0  les, des, membres, sanctions, dans, les membre...  European Union   \n",
      "1  les, prix, par, membres, par le, est, les memb...  European Union   \n",
      "2  des, consommateurs, droits, informations, par,...  European Union   \n",
      "3  les, dispositions, les membres, membres, adopt...  European Union   \n",
      "4  conseil, des, du conseil, parlement, du parlem...  European Union   \n",
      "\n",
      "                                        _source_file _article_marker  \\\n",
      "0  1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉE...                   \n",
      "1  1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉE...                   \n",
      "2  1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉE...                   \n",
      "3  1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉE...                   \n",
      "4  1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉE...                   \n",
      "\n",
      "                                        article_text  \n",
      "0  Article premier\\nModification de la directive ...  \n",
      "1  Article 2\\nModifications de la directive 98/6/...  \n",
      "2  Article 5\\nInformations sur les droits des con...  \n",
      "3  Article 7\\nTransposition\\n1.   Les États membr...  \n",
      "4  Article 9\\nDestinataires\\nLes États membres so...  \n"
     ]
    }
   ],
   "source": [
    "import os, re, json, html, string, hashlib, math, unicodedata, random, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, date\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse as parse_date\n",
    "from langdetect import detect, detect_langs, DetectorFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text as sk_text\n",
    "\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "AWS_REGION=os.getenv(\"AWS_REGION\",\"us-west-2\")\n",
    "MODEL_TRANSLATE_FALLBACK_1=os.getenv(\"BEDROCK_TRANSLATE_PRIMARY\",\"amazon.nova-micro-v1:0\")\n",
    "MODEL_TRANSLATE_FALLBACK_2=os.getenv(\"BEDROCK_TRANSLATE_SECONDARY\",\"anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "MODEL_SPLIT=os.getenv(\"BEDROCK_SPLIT_MODEL\",\"anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "MODEL_EXTRACT=os.getenv(\"BEDROCK_EXTRACT_MODEL\",\"amazon.nova-premier-v1:0\")\n",
    "MODEL_EXTRACT_DOC=os.getenv(\"BEDROCK_EXTRACT_DOC_MODEL\",\"amazon.nova-premier-v1:0:1000k\")\n",
    "\n",
    "INPUT_DIRS=json.loads(os.getenv(\"INPUT_DIRS\",\"[\\\"./directives\\\"]\"))\n",
    "ALLOWED_EXT=set([x.lower().strip() for x in json.loads(os.getenv(\"ALLOWED_EXT\",\"[\\\".html\\\",\\\".xml\\\"]\"))])\n",
    "RECURSIVE=os.getenv(\"RECURSIVE\",\"true\").strip().lower() in {\"1\",\"true\",\"yes\"}\n",
    "OUT_DIR=os.getenv(\"OUT_DIR\",\"out\")\n",
    "MAX_DOC_CHARS=int(os.getenv(\"MAX_DOC_CHARS\",\"180000\"))\n",
    "MAX_ART_CHARS=int(os.getenv(\"MAX_ART_CHARS\",\"18000\"))\n",
    "MIN_ART_LEN=int(os.getenv(\"MIN_ART_LEN\",\"300\"))\n",
    "TAXONOMY_PATH=os.getenv(\"TAXONOMY_PATH\", f\"{OUT_DIR}/dynamic_taxonomy.json\")\n",
    "\n",
    "translate=boto3.client(\"translate\",region_name=AWS_REGION)\n",
    "bedrock=boto3.client(\"bedrock-runtime\",region_name=AWS_REGION)\n",
    "\n",
    "EXTRA_STOPWORDS={\"section\",\"sections\",\"article\",\"articles\",\"annex\",\"annexe\",\"appendix\",\"appendice\",\"subtitle\",\"title\",\"chapter\",\"chapitre\",\"directive\",\"regulation\",\"regulations\",\"law\",\"act\",\"union\",\"paragraph\",\"subparagraph\",\"recital\",\"dispositif\",\"premier\",\"1er\",\"amended\",\"shall\",\"must\",\"may\",\"including\",\"include\",\"pursuant\",\"accordance\",\"specified\",\"provide\",\"provided\",\"applicable\",\"applicability\",\"applying\",\"applicant\",\"applicants\",\"applying\",\"subject\",\"subjects\",\"thereof\",\"hereof\",\"therein\",\"herein\",\"thereby\",\"hereby\",\"whereas\",\"hereunder\",\"thereunder\",\"among\",\"between\",\"within\",\"without\",\"preamble\",\"scope\",\"purpose\",\"purposes\",\"general\",\"specific\"}\n",
    "STOPWORDS_EN=set(sk_text.ENGLISH_STOP_WORDS)|EXTRA_STOPWORDS\n",
    "\n",
    "STOPWORDS_JA={\"第\",\"条\",\"項\",\"号\",\"章\",\"節\",\"款\",\"目次\",\"附則\",\"総則\",\"抄\",\"同\",\"前\",\"又は\",\"及び\",\"並びに\",\"その他\",\"こと\",\"もの\",\"ため\",\"者\",\"うえ\",\"上\",\"下\",\"について\",\"に関する\",\"に係る\",\"する\",\"される\",\"した\",\"して\",\"すると\",\"され\",\"なる\",\"ない\",\"これ\",\"それ\",\"当該\",\"各\",\"同条\",\"政府\",\"国\",\"内閣\",\"大臣\",\"本部\",\"本部長\",\"本法\",\"本章\",\"本条\",\"次項\",\"前項\",\"人工知能\",\"人工知能関連技術\",\"技術\",\"研究開発\",\"活用\",\"推進\",\"計画\",\"基本\",\"基本計画\",\"施策\",\"規定\",\"規範\",\"方針\",\"必要\",\"措置\",\"整備\",\"確保\",\"促進\",\"国際\",\"協力\",\"教育\",\"人材\",\"情報\",\"データ\",\"等\",\"など\"}\n",
    "STRUCTURAL_LABEL_JA={\"総則\",\"附則\",\"目次\",\"人工知能基本計画\",\"人工知能戦略本部\"}\n",
    "\n",
    "JURIS_HINTS={\n",
    "    \"european union\":\"European Union\",\"eu\":\"European Union\",\"gdpr\":\"European Union\",\"european parliament\":\"European Union\",\"cjeu\":\"European Union\",\"commission\":\"European Union\",\n",
    "    \"union européenne\":\"European Union\",\"journal officiel de l’union européenne\":\"European Union\",\"directive (ue)\":\"European Union\",\"parlement européen\":\"European Union\",\n",
    "    \"united states\":\"United States\",\"usa\":\"United States\",\"federal register\":\"United States\",\"congress\":\"United States\",\"sec. code\":\"United States\",\n",
    "    \"canada\":\"Canada\",\"crtc\":\"Canada\",\"osfi\":\"Canada\",\"privacy act rsc\":\"Canada\",\"pipa alberta\":\"Canada\",\"pipa bc\":\"Canada\",\"qp la loi\":\"Canada\",\n",
    "    \"united kingdom\":\"United Kingdom\",\"uk\":\"United Kingdom\",\"ico\":\"United Kingdom\",\"ofcom\":\"United Kingdom\",\"fca\":\"United Kingdom\",\"pra\":\"United Kingdom\",\n",
    "    \"germany\":\"Germany\",\"bafin\":\"Germany\",\"bsig\":\"Germany\",\n",
    "    \"france\":\"France\",\"cnil\":\"France\",\"code monétaire\":\"France\",\"code de la consommation\":\"France\",\n",
    "    \"italy\":\"Italy\",\"gazzetta ufficiale\":\"Italy\",\"garante\":\"Italy\",\n",
    "    \"spain\":\"Spain\",\"boe\":\"Spain\",\"aepd\":\"Spain\",\n",
    "    \"netherlands\":\"Netherlands\",\"ap\":\"Netherlands\",\"autoriteit persoonsgegevens\":\"Netherlands\",\n",
    "    \"switzerland\":\"Switzerland\",\"finma\":\"Switzerland\",\"revue fédérale\":\"Switzerland\",\n",
    "    \"australia\":\"Australia\",\"asic\":\"Australia\",\"oaic\":\"Australia\",\"acma\":\"Australia\",\n",
    "    \"singapore\":\"Singapore\",\"mas\":\"Singapore\",\"pdpa\":\"Singapore\",\n",
    "    \"hong kong\":\"Hong Kong\",\"hkma\":\"Hong Kong\",\"sfc\":\"Hong Kong\",\"pdpo\":\"Hong Kong\",\n",
    "    \"japan\":\"Japan\",\"pipa japan\":\"Japan\",\"fsa japan\":\"Japan\",\"cabinet office order\":\"Japan\",\n",
    "    \"china\":\"China\",\"中华人民共和国\":\"China\",\"国务院\":\"China\",\"全国人民代表大会\":\"China\",\"全国人大常委会\":\"China\"\n",
    "}\n",
    "\n",
    "REGULATOR_HINTS=[\n",
    "    \"European Commission\",\"European Parliament\",\"Council of the European Union\",\"EDPB\",\"EDPS\",\"ESMA\",\"EBA\",\"EIOPA\",\n",
    "    \"United States Congress\",\"Federal Trade Commission\",\"Securities and Exchange Commission\",\"FDIC\",\"OCC\",\"CFPB\",\"FINRA\",\"NIST\",\n",
    "    \"Canadian Radio-television and Telecommunications Commission\",\"OSFI\",\n",
    "    \"UK Information Commissioner's Office\",\"Ofcom\",\"Financial Conduct Authority\",\"Prudential Regulation Authority\",\n",
    "    \"BaFin\",\"CNIL\",\"Garante\",\"AEPD\",\"Autoriteit Persoonsgegevens\",\"FINMA\",\n",
    "    \"ASIC\",\"OAIC\",\"ACMA\",\"Monetary Authority of Singapore\",\n",
    "    \"Hong Kong Monetary Authority\",\"Securities and Futures Commission\",\"Privacy Commissioner for Personal Data\",\n",
    "    \"Personal Information Protection Commission\"\n",
    "]\n",
    "\n",
    "SECTOR_RULES=[\n",
    "    (\"Financial Services\",[\"bank\",\"credit institution\",\"securities\",\"broker\",\"insurer\",\"insurance\",\"reinsurance\",\"fintech\",\"payment\",\"prudential\",\"credit union\",\"investment firm\",\"portfolio\",\"fund\",\"asset manager\",\"trading venue\",\"trading facility\",\"crypto\",\"virtual asset\",\"stablecoin\",\"market abuse\",\"mifid\",\"psd\",\"psd2\",\"psd3\",\"lending\",\"microfinance\",\"wealth management\",\"custody\",\"settlement\",\"clearing\"]),\n",
    "    (\"Telecom & Media\",[\"telecom\",\"telecommunications\",\"spectrum\",\"broadcast\",\"streaming\",\"carrier\",\"sms\",\"mms\",\"voice\",\"over-the-top\",\"subscriber\",\"satellite\",\"internet service provider\",\"isp\",\"5g\",\"fiber\"]),\n",
    "    (\"Technology & Platforms\",[\"platform\",\"online platform\",\"hosting\",\"cloud\",\"saas\",\"paas\",\"iaas\",\"marketplace\",\"intermediary\",\"algorithmic\",\"ai\",\"artificial intelligence\",\"ml\",\"machine learning\",\"foundation model\",\"general purpose ai\",\"gpai\",\"search engine\",\"app store\",\"social network\",\"content moderation\"]),\n",
    "    (\"Healthcare & Pharma\",[\"medical\",\"health\",\"pharma\",\"medicinal\",\"device\",\"clinic\",\"ehr\",\"hipaa\",\"biotech\",\"clinical trial\",\"hospital\",\"telemedicine\"]),\n",
    "    (\"Public Sector\",[\"public authority\",\"ministry\",\"municipal\",\"agency\",\"government department\",\"public administration\",\"state\",\"local authority\"]),\n",
    "    (\"Energy & Utilities\",[\"energy\",\"electricity\",\"gas\",\"utility\",\"grid\",\"renewable\",\"pipeline\",\"oil\",\"mining\",\"nuclear\",\"water\",\"waste\"]),\n",
    "    (\"Retail & Consumer\",[\"retail\",\"consumer\",\"ecommerce\",\"distance selling\",\"consommateur\",\"rétractation\",\"marketplaces\",\"buy now pay later\",\"bnpl\",\"loyalty\"]),\n",
    "    (\"Transportation\",[\"aviation\",\"rail\",\"maritime\",\"shipping\",\"road transport\",\"logistics\",\"vehicle\",\"driver\",\"autonomous vehicle\",\"rideshare\",\"drone\"]),\n",
    "    (\"Education\",[\"school\",\"university\",\"student\",\"educational\",\"edtech\",\"curriculum\"]),\n",
    "    (\"Manufacturing & Industry\",[\"factory\",\"industrial\",\"manufacturing\",\"supply chain\",\"production\",\"standards\"]),\n",
    "    (\"Real Estate\",[\"real estate\",\"property\",\"landlord\",\"tenant\",\"mortgage\",\"construction\"])\n",
    "]\n",
    "\n",
    "ACTIVITY_RULES=[\n",
    "    (\"Data Processing\",[\"process personal data\",\"processing\",\"controller\",\"processor\",\"data subject\",\"consent\",\"profiling\",\"retention\",\"pseudonymisation\",\"anonymisation\",\"data sharing\",\"data transfer\",\"cross-border\",\"automated decision\"]),\n",
    "    (\"Customer Onboarding & KYC\",[\"know your customer\",\"kyc\",\"due diligence\",\"customer due diligence\",\"aml\",\"ctf\",\"sanctions\",\"screening\",\"identity verification\",\"onboarding\"]),\n",
    "    (\"Risk & Compliance\",[\"risk management\",\"governance\",\"compliance\",\"internal control\",\"audit\",\"reporting\",\"supervisory\",\"enforcement\",\"penalty\",\"fine\",\"complaint handling\",\"whistleblowing\",\"internal policy\"]),\n",
    "    (\"Payments & Transfers\",[\"payment service\",\"payment institution\",\"e-money\",\"remittance\",\"transfer\",\"settlement\",\"instant payment\",\"card\",\"token\",\"wallet\"]),\n",
    "    (\"Cybersecurity\",[\"security\",\"breach\",\"incident\",\"vulnerability\",\"encryption\",\"authentication\",\"cybersecurity\",\"network and information security\",\"nis\",\"nis2\",\"penetration test\",\"incident response\"]),\n",
    "    (\"AI Systems\",[\"ai system\",\"foundation model\",\"general purpose ai\",\"gpai\",\"high-risk\",\"risk management framework\",\"human oversight\",\"model transparency\",\"dataset\",\"hallucination\",\"bias\"]),\n",
    "    (\"Marketing & Communications\",[\"marketing\",\"direct marketing\",\"electronic communications\",\"cookies\",\"telemarketing\",\"consent for marketing\",\"adtech\",\"tracking\"]),\n",
    "    (\"Operational Resilience\",[\"business continuity\",\"disaster recovery\",\"outsourcing\",\"third party\",\"supply chain\"]),\n",
    "    (\"Transparency & Disclosure\",[\"disclosure\",\"publish\",\"transparency\",\"public statement\",\"explainability\"]),\n",
    "    (\"Consumer Rights\",[\"withdrawal\",\"cooling-off\",\"returns\",\"refunds\",\"fairness\",\"terms and conditions\"])\n",
    "]\n",
    "\n",
    "THEME_RULES=[\n",
    "    (\"Privacy & Data Protection\",[\"gdpr\",\"data protection\",\"personal data\",\"privacy\",\"dpia\",\"data subject rights\",\"controller\",\"processor\",\"lawful basis\",\"purpose limitation\",\"data minimisation\"]),\n",
    "    (\"Financial Conduct & Prudential\",[\"market abuse\",\"mifid\",\"crr\",\"crd\",\"solvency\",\"basel\",\"capital requirements\",\"liquidity\",\"governance\",\"stress test\",\"prudential\"]),\n",
    "    (\"Consumer Protection\",[\"consumer\",\"fairness\",\"withdrawal\",\"cooling-off\",\"distance selling\",\"complaint handling\",\"dark patterns\",\"information duties\"]),\n",
    "    (\"Cybersecurity & Resilience\",[\"nis\",\"nis2\",\"cybersecurity\",\"incident response\",\"business continuity\",\"resilience\",\"dora\",\"operational resilience\",\"critical infrastructure\"]),\n",
    "    (\"AI & Automated Decisioning\",[\"ai act\",\"ai system\",\"algorithmic\",\"automated decision\",\"training data\",\"testing\",\"validation\",\"human oversight\"]),\n",
    "    (\"Transparency & Reporting\",[\"disclosure\",\"report\",\"publish\",\"transparency\",\"public statement\",\"explainability\",\"auditability\"]),\n",
    "    (\"Sanctions & Financial Crime\",[\"sanction\",\"aml\",\"ctf\",\"terrorist financing\",\"proliferation financing\",\"screening\",\"beneficial owner\"]),\n",
    "    (\"Digital Markets & Platforms\",[\"gatekeeper\",\"platform\",\"interoperability\",\"self-preferencing\",\"ranking\"]),\n",
    "    (\"Data Governance & Sharing\",[\"data governance\",\"data sharing\",\"data altruism\",\"data intermediaries\"])\n",
    "]\n",
    "\n",
    "IMPACT_RULES=[\n",
    "    (\"Obligation\",[\"shall\",\"must\",\"required\",\"obliged\",\"duty\",\"应当\",\"必须\",\"しなければならない\",\"必要がある\"]),\n",
    "    (\"Prohibition\",[\"shall not\",\"may not\",\"prohibited\",\"forbidden\",\"不得\",\"禁止\",\"してはならない\",\"禁ずる\"]),\n",
    "    (\"Permission\",[\"may\",\"is permitted\",\"allowed\",\"可以\",\"ことができる\",\"許可\"]),\n",
    "    (\"Enforcement\",[\"penalty\",\"fine\",\"sanction\",\"offence\",\"offense\",\"罰則\",\"罚款\",\"处罚\",\"処罰\"]),\n",
    "    (\"Reporting\",[\"report\",\"notify\",\"notification\",\"disclosure\",\"報告\",\"备案\",\"届出\"])\n",
    "]\n",
    "\n",
    "PARLIAMENTARY=re.compile(r\"^\\s*(having regard|after transmission|after consulting|in accordance with|whereas|pursuant to|considering|vu(?:\\s+la|(?:x|es)?)?)\\b.*$\",re.IGNORECASE|re.MULTILINE)\n",
    "RECITALS=re.compile(r\"\\b(whereas|considérant(?:\\s+que)?|vu(?:\\s+la|(?:x|es)?)?)\\b.*?(?=(^|\\n)\\s*(article|art\\.?|dispositif|chapitre|titre)\\s*[^\\n]*\\b(1|premier|1er)\\b|\\bannex|annexe|appendix|appendice|schedule\\b)\",re.IGNORECASE|re.DOTALL)\n",
    "TAIL=re.compile(r\"(done at\\s+[A-Za-z]+\\s+\\d{1,2}\\s+[A-Za-z]+\\s+\\d{4}.*$|for the european parliament.*$|for the council.*$)\",re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "HEAD_PATTERNS=[\n",
    "    r\"(^|\\n)\\s*Article\\s+(premier|1er|first|one|i|\\d+[A-Za-z]?(?:\\s*(?:bis|ter|quater))?)\\b\",\n",
    "    r\"(^|\\n)\\s*Art\\.\\s*\\d+[A-Za-z]?\\b\",\n",
    "    r\"(^|\\n)\\s*Section\\s+\\d+[A-Za-z.-]*\\b\",\n",
    "    r\"(^|\\n)\\s*Sec\\.\\s*\\d+[A-Za-z.-]*\\b\",\n",
    "    r\"(^|\\n)\\s*Subtitle\\s+[A-Z]+\\b\",\n",
    "    r\"(^|\\n)\\s*Chapter\\s+([IVXLCDM]+|\\d+)\\b\",\n",
    "    r\"(^|\\n)\\s*Article\\s+\\d+\\b\",\n",
    "    r\"(^|\\n)\\s*Chapitre\\s+([IVXLCDM]+|\\d+)\\b\",\n",
    "    r\"(^|\\n)\\s*Titre\\s+([IVXLCDM]+|\\d+)\\b\",\n",
    "    r\"(^|\\n)\\s*Article\\s+\\d+\\s*-\\s*\"\n",
    "]\n",
    "CJK_HEAD_PATTERNS=[\n",
    "    r\"(^|\\n)\\s*第[一二三四五六七八九十百千\\d]+条\\b\",\n",
    "    r\"(^|\\n)\\s*第[一二三四五六七八九十百千\\d]+章\\b\",\n",
    "    r\"(^|\\n)\\s*第[一二三四五六七八九十百千\\d]+節\\b\",\n",
    "    r\"(^|\\n)\\s*章\\s*名\",\n",
    "    r\"(^|\\n)\\s*条\\s*文\"\n",
    "]\n",
    "ART_SPLIT=re.compile(\"|\".join(HEAD_PATTERNS+CJK_HEAD_PATTERNS),re.IGNORECASE)\n",
    "ANNEX_HEAD=re.compile(r\"\\b(Annex|Annexe|Appendix|Appendice|Schedule|附則|附录|附件|附錄)\\b\",re.IGNORECASE)\n",
    "DATE_RX=re.compile(r\"(\\b\\d{1,2}\\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*\\s+\\d{4}\\b|\\b\\d{4}-\\d{1,2}-\\d{1,2}\\b|\\b\\d{1,2}[./]\\d{1,2}[./]\\d{2,4}\\b)\",re.IGNORECASE)\n",
    "CJK_DATE_RX=re.compile(r\"(\\d{4})年(\\d{1,2})月(\\d{1,2})日\")\n",
    "\n",
    "JP_ERA=[(\"令和\",2019),(\"平成\",1989),(\"昭和\",1926),(\"大正\",1912),(\"明治\",1868)]\n",
    "\n",
    "JP_EFFECTIVE_PATTERNS=[\n",
    "    r\"この法律は、?公布の日から施行する\",\n",
    "    r\"施行期日\",\n",
    "    r\"公布の日から起算して[^\\n。]{0,20}施行する\",\n",
    "    r\"この法律は[^\\n。]{0,40}施行する\",\n",
    "    r\"施行[^\\n。]{0,20}日は[^\\n。]+\"\n",
    "]\n",
    "\n",
    "def _strip_japanese_structural_tokens(s: str) -> str:\n",
    "    s=(s or \"\").strip()\n",
    "    s=re.sub(r\"[第章条項号節款]\",\" \",s)\n",
    "    for w in STRUCTURAL_LABEL_JA:\n",
    "        s=s.replace(w,\" \")\n",
    "    return re.sub(r\"\\s+\",\" \",s).strip()\n",
    "\n",
    "def _resolve_input_dir():\n",
    "    for d in INPUT_DIRS:\n",
    "        p=Path(d)\n",
    "        if p.exists():\n",
    "            return p\n",
    "    p=Path(\"./directives\")\n",
    "    p.mkdir(parents=True,exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def _canon_name(p: Path):\n",
    "    stem=p.stem\n",
    "    stem=re.sub(r\"(?i)(^|[-_\\s])(checkpoint|copy|copie|copiar)$\",\"\",stem).strip()\n",
    "    stem=re.sub(r\"(?i)[-_]checkpoint\",\"\",stem)\n",
    "    stem=re.sub(r\"\\s*\\(\\d+\\)$\",\"\",stem)\n",
    "    stem=re.sub(r\"\\s+\",\" \",stem)\n",
    "    return (stem.lower(), p.suffix.lower())\n",
    "\n",
    "def _is_checkpoint(p: Path):\n",
    "    name=p.name.lower()\n",
    "    if (\"-checkpoint\" in name or name.endswith(\"_checkpoint.html\") or \"checkpoint.html\" in name or name.endswith(\".ipynb\") or p.parent.name.lower()==\".ipynb_checkpoints\" or \".ipynb_checkpoints\" in str(p.parent).lower()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _list_files(root,recursive=False):\n",
    "    def ok(p):\n",
    "        n=p.name\n",
    "        if _is_checkpoint(p): return False\n",
    "        if p.parent.name == \".ipynb_checkpoints\": return False\n",
    "        return p.is_file() and p.suffix.lower() in ALLOWED_EXT and not (n.startswith(\".\") or n.startswith(\"._\") or n.endswith(\"~\"))\n",
    "    it = (root.rglob(\"*\") if recursive else root.iterdir())\n",
    "    cand=[p for p in it if ok(p)]\n",
    "    grouped={}\n",
    "    for p in cand:\n",
    "        key=_canon_name(p)\n",
    "        best=grouped.get(key)\n",
    "        if best is None:\n",
    "            grouped[key]=p\n",
    "        else:\n",
    "            best_is_checkpoint=_is_checkpoint(best)\n",
    "            p_is_checkpoint=_is_checkpoint(p)\n",
    "            if best_is_checkpoint and not p_is_checkpoint:\n",
    "                grouped[key]=p\n",
    "            elif best_is_checkpoint==p_is_checkpoint:\n",
    "                if p.stat().st_mtime > best.stat().st_mtime:\n",
    "                    grouped[key]=p\n",
    "    files=sorted(grouped.values(),key=lambda x:x.name.lower())\n",
    "    print(f\"[INPUT] {len(files)} files in {root} (recursive={recursive})\")\n",
    "    for p in files: print(\" -\",p.name)\n",
    "    return files\n",
    "\n",
    "def _save_per_input(df,input_path):\n",
    "    Path(OUT_DIR).mkdir(parents=True,exist_ok=True)\n",
    "    stem=input_path.stem\n",
    "    csv_path=f\"{OUT_DIR}/{stem}.csv\"\n",
    "    pq_path=f\"{OUT_DIR}/{stem}.parquet\"\n",
    "    df.to_csv(csv_path,index=False)\n",
    "    try:\n",
    "        df.to_parquet(pq_path,index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"[SAVE] {csv_path}\")\n",
    "\n",
    "def _save_all(df):\n",
    "    Path(OUT_DIR).mkdir(parents=True,exist_ok=True)\n",
    "    csv_path=f\"{OUT_DIR}/Regulatory_Article_Extraction_ALL.csv\"\n",
    "    pq_path=f\"{OUT_DIR}/Regulatory_Article_Extraction_ALL.parquet\"\n",
    "    df.to_csv(csv_path,index=False)\n",
    "    try:\n",
    "        df.to_parquet(pq_path,index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"[SAVE] {csv_path}\")\n",
    "\n",
    "def _read(path):\n",
    "    return Path(path).read_text(encoding=\"utf-8\",errors=\"ignore\")\n",
    "\n",
    "def _extract_main_container(soup):\n",
    "    targets=[\"#innerDocument\",\"main#contentsLaw\",\"#docHtml\",\"article\",\"#content\",\"body\"]\n",
    "    for sel in targets:\n",
    "        el=soup.select_one(sel)\n",
    "        if el and len(el.get_text(strip=True))>200:\n",
    "            return el\n",
    "    return soup\n",
    "\n",
    "def _html_to_text(raw):\n",
    "    try:\n",
    "        soup=BeautifulSoup(raw,\"lxml\")\n",
    "    except Exception:\n",
    "        soup=BeautifulSoup(raw,\"html.parser\")\n",
    "    main=_extract_main_container(soup)\n",
    "    for tag in main([\"script\",\"style\",\"nav\",\"header\",\"footer\",\"noscript\",\"aside\",\"form\"]):\n",
    "        try: tag.extract()\n",
    "        except Exception: pass\n",
    "    for br in main.find_all([\"br\",\"hr\"]):\n",
    "        br.replace_with(\"\\n\")\n",
    "    for li in main.find_all(\"li\"):\n",
    "        txt=li.get_text(\" \",strip=True)\n",
    "        li.string=(\"\\n- \"+txt+\"\\n\") if txt else \"\\n\"\n",
    "    for th in main.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"strong\",\"b\"]):\n",
    "        t=th.get_text(\" \",strip=True)\n",
    "        th.string=(\"\\n\"+t+\"\\n\") if t else \"\\n\"\n",
    "    text=html.unescape(main.get_text(\"\\n\",strip=True))\n",
    "    text=re.sub(r\"\\n{3,}\",\"\\n\\n\",text)\n",
    "    return text,soup\n",
    "\n",
    "def _has_cjk(s):\n",
    "    return bool(re.search(r\"[\\u3400-\\u4dbf\\u4e00-\\u9fff\\u3040-\\u30ff]\",s or \"\"))\n",
    "\n",
    "def _lang_probs(text):\n",
    "    try:\n",
    "        return detect_langs(text)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return [type(\"LP\",(object,),{\"lang\":detect(text),\"prob\":1.0})()]\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "def _english_confidence(text):\n",
    "    t=(text or \"\").strip()\n",
    "    if not t:\n",
    "        return 1.0\n",
    "    sample=t[:8000]\n",
    "    letters=[ch for ch in sample if ch.isalpha()]\n",
    "    ascii_letters=[ch for ch in letters if (\"A\"<=ch<=\"Z\") or (\"a\"<=ch<=\"z\")]\n",
    "    ascii_ratio=(len(ascii_letters)/max(1,len(letters))) if letters else 0.0\n",
    "    stop=sk_text.ENGLISH_STOP_WORDS\n",
    "    tokens=[w.strip(string.punctuation).lower() for w in re.split(r\"\\s+\",sample) if w]\n",
    "    stop_hits=sum(1 for tok in tokens if tok in stop)\n",
    "    stop_ratio=stop_hits/max(1,len(tokens))\n",
    "    ld_prob=0.0\n",
    "    for lp in _lang_probs(sample):\n",
    "        if getattr(lp,\"lang\",\"\")==\"en\":\n",
    "            ld_prob=max(ld_prob,float(getattr(lp,\"prob\",0.0)))\n",
    "    score=0.75*ld_prob+0.25*(0.6*ascii_ratio+0.4*stop_ratio)\n",
    "    return max(0.0,min(1.0,score))\n",
    "\n",
    "def _is_english(text):\n",
    "    if not text or len(text.strip())==0: return True\n",
    "    if _has_cjk(text):\n",
    "        try:\n",
    "            return detect(text)==\"en\"\n",
    "        except Exception:\n",
    "            return False\n",
    "    if len(text)<160:\n",
    "        try:\n",
    "            return detect(text)==\"en\"\n",
    "        except Exception:\n",
    "            return False\n",
    "    return _english_confidence(text)>=0.55\n",
    "\n",
    "def _chunk_for_translate(t,limit=4200):\n",
    "    out=[]; i=0; n=len(t)\n",
    "    seps=[\"\\n\\n\",\"。\\n\",\"。\\n\\n\",\"；\",\"；\\n\",\"；\\n\\n\",\"，\",\"。\\n—\\n\",\"\\n- \"]\n",
    "    while i<n:\n",
    "        j=min(i+limit,n)\n",
    "        k=-1\n",
    "        for sep in seps:\n",
    "            ks=t.rfind(sep,i,j)\n",
    "            if ks>k: k=ks+len(sep)\n",
    "        if k<i+200: k=j\n",
    "        piece=t[i:k].strip(); i=k\n",
    "        if piece: out.append(piece)\n",
    "    return out\n",
    "\n",
    "def _translate_piece_bedrock(piece,model_id,system_prompt=None):\n",
    "    body={\"max_tokens\":4000,\"temperature\":0.0}\n",
    "    if \"anthropic\" in model_id:\n",
    "        body={\"anthropic_version\":\"bedrock-2023-05-31\",\"max_tokens\":4000,\"temperature\":0.0,\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"Translate this to precise legal English. Keep headings, numbering, dates, entities verbatim. No commentary.\\n\\n\"+piece}]}]}\n",
    "    else:\n",
    "        prompt=\"Translate the following text into precise legal English. Preserve headings, numbering, and dates. No commentary.\\n\\n\"+piece\n",
    "        body={\"inputText\":prompt,\"textGenerationConfig\":{\"maxTokenCount\":4000,\"temperature\":0.0}}\n",
    "    try:\n",
    "        resp=bedrock.invoke_model(modelId=model_id,body=json.dumps(body))\n",
    "        data=json.loads(resp[\"body\"].read())\n",
    "        if \"anthropic\" in model_id:\n",
    "            return data.get(\"content\",[{}])[0].get(\"text\",\"\")\n",
    "        return data.get(\"outputText\",\"\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _force_english(text):\n",
    "    if not text: return text\n",
    "    if _is_english(text): return text\n",
    "    chunks=_chunk_for_translate(text,limit=4000)\n",
    "    out=[]\n",
    "    for piece in chunks:\n",
    "        ok=False\n",
    "        try:\n",
    "            r=translate.translate_text(Text=piece,SourceLanguageCode=\"auto\",TargetLanguageCode=\"en\")\n",
    "            cand=r.get(\"TranslatedText\",\"\") or \"\"\n",
    "            if cand.strip() and _is_english(cand):\n",
    "                out.append(cand); ok=True\n",
    "        except Exception:\n",
    "            pass\n",
    "        if not ok:\n",
    "            cand=_translate_piece_bedrock(piece,MODEL_TRANSLATE_FALLBACK_1)\n",
    "            if cand.strip() and _is_english(cand):\n",
    "                out.append(cand); ok=True\n",
    "        if not ok:\n",
    "            cand=_translate_piece_bedrock(piece,MODEL_TRANSLATE_FALLBACK_2)\n",
    "            if cand.strip():\n",
    "                out.append(cand if _is_english(cand) else _translate_piece_bedrock(cand,MODEL_TRANSLATE_FALLBACK_1))\n",
    "    final=\"\\n\".join([c for c in out if c]).strip()\n",
    "    return final if final else text\n",
    "\n",
    "def _translate(text):\n",
    "    t=(text or \"\").strip()\n",
    "    if not t: return t\n",
    "    if _is_english(t): return t\n",
    "    out=_force_english(t)\n",
    "    if not _is_english(out):\n",
    "        out=_force_english(out)\n",
    "    return out\n",
    "\n",
    "def _extract_main_title(soup, fallback):\n",
    "    try:\n",
    "        t=soup.title.get_text(strip=True) if soup and soup.title else \"\"\n",
    "    except Exception:\n",
    "        t=\"\"\n",
    "    h1=\"\"\n",
    "    try:\n",
    "        h=soup.find([\"h1\",\"h2\"])\n",
    "        h1=h.get_text(\" \",strip=True) if h else \"\"\n",
    "    except Exception:\n",
    "        h1=\"\"\n",
    "    cand=next((x for x in [h1,t,fallback] if x),fallback)\n",
    "    return cand\n",
    "\n",
    "def _extract_main_container_text(raw):\n",
    "    txt,soup=_html_to_text(raw)\n",
    "    return txt,soup\n",
    "\n",
    "def _prune_operative(text_en):\n",
    "    x=PARLIAMENTARY.sub(\"\",text_en)\n",
    "    x=RECITALS.sub(\"\",x)\n",
    "    x=TAIL.sub(\"\",x)\n",
    "    x=re.sub(r\"\\bTable of Contents\\b.*?(?=(^|\\n)\\s*Article\\s+(1|premier|1er)\\b)\", \"\", x, flags=re.IGNORECASE|re.DOTALL)\n",
    "    return x.strip()\n",
    "\n",
    "def _split_regex(clean):\n",
    "    ms=list(ART_SPLIT.finditer(clean))\n",
    "    if not ms:\n",
    "        return [{\"marker\":\"Article 1\",\"text\":clean.strip()}]\n",
    "    out=[]\n",
    "    idxs=[m.start() for m in ms]+[len(clean)]\n",
    "    for i in range(len(ms)):\n",
    "        s=idxs[i]; e=idxs[i+1]\n",
    "        line_end=clean.find(\"\\n\",ms[i].start(),e)\n",
    "        if line_end==-1: line_end=ms[i].end()\n",
    "        marker=clean[ms[i].start():line_end].strip()\n",
    "        body=clean[s:e].strip()\n",
    "        if not body.lower().startswith(marker.lower()): body=marker+\"\\n\"+body\n",
    "        out.append({\"marker\":marker,\"text\":body})\n",
    "    return out\n",
    "\n",
    "def _extract_article_blocks_jp(soup):\n",
    "    if not soup:\n",
    "        return []\n",
    "    results=[]\n",
    "    titles=soup.select(\"._div_ArticleTitle\")\n",
    "    for title in titles:\n",
    "        marker_text=title.get_text(\"\\n\",strip=True)\n",
    "        caption=None\n",
    "        prev=title.find_previous_sibling(lambda tag: tag.name and \"_div_ArticleCaption\" in \" \".join(tag.get(\"class\", [])))\n",
    "        if prev and prev.find_next_sibling() is title:\n",
    "            caption=prev.get_text(\"\\n\",strip=True)\n",
    "        heading=f\"{marker_text} {caption}\" if caption else marker_text\n",
    "        body_parts=[heading]\n",
    "        cur=title.next_sibling\n",
    "        while cur:\n",
    "            if getattr(cur,\"name\",None):\n",
    "                cls=\" \".join(cur.get(\"class\",[]))\n",
    "                if \"_div_ArticleTitle\" in cls or \"_div_ChapterTitle\" in cls:\n",
    "                    break\n",
    "                if any(k in cls for k in [\"_div_ParagraphSentence\",\"_div_ItemSentence\",\"_div_ArticleCaption\"]):\n",
    "                    body_parts.append(cur.get_text(\"\\n\",strip=True))\n",
    "            cur=cur.next_sibling\n",
    "        text=\"\\n\".join([p for p in body_parts if p]).strip()\n",
    "        if text:\n",
    "            results.append({\"marker\":heading,\"text\":text})\n",
    "    return results\n",
    "\n",
    "def _split_from_soup_sections(soup):\n",
    "    try:\n",
    "        if soup.select(\"._div_ArticleTitle\"):\n",
    "            jp=_extract_article_blocks_jp(soup)\n",
    "            if jp:\n",
    "                return jp\n",
    "    except Exception:\n",
    "        pass\n",
    "    secs=[]\n",
    "    try:\n",
    "        for sec in soup.find_all([\"section\"],recursive=True):\n",
    "            txt=sec.get_text(\"\\n\",strip=True)\n",
    "            if txt and (re.search(r\"(第[一二三四五六七八九十百千\\d]+条|Article\\s+\\d+|Art\\.\\s*\\d+|Article\\s+(premier|1er))\",txt,flags=re.IGNORECASE) or len(txt)>400):\n",
    "                marker=\"Section\"\n",
    "                m=re.search(r\"(第[一二三四五六七八九十百千\\d]+条|Article\\s+\\d+|Art\\.\\s*\\d+|Article\\s+(premier|1er))\",txt,flags=re.IGNORECASE)\n",
    "                if m: marker=m.group(0)\n",
    "                secs.append({\"marker\":marker,\"text\":txt})\n",
    "        for div in soup.select(\"._div_ArticleTitle, ._div_ParagraphSentence\"):\n",
    "            block=div.get_text(\"\\n\",strip=True)\n",
    "            if block:\n",
    "                parent=div.parent\n",
    "                grab=[]\n",
    "                for sib in parent.find_all(recursive=False):\n",
    "                    grab.append(sib.get_text(\"\\n\",strip=True))\n",
    "                txt=\"\\n\".join([g for g in grab if g]).strip()\n",
    "                if txt and len(txt)>150:\n",
    "                    m=re.search(r\"(第[一二三四五六七八九十百千\\d]+条)\",txt)\n",
    "                    marker=m.group(1) if m else \"Article\"\n",
    "                    secs.append({\"marker\":marker,\"text\":txt})\n",
    "    except Exception:\n",
    "        pass\n",
    "    return secs\n",
    "\n",
    "def _ai_split(text_en):\n",
    "    prompt=(\"Extract ONLY operative Articles/Sections with exact headings + full bodies. Exclude preambles, recitals, annexes, signatures, tables of contents. Return a JSON array of strings; each string is ONE complete Article/Section starting with its heading line (e.g., 'Article 14 Human oversight').\")\n",
    "    body={\"anthropic_version\":\"bedrock-2023-05-31\",\"max_tokens\":12000,\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":prompt+\"\\n\\nDOCUMENT:\\n\"+text_en[:MAX_DOC_CHARS]}]}],\"temperature\":0.0}\n",
    "    try:\n",
    "        resp=bedrock.invoke_model(modelId=MODEL_SPLIT,body=json.dumps(body))\n",
    "        parsed=json.loads(resp[\"body\"].read())\n",
    "        content=parsed[\"content\"][0][\"text\"]\n",
    "        arts=json.loads(content)\n",
    "        return [a for a in arts if isinstance(a,str) and len(a.strip())>MIN_ART_LEN]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _has_heading(text_en):\n",
    "    return bool(re.search(r\"(^|\\n)\\s*(article\\s+(premier|1er|first|one|i|\\d+)|art\\.\\s*\\d+|section\\s+\\d+|sec\\.\\s*\\d+|subtitle\\s+[A-Z]+|chapter\\s+([IVXLCDM]+|\\d+)|第[一二三四五六七八九十百千\\d]+条)\\b\",text_en,re.IGNORECASE))\n",
    "\n",
    "def _is_article(text_en):\n",
    "    if ANNEX_HEAD.search(text_en): return False\n",
    "    b=text_en.strip()\n",
    "    if not _has_heading(b): return False\n",
    "    if not re.search(r\"\\b(shall|must|may\\s+not|is\\s+prohibited|is\\s+required|are\\s+required|duty|penalt|fine|obligat|require|してはならない|しなければならない|必要がある|禁止|罚|应当|必须)\\b\",b,re.IGNORECASE):\n",
    "        if len(b)<MIN_ART_LEN:\n",
    "            return False\n",
    "    toks=re.findall(r\"[A-Za-z%/_-]+|[\\u3400-\\u9fff\\u3040-\\u30ff]\",b)\n",
    "    if len(toks)<30: return False\n",
    "    return True\n",
    "\n",
    "def _to_ymd(s):\n",
    "    try:\n",
    "        return parse_date(s,fuzzy=True,dayfirst=False).date().isoformat()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _parse_cjk_date(s):\n",
    "    m=CJK_DATE_RX.search(s)\n",
    "    if not m: return \"\"\n",
    "    y=int(m.group(1)); mth=int(m.group(2)); d=int(m.group(3))\n",
    "    try:\n",
    "        return date(y,mth,d).isoformat()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _parse_jp_era(s):\n",
    "    m=re.search(r\"(令和|平成|昭和|大正|明治)\\s*([元\\d]+)\\s*年\\s*([0-9]{1,2})\\s*月\\s*([0-9]{1,2})\\s*日\",s)\n",
    "    if not m: return \"\"\n",
    "    era=m.group(1); year=m.group(2); month=int(m.group(3)); day=int(m.group(4))\n",
    "    base=dict(JP_ERA)[era]\n",
    "    y=1 if year==\"元\" else int(year)\n",
    "    try:\n",
    "        return date(base+y-1,month,day).isoformat()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _doc_date(raw_text,soup,fname):\n",
    "    cands=[]\n",
    "    try:\n",
    "        cands.extend(el.get_text(\" \",strip=True) for el in soup.select(\"#lawTitleNo,#lawTitle,.oj-hd-date,.oj-doc-ti,.date,.document-date,.pubdate,.issued,.enacted,.approved,#lawTitleNo\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    head=\" \".join(cands)\n",
    "    jp=_parse_jp_era(head) or _parse_jp_era(raw_text[:120000])\n",
    "    if jp: return jp\n",
    "    cjk=_parse_cjk_date(head) or _parse_cjk_date(raw_text[:120000])\n",
    "    if cjk: return cjk\n",
    "    for m in DATE_RX.findall(head):\n",
    "        d=_to_ymd(m[0])\n",
    "        if d: return d\n",
    "    for m in DATE_RX.findall(raw_text[:120000]):\n",
    "        d=_to_ymd(m[0])\n",
    "        if d: return d\n",
    "    m=re.search(r\"(\\d{4})[-_](\\d{1,2})[-_](\\d{1,2})\",Path(fname).name)\n",
    "    if m:\n",
    "        try:\n",
    "            return datetime(int(m.group(1)),int(m.group(2)),int(m.group(3))).date().isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    m=re.search(r\"\\b(19|20)\\d{2}\\b\",Path(fname).name)\n",
    "    if m:\n",
    "        try:\n",
    "            return datetime(int(m.group(0)),6,30).date().isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        ts=Path(fname).stat().st_mtime\n",
    "        return datetime.fromtimestamp(ts,tz=timezone.utc).date().isoformat()\n",
    "    except Exception:\n",
    "        return datetime.now(timezone.utc).date().isoformat()\n",
    "\n",
    "def _ensure_ascii_lower(s: str) -> str:\n",
    "    s=(s or \"\").strip()\n",
    "    s=unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    s=re.sub(r\"[^A-Za-z0-9%/_\\-\\s]\", \"\", s)\n",
    "    s=re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def _normalize_label_list(items):\n",
    "    if isinstance(items,str):\n",
    "        items=[x.strip() for x in items.split(\",\") if x.strip()]\n",
    "    items=[_ensure_ascii_lower(x) for x in (items or []) if _ensure_ascii_lower(x)]\n",
    "    bad={\"article\",\"section\",\"chapter\",\"directive\",\"regulation\",\"annex\",\"annexe\",\"appendix\",\"title\",\"subtitle\"}\n",
    "    return [x for x in dict.fromkeys(items) if x not in bad]\n",
    "\n",
    "def _load_taxonomy():\n",
    "    try:\n",
    "        p=Path(TAXONOMY_PATH)\n",
    "        if p.exists():\n",
    "            with open(p,\"r\",encoding=\"utf-8\") as f:\n",
    "                data=json.load(f)\n",
    "            for k in [\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\"]:\n",
    "                data.setdefault(k,{})\n",
    "            return data\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {\"sector\":{}, \"activity\":{}, \"regulatory_theme\":{}, \"impact_type\":{}}\n",
    "\n",
    "def _save_taxonomy(tax):\n",
    "    Path(OUT_DIR).mkdir(parents=True,exist_ok=True)\n",
    "    with open(TAXONOMY_PATH,\"w\",encoding=\"utf-8\") as f:\n",
    "        json.dump(tax, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def _tokenize_terms(label: str):\n",
    "    toks=[t for t in re.split(r\"[^A-Za-z0-9%/_-]+\", label) if t]\n",
    "    out=set()\n",
    "    for t in toks:\n",
    "        if len(t) >= 3:\n",
    "            out.add(t)\n",
    "    for i in range(len(toks)-1):\n",
    "        big=f\"{toks[i]} {toks[i+1]}\"\n",
    "        if 3 <= len(big.replace(\" \",\"\")) <= 30:\n",
    "            out.add(big)\n",
    "    return sorted(out)\n",
    "\n",
    "def _taxonomy_add_labels(tax, category: str, labels):\n",
    "    cat = tax.setdefault(category, {})\n",
    "    for lbl in _normalize_label_list(labels):\n",
    "        if not lbl: \n",
    "            continue\n",
    "        entry = cat.setdefault(lbl, {\"terms\":[], \"count\":0})\n",
    "        terms=set(entry.get(\"terms\",[]))\n",
    "        for t in _tokenize_terms(lbl):\n",
    "            terms.add(t)\n",
    "        entry[\"terms\"]=sorted(terms)\n",
    "        entry[\"count\"]=int(entry.get(\"count\",0))+1\n",
    "        cat[lbl]=entry\n",
    "    return tax\n",
    "\n",
    "def _taxonomy_merge_model_keywords(tax, category: str, label: str, kws):\n",
    "    label=_ensure_ascii_lower(label)\n",
    "    if not label: return tax\n",
    "    cat=tax.setdefault(category,{})\n",
    "    entry=cat.setdefault(label, {\"terms\":[], \"count\":0})\n",
    "    terms=set(entry.get(\"terms\",[]))\n",
    "    for w in _normalize_label_list(kws):\n",
    "        for t in _tokenize_terms(w):\n",
    "            terms.add(t)\n",
    "    entry[\"terms\"]=sorted(terms)\n",
    "    cat[label]=entry\n",
    "    return tax\n",
    "\n",
    "def _guess_from_taxonomy(text_en, tax, category: str):\n",
    "    text_low=\" \"+_ensure_ascii_lower(text_en)+\" \"\n",
    "    hits=[]\n",
    "    cat=tax.get(category,{})\n",
    "    for lbl, meta in cat.items():\n",
    "        for t in meta.get(\"terms\",[]):\n",
    "            if re.search(r\"(?<![A-Za-z0-9_/%-])\"+re.escape(t)+r\"(?![A-Za-z0-9_/%-])\", text_low):\n",
    "                hits.append(lbl); break\n",
    "    hits_unique=list(dict.fromkeys(hits))\n",
    "    hits_unique.sort(key=lambda x: -int(cat.get(x,{}).get(\"count\",0)))\n",
    "    return hits_unique\n",
    "\n",
    "def _extract_doc_fields(doc_title, doc_text_en):\n",
    "    prompt=(\"Extract a single JSON object with keys: jurisdiction (string), regulator (list), sector (list), activity (list), regulatory_theme (list), impact_type (list), company_country (list), default_effective_date (YYYY-MM-DD or empty). Infer if implicit.\\n\\nTITLE:\\n\"+(doc_title or \"\")+\"\\n\\nDOCUMENT:\\n\"+(doc_text_en[:MAX_DOC_CHARS] or \"\"))\n",
    "    body={\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":prompt}]}],\"max_tokens\":6000,\"temperature\":0.0}\n",
    "    try:\n",
    "        resp=bedrock.invoke_model(modelId=MODEL_EXTRACT_DOC,body=json.dumps(body))\n",
    "        parsed=json.loads(resp[\"body\"].read())\n",
    "        text_out=parsed.get(\"content\",[{}])[0].get(\"text\",\"{}\")\n",
    "        data=json.loads(text_out)\n",
    "        def join(v): return \", \".join([str(x).strip() for x in (v or []) if str(x).strip()]) if isinstance(v,list) else str(v or \"\").strip()\n",
    "        return {\n",
    "            \"jurisdiction\":str(data.get(\"jurisdiction\",\"\")).strip(),\n",
    "            \"sector\":join(data.get(\"sector\",[])),\n",
    "            \"activity\":join(data.get(\"activity\",[])),\n",
    "            \"regulatory_theme\":join(data.get(\"regulatory_theme\",[])),\n",
    "            \"impact_type\":join(data.get(\"impact_type\",[])),\n",
    "            \"regulator\":join(data.get(\"regulator\",[])),\n",
    "            \"company_country\":join(data.get(\"company_country\",[])),\n",
    "            \"default_effective_date\":str(data.get(\"default_effective_date\",\"\")).strip()\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"jurisdiction\":\"\",\"sector\":\"\",\"activity\":\"\",\"regulatory_theme\":\"\",\"impact_type\":\"\",\"regulator\":\"\",\"company_country\":\"\",\"default_effective_date\":\"\"}\n",
    "\n",
    "def _clean_tokens(tokens):\n",
    "    out=[]\n",
    "    for w in tokens:\n",
    "        wl=_ensure_ascii_lower(w)\n",
    "        if not wl: continue\n",
    "        if wl in STOPWORDS_EN: continue\n",
    "        if re.fullmatch(r\"\\d+(\\.\\d+)?\",wl): continue\n",
    "        if len(wl)<=2: continue\n",
    "        out.append(wl)\n",
    "    return out\n",
    "\n",
    "def _keywords_en(text_en,max_k=20):\n",
    "    try:\n",
    "        vec=TfidfVectorizer(stop_words=list(STOPWORDS_EN),ngram_range=(1,2),min_df=1,token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z%/_-]+\\b\")\n",
    "        X=vec.fit_transform([text_en]); scores=X.toarray()[0]; terms=vec.get_feature_names_out()\n",
    "        pairs=sorted(zip(terms,scores),key=lambda x:-x[1])\n",
    "        bad={\"article\",\"articles\",\"annex\",\"section\",\"chapter\",\"law\",\"act\",\"directive\",\"regulation\",\"regulations\",\"union\",\"paragraph\",\"recital\",\"subparagraph\",\"subtitle\",\"title\"}\n",
    "        out=[]\n",
    "        for t,_ in pairs:\n",
    "            tl=_ensure_ascii_lower(t)\n",
    "            if tl in bad: continue\n",
    "            if tl in STOPWORDS_EN: continue\n",
    "            if re.fullmatch(r\"\\d+(\\.\\d+)?\",tl): continue\n",
    "            if len(tl)<=2: continue\n",
    "            if tl not in out: out.append(tl)\n",
    "            if len(out)>=max_k: break\n",
    "        return out\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _keywords_cjk(raw,max_k=20):\n",
    "    txt=re.sub(r\"\\s+\",\"\",raw)\n",
    "    grams={}\n",
    "    for n in (2,3):\n",
    "        for i in range(len(txt)-n+1):\n",
    "            g=txt[i:i+n]\n",
    "            if re.search(r\"[\\u3400-\\u9fff\\u3040-\\u30ff]\",g) and not re.search(r\"^[\\d\\W_]+$\",g):\n",
    "                grams[g]=grams.get(g,0)+1\n",
    "    ordered=sorted(grams.items(),key=lambda x:-x[1])\n",
    "    return [k for k,_ in ordered[:max_k]]\n",
    "\n",
    "def _translate_list_to_english(items):\n",
    "    items=[str(x).strip() for x in (items or []) if str(x).strip()]\n",
    "    if not items:\n",
    "        return []\n",
    "    joined=\", \".join(items)\n",
    "    def _split_clean(s):\n",
    "        parts=[w.strip() for w in re.split(r\"[;,]\", s) if w.strip()]\n",
    "        seen=set(); out=[]\n",
    "        for p in parts:\n",
    "            p=unicodedata.normalize(\"NFKD\", p).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "            p=re.sub(r\"[^A-Za-z0-9%/_\\-\\s]\", \"\", p)\n",
    "            p=re.sub(r\"\\s+\",\" \",p).strip()\n",
    "            if p and p.lower() not in seen:\n",
    "                seen.add(p.lower()); out.append(p)\n",
    "        return out\n",
    "    try:\n",
    "        r=translate.translate_text(Text=joined, SourceLanguageCode=\"auto\", TargetLanguageCode=\"en\")\n",
    "        cand=(r.get(\"TranslatedText\") or \"\").strip()\n",
    "        if cand:\n",
    "            out=_split_clean(cand)\n",
    "            if out:\n",
    "                return out\n",
    "    except Exception:\n",
    "        pass\n",
    "    cand=_translate_piece_bedrock(joined, MODEL_TRANSLATE_FALLBACK_1)\n",
    "    if cand and cand.strip():\n",
    "        out=_split_clean(cand)\n",
    "        if out:\n",
    "            return out\n",
    "    cand=_translate_piece_bedrock(joined, MODEL_TRANSLATE_FALLBACK_2)\n",
    "    if cand and cand.strip():\n",
    "        out=_split_clean(cand)\n",
    "        if out:\n",
    "            return out\n",
    "    return _split_clean(joined)\n",
    "\n",
    "def _ensure_keywords_english(kws):\n",
    "    kws=_translate_list_to_english(kws)\n",
    "    clean=[]\n",
    "    for w in kws:\n",
    "        x=unicodedata.normalize(\"NFKD\", w).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "        x=re.sub(r\"[^A-Za-z0-9%/_\\-\\s]\", \"\", x)\n",
    "        x=re.sub(r\"\\s+\",\" \", x).strip().lower()\n",
    "        if not x: continue\n",
    "        if x in STOPWORDS_EN: continue\n",
    "        if re.fullmatch(r\"\\d+(\\.\\d+)?\",x): continue\n",
    "        if len(x)<=2: continue\n",
    "        if x not in clean: clean.append(x)\n",
    "    return clean\n",
    "\n",
    "def _extract_fields(article_en):\n",
    "    prompt=(\"Extract JSON with keys: jurisdiction (string), sector (list), activity (list), regulatory_theme (list), impact_type (list), effective_date (YYYY-MM-DD or empty string), regulator (list), keywords (list of 5-12), company_country (list). Use only THIS article's text.\\n\\nARTICLE:\\n\"+(article_en[:MAX_ART_CHARS] or \"\"))\n",
    "    body={\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":prompt}]}],\"max_tokens\":6000,\"temperature\":0.0}\n",
    "    try:\n",
    "        resp=bedrock.invoke_model(modelId=MODEL_EXTRACT,body=json.dumps(body))\n",
    "        parsed=json.loads(resp[\"body\"].read())\n",
    "        text=parsed.get(\"content\",[{}])[0].get(\"text\",\"{}\")\n",
    "        data=json.loads(text)\n",
    "        def norm_list(v):\n",
    "            if isinstance(v,list):\n",
    "                return [str(x).strip() for x in v if str(x).strip()]\n",
    "            s=str(v or \"\").strip()\n",
    "            return [x.strip() for x in s.split(\",\") if x.strip()] if s else []\n",
    "        return {\n",
    "            \"jurisdiction\":str(data.get(\"jurisdiction\",\"\")).strip(),\n",
    "            \"sector\":norm_list(data.get(\"sector\",[])),\n",
    "            \"activity\":norm_list(data.get(\"activity\",[])),\n",
    "            \"regulatory_theme\":norm_list(data.get(\"regulatory_theme\",[])),\n",
    "            \"impact_type\":norm_list(data.get(\"impact_type\",[])),\n",
    "            \"effective_date\":str(data.get(\"effective_date\",\"\")).strip(),\n",
    "            \"regulator\":norm_list(data.get(\"regulator\",[])),\n",
    "            \"keywords\":norm_list(data.get(\"keywords\",[])),\n",
    "            \"company_country\":norm_list(data.get(\"company_country\",[])),\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"jurisdiction\":\"\",\"sector\":[],\"activity\":[],\"regulatory_theme\":[],\"impact_type\":[],\"effective_date\":\"\",\"regulator\":[],\"keywords\":[],\"company_country\":[]}\n",
    "\n",
    "def _guess_jurisdiction(title, body):\n",
    "    s=(title+\" \"+body).lower()\n",
    "    for k,v in JURIS_HINTS.items():\n",
    "        if k in s:\n",
    "            return v\n",
    "    if _has_cjk(title+body):\n",
    "        if \"内閣\" in (title+body) or \"法律\" in (title+body): return \"Japan\"\n",
    "        if \"中华人民共和国\" in (title+body) or \"国务院\" in (title+body): return \"China\"\n",
    "    m=re.search(r\"\\b(united states|united kingdom|canada|germany|france|italy|spain|netherlands|switzerland|australia|singapore|hong kong|japan|china)\\b\",s)\n",
    "    if m:\n",
    "        name=m.group(1).title()\n",
    "        return name if name in [\"United States\",\"United Kingdom\"] else name\n",
    "    return \"Global\"\n",
    "\n",
    "def _guess_regulators(text_en):\n",
    "    s=text_en.lower()\n",
    "    found=[]\n",
    "    for name in REGULATOR_HINTS:\n",
    "        if name.lower() in s:\n",
    "            found.append(name)\n",
    "    if not found:\n",
    "        if \"supervis\" in s or \"competent authority\" in s: found.append(\"Competent Supervisory Authority\")\n",
    "    return list(dict.fromkeys(found)) or [\"General Regulator\"]\n",
    "\n",
    "def _guess_effective_date_jp(article_text, doc_date):\n",
    "    head=article_text[:3000]\n",
    "    for pat in JP_EFFECTIVE_PATTERNS:\n",
    "        m=re.search(pat, head)\n",
    "        if m:\n",
    "            window=head[max(0,m.start()-50):m.end()+50]\n",
    "            d=_parse_jp_era(window) or _parse_cjk_date(window)\n",
    "            if not d:\n",
    "                m2=DATE_RX.search(window)\n",
    "                if m2:\n",
    "                    d=_to_ymd(m2.group(0))\n",
    "            if d:\n",
    "                return d\n",
    "    return \"\"\n",
    "\n",
    "def _guess_effective_date(article_en, doc_date):\n",
    "    m=re.search(r\"\\bapplicab(le|ility)\\s+from\\s+([^\\n.;]+)\",article_en,flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        d=_to_ymd(m.group(2))\n",
    "        if d: return d\n",
    "    m=re.search(r\"\\benter(s|ed)?\\s+into\\s+force\\s+on\\s+([^\\n.;]+)\",article_en,flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        d=_to_ymd(m.group(2))\n",
    "        if d: return d\n",
    "    m=DATE_RX.search(article_en[:8000])\n",
    "    if m:\n",
    "        d=_to_ymd(m[0])\n",
    "        if d: return d\n",
    "    cjk=_parse_cjk_date(article_en[:8000])\n",
    "    if cjk: return cjk\n",
    "    return doc_date or datetime.now(timezone.utc).date().isoformat()\n",
    "\n",
    "def _guess_from_rules(text_en, rules):\n",
    "    text_low=\" \"+_ensure_ascii_lower(text_en)+\" \"\n",
    "    hits=[]\n",
    "    for label,keys in rules:\n",
    "        for k in keys:\n",
    "            if f\" {k.lower()} \" in text_low or re.search(r\"(?<![A-Za-z0-9_/%-])\"+re.escape(k.lower())+r\"(?![A-Za-z0-9_/%-])\",text_low):\n",
    "                hits.append(_ensure_ascii_lower(label)); break\n",
    "    return list(dict.fromkeys(hits))\n",
    "\n",
    "def _merge_fill(fields,doc_date,article_en,doc_backfill=None, title=\"\"):\n",
    "    tax=_load_taxonomy()\n",
    "    out=dict(fields)\n",
    "    title=title or \"\"\n",
    "    body=article_en or \"\"\n",
    "    if not (out.get(\"jurisdiction\") or \"\").strip():\n",
    "        out[\"jurisdiction\"]=doc_backfill.get(\"jurisdiction\",\"\").strip() if doc_backfill else \"\"\n",
    "        if not out[\"jurisdiction\"]:\n",
    "            out[\"jurisdiction\"]=_guess_jurisdiction(title,body)\n",
    "    for k in [\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\",\"regulator\",\"company_country\"]:\n",
    "        v=out.get(k,[])\n",
    "        if isinstance(v,str):\n",
    "            v=[x.strip() for x in v.split(\",\") if x.strip()]\n",
    "        if k in [\"regulator\",\"company_country\"]:\n",
    "            v=_translate_list_to_english(v)\n",
    "            v=[_ensure_ascii_lower(x).title() for x in v if _ensure_ascii_lower(x)]\n",
    "        else:\n",
    "            v=_ensure_keywords_english(v)\n",
    "        out[k]=\", \".join(dict.fromkeys(v))\n",
    "    for cat,rule_set in [(\"sector\",SECTOR_RULES),(\"activity\",ACTIVITY_RULES),(\"regulatory_theme\",THEME_RULES),(\"impact_type\",IMPACT_RULES)]:\n",
    "        if not out.get(cat,\"\").strip():\n",
    "            guesses=_guess_from_taxonomy(body, tax, cat)\n",
    "            if not guesses:\n",
    "                guesses=_guess_from_rules(body, rule_set)\n",
    "            if guesses:\n",
    "                out[cat]=\", \".join(guesses[:5])\n",
    "            else:\n",
    "                out[cat]=\"Obligation\" if cat==\"impact_type\" else \"General\"\n",
    "    if not out.get(\"regulator\",\"\").strip():\n",
    "        out[\"regulator\"]=\", \".join(_guess_regulators(body))\n",
    "    if not out.get(\"company_country\",\"\").strip():\n",
    "        out[\"company_country\"]=out[\"jurisdiction\"] if out[\"jurisdiction\"]!=\"Global\" else \"Global\"\n",
    "    eff=(out.get(\"effective_date\",\"\") or \"\").strip()\n",
    "    if not eff and doc_backfill and (doc_backfill.get(\"default_effective_date\",\"\") or \"\").strip():\n",
    "        eff=doc_backfill[\"default_effective_date\"]\n",
    "    if not eff:\n",
    "        eff=_guess_effective_date(body,doc_date)\n",
    "    out[\"effective_date\"]=eff\n",
    "    for cat in [\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\"]:\n",
    "        labels=_normalize_label_list(out.get(cat,\"\").split(\",\"))\n",
    "        tax=_taxonomy_add_labels(tax, cat, labels)\n",
    "        if cat in fields and isinstance(fields[cat], list):\n",
    "            tax=_taxonomy_merge_model_keywords(tax, cat, labels[0] if labels else \"\", fields.get(\"keywords\",[]))\n",
    "    _save_taxonomy(tax)\n",
    "    return out\n",
    "\n",
    "def _normhash(t):\n",
    "    x=re.sub(r\"\\s+\",\" \",(t or \"\")).strip().lower()\n",
    "    x=re.sub(r\"[^\\w\\s%/_-]+\",\"\",x)\n",
    "    return hashlib.sha1(x.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _ensure_min_keywords(en_kws, raw_text, en_text, k=8):\n",
    "    out=list(dict.fromkeys(en_kws))\n",
    "    if len(out)>=k:\n",
    "        return out[:k]\n",
    "    more=_keywords_en(en_text, max_k=k*2)\n",
    "    for w in more:\n",
    "        if w not in out:\n",
    "            out.append(w)\n",
    "        if len(out)>=k:\n",
    "            break\n",
    "    if len(out)<k and _has_cjk(raw_text) and not _is_english(raw_text):\n",
    "        cjk=_keywords_cjk(raw_text, max_k=k*2)\n",
    "        cjk_en=_ensure_keywords_english(_translate_list_to_english(cjk))\n",
    "        for w in cjk_en:\n",
    "            if w not in out:\n",
    "                out.append(w)\n",
    "            if len(out)>=k:\n",
    "                break\n",
    "    return out[:k]\n",
    "\n",
    "def _split_candidates(en_text,soup):\n",
    "    secs=_split_from_soup_sections(soup)\n",
    "    if secs: return secs\n",
    "    ai=_ai_split(en_text)\n",
    "    if ai: return [{\"marker\": ai[i].split(\"\\n\",1)[0].strip() if \"\\n\" in ai[i] else f\"Article {i+1}\", \"text\": ai[i]} for i in range(len(ai))]\n",
    "    return _split_regex(en_text)\n",
    "\n",
    "def _filter_jp_grams(grams):\n",
    "    out=[]\n",
    "    for g in grams:\n",
    "        if any(ch in \"第章条項号節款\" for ch in g): \n",
    "            continue\n",
    "        if g in STOPWORDS_JA:\n",
    "            continue\n",
    "        out.append(g)\n",
    "    return out\n",
    "\n",
    "def _keywords_pipeline(article_raw, article_en, model_kws):\n",
    "    model_kws=_ensure_keywords_english(model_kws)\n",
    "    if _has_cjk(article_raw) and not _is_english(article_raw):\n",
    "        tfidf_raw=_keywords_cjk(article_raw,20)\n",
    "        tfidf_raw=_filter_jp_grams(tfidf_raw)\n",
    "        tfidf=_ensure_keywords_english(_translate_list_to_english(tfidf_raw))\n",
    "    else:\n",
    "        tfidf=_ensure_keywords_english(_keywords_en(article_en,20))\n",
    "    out=[]\n",
    "    for w in model_kws+tfidf:\n",
    "        if w and w not in out: out.append(w)\n",
    "    out=_ensure_min_keywords(out, article_raw, article_en, k=10)\n",
    "    return out[:20]\n",
    "\n",
    "def _post_ensure_english_fields(row):\n",
    "    row[\"article_text\"]=_force_english(row.get(\"article_text\",\"\") or \"\")\n",
    "    row[\"keywords\"]=\", \".join(_ensure_keywords_english([w.strip() for w in (row.get(\"keywords\",\"\") or \"\").split(\",\") if w.strip()]))\n",
    "    row[\"jurisdiction\"]=_translate_list_to_english([row.get(\"jurisdiction\",\"\")])[0] if row.get(\"jurisdiction\",\"\") else \"Global\"\n",
    "    row[\"sector\"]=\", \".join(_ensure_keywords_english([w for w in (row.get(\"sector\",\"\") or \"\").split(\",\") if w.strip()]))\n",
    "    row[\"activity\"]=\", \".join(_ensure_keywords_english([w for w in (row.get(\"activity\",\"\") or \"\").split(\",\") if w.strip()]))\n",
    "    row[\"regulatory_theme\"]=\", \".join(_ensure_keywords_english([w for w in (row.get(\"regulatory_theme\",\"\") or \"\").split(\",\") if w.strip()]))\n",
    "    row[\"impact_type\"]=\", \".join(_ensure_keywords_english([w for w in (row.get(\"impact_type\",\"\") or \"\").split(\",\") if w.strip()])) or \"obligation\"\n",
    "    row[\"regulator\"]=\", \".join(_translate_list_to_english([w for w in (row.get(\"regulator\",\"\") or \"\").split(\",\") if w.strip()])) or \"General Regulator\"\n",
    "    row[\"company_country\"]=\", \".join(_translate_list_to_english([w for w in (row.get(\"company_country\",\"\") or \"\").split(\",\") if w.strip()])) or (row[\"jurisdiction\"] if row[\"jurisdiction\"]!=\"Global\" else \"Global\")\n",
    "    return row\n",
    "\n",
    "def _process_file(path,start_id):\n",
    "    raw=_read(path)\n",
    "    raw_txt,soup=_extract_main_container_text(raw)\n",
    "    en_doc=_translate(raw_txt)\n",
    "    if not _is_english(en_doc): en_doc=_force_english(en_doc)\n",
    "    doc_date=_doc_date(raw_txt,soup,path)\n",
    "    title_guess=_extract_main_title(soup, Path(path).stem)\n",
    "    doc_backfill=_extract_doc_fields(title_guess,en_doc)\n",
    "    for k in [\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\",\"regulator\",\"company_country\",\"jurisdiction\"]:\n",
    "        v=doc_backfill.get(k,\"\")\n",
    "        if isinstance(v,str):\n",
    "            doc_backfill[k]=\", \".join(_translate_list_to_english([w for w in v.split(\",\") if w.strip()]))\n",
    "    if doc_backfill.get(\"jurisdiction\",\"\"):\n",
    "        doc_backfill[\"jurisdiction\"]=doc_backfill[\"jurisdiction\"].strip()\n",
    "    operative=_prune_operative(en_doc)\n",
    "    articles=_split_candidates(operative,soup)\n",
    "    seen=set(); rows=[]; cur=start_id\n",
    "    for art in articles:\n",
    "        raw_body=art[\"text\"].strip()\n",
    "        en_body=_translate(raw_body) if not _is_english(raw_body) else raw_body\n",
    "        if not _is_article(en_body): continue\n",
    "        h=_normhash(en_body)\n",
    "        if h in seen: continue\n",
    "        seen.add(h)\n",
    "        fields_raw=_extract_fields(en_body)\n",
    "        fields_raw[\"regulator\"]=_translate_list_to_english(fields_raw.get(\"regulator\",[]))\n",
    "        fields_raw[\"keywords\"]=_ensure_keywords_english(fields_raw.get(\"keywords\",[]))\n",
    "        fields=_merge_fill(fields_raw,doc_date,en_body,doc_backfill,title_guess)\n",
    "        jp_eff=_guess_effective_date_jp(raw_body,doc_date) if _has_cjk(raw_body) else \"\"\n",
    "        if jp_eff and not fields.get(\"effective_date\"):\n",
    "            fields[\"effective_date\"]=jp_eff\n",
    "        keywords_list=_keywords_pipeline(raw_body, en_body, fields_raw.get(\"keywords\",[]))\n",
    "        row={\n",
    "            \"article_id\":cur,\n",
    "            \"jurisdiction\":fields[\"jurisdiction\"] or \"Global\",\n",
    "            \"sector\":fields[\"sector\"] or \"General\",\n",
    "            \"activity\":fields[\"activity\"] or \"General\",\n",
    "            \"regulatory_theme\":fields[\"regulatory_theme\"] or \"General\",\n",
    "            \"impact_type\":fields[\"impact_type\"] or \"Obligation\",\n",
    "            \"effective_date\":fields[\"effective_date\"],\n",
    "            \"regulator\":fields[\"regulator\"] or \"General Regulator\",\n",
    "            \"keywords\":\", \".join(keywords_list),\n",
    "            \"company_country\":fields[\"company_country\"] or (fields[\"jurisdiction\"] if fields[\"jurisdiction\"]!=\"Global\" else \"Global\"),\n",
    "            \"_source_file\":Path(path).name,\n",
    "            \"_article_marker\":art[\"marker\"],\n",
    "            \"article_text\":en_body\n",
    "        }\n",
    "        row=_post_ensure_english_fields(row)\n",
    "        rows.append(row)\n",
    "        cur+=1\n",
    "    return rows,cur\n",
    "\n",
    "def process_all_documents():\n",
    "    root=_resolve_input_dir()\n",
    "    files=_list_files(root,recursive=RECURSIVE)\n",
    "    if not files:\n",
    "        print(\"[WARN] no inputs\"); return pd.DataFrame()\n",
    "    global_counter=0\n",
    "    all_rows=[]\n",
    "    for f in files:\n",
    "        per_file_rows,global_counter=_process_file(str(f),global_counter)\n",
    "        if not per_file_rows:\n",
    "            raw=_read(str(f))\n",
    "            raw_txt,soup=_html_to_text(raw)\n",
    "            en=_translate(raw_txt)\n",
    "            if not _is_english(en): en=_force_english(en)\n",
    "            title_guess=_extract_main_title(soup, Path(f).stem)\n",
    "            back=_extract_doc_fields(title_guess,en)\n",
    "            for k in [\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\",\"regulator\",\"company_country\",\"jurisdiction\"]:\n",
    "                v=back.get(k,\"\")\n",
    "                if isinstance(v,str):\n",
    "                    back[k]=\", \".join(_translate_list_to_english([w for w in v.split(\",\") if w.strip()]))\n",
    "            dd=_doc_date(raw_txt,soup,str(f))\n",
    "            if _has_cjk(raw_txt) and not _is_english(raw_txt):\n",
    "                tfidf_kws=_ensure_keywords_english(_translate_list_to_english(_keywords_cjk(raw_txt,20)))\n",
    "            else:\n",
    "                tfidf_kws=_ensure_keywords_english(_keywords_en(en,20))\n",
    "            tax=_load_taxonomy()\n",
    "            body=en\n",
    "            def guess(cat):\n",
    "                g=_guess_from_taxonomy(body, tax, cat)\n",
    "                if not g:\n",
    "                    g=_guess_from_rules(body, {\"sector\":SECTOR_RULES,\"activity\":ACTIVITY_RULES,\"regulatory_theme\":THEME_RULES,\"impact_type\":IMPACT_RULES}[cat])\n",
    "                return \", \".join(g[:5]) if g else (\"Obligation\" if cat==\"impact_type\" else \"General\")\n",
    "            per_file_rows=[{\n",
    "                \"article_id\":global_counter,\n",
    "                \"jurisdiction\":back.get(\"jurisdiction\") or _guess_jurisdiction(title_guess,en),\n",
    "                \"sector\":back.get(\"sector\") or guess(\"sector\"),\n",
    "                \"activity\":back.get(\"activity\") or guess(\"activity\"),\n",
    "                \"regulatory_theme\":back.get(\"regulatory_theme\") or guess(\"regulatory_theme\"),\n",
    "                \"impact_type\":back.get(\"impact_type\") or guess(\"impact_type\"),\n",
    "                \"effective_date\":back.get(\"default_effective_date\") or dd or datetime.now(timezone.utc).date().isoformat(),\n",
    "                \"regulator\":back.get(\"regulator\") or \", \".join(_guess_regulators(en)),\n",
    "                \"keywords\":\", \".join(_ensure_min_keywords(_ensure_keywords_english(tfidf_kws), raw_txt, en, k=10)),\n",
    "                \"company_country\":back.get(\"company_country\") or (_guess_jurisdiction(title_guess,en) if _guess_jurisdiction(title_guess,en)!=\"Global\" else \"Global\"),\n",
    "                \"_source_file\":Path(f).name,\n",
    "                \"_article_marker\":\"Document\",\n",
    "                \"article_text\":en\n",
    "            }]\n",
    "            per_file_rows=[_post_ensure_english_fields(r) for r in per_file_rows]\n",
    "            tax=_taxonomy_add_labels(_load_taxonomy(),\"sector\",_normalize_label_list(per_file_rows[0][\"sector\"].split(\",\")))\n",
    "            tax=_taxonomy_add_labels(tax,\"activity\",_normalize_label_list(per_file_rows[0][\"activity\"].split(\",\")))\n",
    "            tax=_taxonomy_add_labels(tax,\"regulatory_theme\",_normalize_label_list(per_file_rows[0][\"regulatory_theme\"].split(\",\")))\n",
    "            tax=_taxonomy_add_labels(tax,\"impact_type\",_normalize_label_list(per_file_rows[0][\"impact_type\"].split(\",\")))\n",
    "            _save_taxonomy(tax)\n",
    "            global_counter+=1\n",
    "        df_file=pd.DataFrame(per_file_rows,columns=[\n",
    "            \"article_id\",\"jurisdiction\",\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\",\n",
    "            \"effective_date\",\"regulator\",\"keywords\",\"company_country\",\n",
    "            \"_source_file\",\"_article_marker\",\"article_text\"\n",
    "        ])\n",
    "        _save_per_input(df_file,Path(f))\n",
    "        print(f\"[FILE] {Path(f).name}: {len(per_file_rows)} rows; next_id={global_counter}\")\n",
    "        all_rows.extend(per_file_rows)\n",
    "    df_all=pd.DataFrame(all_rows,columns=[\n",
    "        \"article_id\",\"jurisdiction\",\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\",\n",
    "        \"effective_date\",\"regulator\",\"keywords\",\"company_country\",\n",
    "        \"_source_file\",\"_article_marker\",\"article_text\"\n",
    "    ])\n",
    "    if not df_all.empty: _save_all(df_all)\n",
    "    print(f\"[TOTAL] {len(df_all)} rows\")\n",
    "    return df_all\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    df=process_all_documents()\n",
    "    print(df.head() if not df.empty else \"No results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "execution_state": "idle",
   "id": "80304389-f7db-4725-899c-c56aae2d18e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:27:53.301507Z",
     "iopub.status.busy": "2025-11-02T01:27:53.301155Z",
     "iopub.status.idle": "2025-11-02T01:29:09.424512Z",
     "shell.execute_reply": "2025-11-02T01:29:09.423859Z",
     "shell.execute_reply.started": "2025-11-02T01:27:53.301483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INPUT] 5 files in directives (recursive=True)\n",
      " - 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL.html\n",
      " - 3.H.R.5376 - Inflation Reduction Act of 2022.xml\n",
      " - 4.REGULATION (EU) 20241689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL.html\n",
      " - 5.中华人民共和国能源法__中国政府网.html\n",
      " - 6.人工知能関連技術の研究開発及び活用の推進に関する法律.html\n",
      "[SAVE] out/1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL.csv\n",
      "[FILE] 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL.html: 1 row\n",
      "[SAVE] out/3.H.R.5376 - Inflation Reduction Act of 2022.csv\n",
      "[FILE] 3.H.R.5376 - Inflation Reduction Act of 2022.xml: 1 row\n",
      "[SAVE] out/4.REGULATION (EU) 20241689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL.csv\n",
      "[FILE] 4.REGULATION (EU) 20241689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL.html: 1 row\n",
      "[SAVE] out/5.中华人民共和国能源法__中国政府网.csv\n",
      "[FILE] 5.中华人民共和国能源法__中国政府网.html: 1 row\n",
      "[SAVE] out/6.人工知能関連技術の研究開発及び活用の推進に関する法律.csv\n",
      "[FILE] 6.人工知能関連技術の研究開発及び活用の推進に関する法律.html: 1 row\n",
      "[SAVE] out/Regulatory_Extraction_ALL.csv\n",
      "[TOTAL] 5 rows\n",
      "  jurisdiction       sector     activity regulatory_theme     impact_type  \\\n",
      "0       Global  [\"general\"]  [\"general\"]      [\"general\"]  [\"obligation\"]   \n",
      "1       Global  [\"general\"]  [\"general\"]      [\"general\"]  [\"obligation\"]   \n",
      "2       Global  [\"general\"]  [\"general\"]      [\"general\"]  [\"obligation\"]   \n",
      "3       Global  [\"general\"]  [\"general\"]      [\"general\"]  [\"obligation\"]   \n",
      "4       Global  [\"general\"]  [\"general\"]      [\"general\"]  [\"obligation\"]   \n",
      "\n",
      "  effective_date              regulator  \\\n",
      "0     2019-12-18  [\"General Regulator\"]   \n",
      "1     2022-08-16  [\"General Regulator\"]   \n",
      "2     2024-12-07  [\"General Regulator\"]   \n",
      "3     2024-11-09  [\"General Regulator\"]   \n",
      "4     2025-11-01  [\"General Regulator\"]   \n",
      "\n",
      "                                            keywords company_country  \n",
      "0  [\"les\", \"des\", \"par\", \"que\", \"professionnel\", ...      [\"Global\"]  \n",
      "1  [\"year\", \"subsection\", \"drug\", \"respect\", \"sec...      [\"Global\"]  \n",
      "2  [\"systems\", \"ai systems\", \"high-risk\", \"high-r...      [\"Global\"]  \n",
      "3  [\"des\", \"les\", \"et la\", \"dans\", \"conseil\", \"du...      [\"Global\"]  \n",
      "4                                                 []      [\"Global\"]  \n"
     ]
    }
   ],
   "source": [
    "import os, re, json, html, string, unicodedata\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, date\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse as parse_date\n",
    "from langdetect import detect, detect_langs, DetectorFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text as sk_text\n",
    "\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "AWS_REGION=os.getenv(\"AWS_REGION\",\"us-west-2\")\n",
    "MODEL_TRANSLATE_FALLBACK_1=os.getenv(\"BEDROCK_TRANSLATE_PRIMARY\",\"amazon.nova-micro-v1:0\")\n",
    "MODEL_TRANSLATE_FALLBACK_2=os.getenv(\"BEDROCK_TRANSLATE_SECONDARY\",\"anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "MODEL_EXTRACT=os.getenv(\"BEDROCK_EXTRACT_MODEL\",\"amazon.nova-premier-v1:0\")\n",
    "MODEL_EXTRACT_DOC=os.getenv(\"BEDROCK_EXTRACT_DOC_MODEL\",\"amazon.nova-premier-v1:0:1000k\")\n",
    "\n",
    "INPUT_DIRS=json.loads(os.getenv(\"INPUT_DIRS\",\"[\\\"./directives\\\"]\"))\n",
    "ALLOWED_EXT=set([x.lower().strip() for x in json.loads(os.getenv(\"ALLOWED_EXT\",\"[\\\".html\\\",\\\".xml\\\"]\"))])\n",
    "RECURSIVE=os.getenv(\"RECURSIVE\",\"true\").strip().lower() in {\"1\",\"true\",\"yes\"}\n",
    "OUT_DIR=os.getenv(\"OUT_DIR\",\"out\")\n",
    "MAX_DOC_CHARS=int(os.getenv(\"MAX_DOC_CHARS\",\"180000\"))\n",
    "MAX_CHUNK_CHARS=int(os.getenv(\"MAX_CHUNK_CHARS\",\"18000\"))\n",
    "\n",
    "translate=boto3.client(\"translate\",region_name=AWS_REGION)\n",
    "bedrock=boto3.client(\"bedrock-runtime\",region_name=AWS_REGION)\n",
    "\n",
    "EXTRA_STOPWORDS={\"section\",\"sections\",\"article\",\"articles\",\"annex\",\"annexe\",\"appendix\",\"appendice\",\"subtitle\",\"title\",\"chapter\",\"chapitre\",\"directive\",\"regulation\",\"regulations\",\"law\",\"act\",\"union\",\"paragraph\",\"subparagraph\",\"recital\",\"dispositif\",\"premier\",\"1er\",\"amended\",\"shall\",\"must\",\"may\",\"including\",\"include\",\"pursuant\",\"accordance\",\"specified\",\"provide\",\"provided\",\"applicable\",\"applicability\",\"applying\",\"applicant\",\"applicants\",\"applying\",\"subject\",\"subjects\",\"thereof\",\"hereof\",\"therein\",\"herein\",\"thereby\",\"hereby\",\"whereas\",\"hereunder\",\"thereunder\",\"among\",\"between\",\"within\",\"without\",\"preamble\",\"scope\",\"purpose\",\"purposes\",\"general\",\"specific\"}\n",
    "STOPWORDS_EN=set(sk_text.ENGLISH_STOP_WORDS)|EXTRA_STOPWORDS\n",
    "\n",
    "STOPWORDS_JA={\"第\",\"条\",\"項\",\"号\",\"章\",\"節\",\"款\",\"目次\",\"附則\",\"総則\",\"抄\",\"同\",\"前\",\"又は\",\"及び\",\"並びに\",\"その他\",\"こと\",\"もの\",\"ため\",\"者\",\"うえ\",\"上\",\"下\",\"について\",\"に関する\",\"に係る\",\"する\",\"される\",\"した\",\"して\",\"すると\",\"され\",\"なる\",\"ない\",\"これ\",\"それ\",\"当該\",\"各\",\"同条\",\"政府\",\"国\",\"内閣\",\"大臣\",\"本部\",\"本部長\",\"本法\",\"本章\",\"本条\",\"次項\",\"前項\",\"人工知能\",\"人工知能関連技術\",\"技術\",\"研究開発\",\"活用\",\"推進\",\"計画\",\"基本\",\"基本計画\",\"施策\",\"規定\",\"規範\",\"方針\",\"必要\",\"措置\",\"整備\",\"確保\",\"促進\",\"国際\",\"協力\",\"教育\",\"人材\",\"情報\",\"データ\",\"等\",\"など\"}\n",
    "STRUCTURAL_LABEL_JA={\"総則\",\"附則\",\"目次\",\"人工知能基本計画\",\"人工知能戦略本部\"}\n",
    "\n",
    "PARLIAMENTARY=re.compile(r\"^\\s*(having regard|after transmission|after consulting|in accordance with|whereas|pursuant to|considering|vu(?:\\s+la|(?:x|es)?)?)\\b.*$\",re.IGNORECASE|re.MULTILINE)\n",
    "RECITALS=re.compile(r\"\\b(whereas|considérant(?:\\s+que)?|vu(?:\\s+la|(?:x|es)?)?)\\b.*?(?=(^|\\n)\\s*(article|art\\.?|dispositif|chapitre|titre)\\s*[^\\n]*\\b(1|premier|1er)\\b|\\bannex|annexe|appendix|appendice|schedule\\b)\",re.IGNORECASE|re.DOTALL)\n",
    "TAIL=re.compile(r\"(done at\\s+[A-Za-z]+\\s+\\d{1,2}\\s+[A-Za-z]+\\s+\\d{4}.*$|for the european parliament.*$|for the council.*$)\",re.IGNORECASE|re.DOTALL)\n",
    "DATE_RX=re.compile(r\"(\\b\\d{1,2}\\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*\\s+\\d{4}\\b|\\b\\d{4}-\\d{1,2}-\\d{1,2}\\b|\\b\\d{1,2}[./]\\d{1,2}[./]\\d{2,4}\\b)\",re.IGNORECASE)\n",
    "CJK_DATE_RX=re.compile(r\"(\\d{4})年(\\d{1,2})月(\\d{1,2})日\")\n",
    "JP_ERA=[(\"令和\",2019),(\"平成\",1989),(\"昭和\",1926),(\"大正\",1912),(\"明治\",1868)]\n",
    "\n",
    "def _resolve_input_dir():\n",
    "    for d in INPUT_DIRS:\n",
    "        p=Path(d)\n",
    "        if p.exists(): return p\n",
    "    p=Path(\"./directives\"); p.mkdir(parents=True,exist_ok=True); return p\n",
    "\n",
    "def _canon_name(p: Path):\n",
    "    stem=p.stem\n",
    "    stem=re.sub(r\"(?i)(^|[-_\\s])(checkpoint|copy|copie|copiar)$\",\"\",stem).strip()\n",
    "    stem=re.sub(r\"(?i)[-_]checkpoint\",\"\",stem)\n",
    "    stem=re.sub(r\"\\s*\\(\\d+\\)$\",\"\",stem)\n",
    "    stem=re.sub(r\"\\s+\",\" \",stem)\n",
    "    return (stem.lower(), p.suffix.lower())\n",
    "\n",
    "def _is_checkpoint(p: Path):\n",
    "    name=p.name.lower()\n",
    "    return (\"-checkpoint\" in name or name.endswith(\"_checkpoint.html\") or \"checkpoint.html\" in name or name.endswith(\".ipynb\") or p.parent.name.lower()==\".ipynb_checkpoints\" or \".ipynb_checkpoints\" in str(p.parent).lower())\n",
    "\n",
    "def _list_files(root,recursive=False):\n",
    "    def ok(p):\n",
    "        n=p.name\n",
    "        if _is_checkpoint(p): return False\n",
    "        if p.parent.name == \".ipynb_checkpoints\": return False\n",
    "        return p.is_file() and p.suffix.lower() in ALLOWED_EXT and not (n.startswith(\".\") or n.startswith(\"._\") or n.endswith(\"~\"))\n",
    "    it=(root.rglob(\"*\") if recursive else root.iterdir())\n",
    "    cand=[p for p in it if ok(p)]\n",
    "    grouped={}\n",
    "    for p in cand:\n",
    "        key=_canon_name(p); best=grouped.get(key)\n",
    "        if best is None: grouped[key]=p\n",
    "        else:\n",
    "            if _is_checkpoint(best) and not _is_checkpoint(p): grouped[key]=p\n",
    "            elif _is_checkpoint(best)==_is_checkpoint(p):\n",
    "                if p.stat().st_mtime>best.stat().st_mtime: grouped[key]=p\n",
    "    files=sorted(grouped.values(),key=lambda x:x.name.lower())\n",
    "    print(f\"[INPUT] {len(files)} files in {root} (recursive={recursive})\")\n",
    "    for p in files: print(\" -\",p.name)\n",
    "    return files\n",
    "\n",
    "def _read(path): return Path(path).read_text(encoding=\"utf-8\",errors=\"ignore\")\n",
    "\n",
    "def _extract_main_container(soup):\n",
    "    for sel in [\"#innerDocument\",\"main#contentsLaw\",\"#docHtml\",\"article\",\"#content\",\"body\"]:\n",
    "        el=soup.select_one(sel)\n",
    "        if el and len(el.get_text(strip=True))>200: return el\n",
    "    return soup\n",
    "\n",
    "def _html_to_text(raw):\n",
    "    try: soup=BeautifulSoup(raw,\"lxml\")\n",
    "    except Exception: soup=BeautifulSoup(raw,\"html.parser\")\n",
    "    main=_extract_main_container(soup)\n",
    "    for tag in main([\"script\",\"style\",\"nav\",\"header\",\"footer\",\"noscript\",\"aside\",\"form\"]):\n",
    "        try: tag.extract()\n",
    "        except Exception: pass\n",
    "    for br in main.find_all([\"br\",\"hr\"]): br.replace_with(\"\\n\")\n",
    "    for li in main.find_all(\"li\"):\n",
    "        txt=li.get_text(\" \",strip=True); li.string=(\"\\n- \"+txt+\"\\n\") if txt else \"\\n\"\n",
    "    for th in main.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"strong\",\"b\"]):\n",
    "        t=th.get_text(\" \",strip=True); th.string=(\"\\n\"+t+\"\\n\") if t else \"\\n\"\n",
    "    text=html.unescape(main.get_text(\"\\n\",strip=True))\n",
    "    text=re.sub(r\"\\n{3,}\",\"\\n\\n\",text)\n",
    "    return text,soup\n",
    "\n",
    "def _extract_main_title(soup, fallback):\n",
    "    try: t=soup.title.get_text(strip=True) if soup and soup.title else \"\"\n",
    "    except Exception: t=\"\"\n",
    "    try: h=soup.find([\"h1\",\"h2\"]); h1=h.get_text(\" \",strip=True) if h else \"\"\n",
    "    except Exception: h1=\"\"\n",
    "    return next((x for x in [h1,t,fallback] if x),fallback)\n",
    "\n",
    "def _has_cjk(s): return bool(re.search(r\"[\\u3400-\\u4dbf\\u4e00-\\u9fff\\u3040-\\u30ff]\",s or \"\"))\n",
    "\n",
    "def _lang_probs(text):\n",
    "    try: return detect_langs(text)\n",
    "    except Exception:\n",
    "        try: return [type(\"LP\",(object,),{\"lang\":detect(text),\"prob\":1.0})()]\n",
    "        except Exception: return []\n",
    "\n",
    "def _english_confidence(text):\n",
    "    t=(text or \"\").strip()\n",
    "    if not t: return 1.0\n",
    "    sample=t[:8000]\n",
    "    letters=[ch for ch in sample if ch.isalpha()]\n",
    "    ascii_letters=[ch for ch in letters if (\"A\"<=ch<=\"Z\") or (\"a\"<=ch<=\"z\")]\n",
    "    ascii_ratio=(len(ascii_letters)/max(1,len(letters))) if letters else 0.0\n",
    "    tokens=[w.strip(string.punctuation).lower() for w in re.split(r\"\\s+\",sample) if w]\n",
    "    stop_hits=sum(1 for tok in tokens if tok in sk_text.ENGLISH_STOP_WORDS)\n",
    "    stop_ratio=stop_hits/max(1,len(tokens))\n",
    "    ld_prob=0.0\n",
    "    for lp in _lang_probs(sample):\n",
    "        if getattr(lp,\"lang\",\"\")==\"en\": ld_prob=max(ld_prob,float(getattr(lp,\"prob\",0.0)))\n",
    "    score=0.75*ld_prob+0.25*(0.6*ascii_ratio+0.4*stop_ratio)\n",
    "    return max(0.0,min(1.0,score))\n",
    "\n",
    "def _is_english(text):\n",
    "    if not text or len(text.strip())==0: return True\n",
    "    if _has_cjk(text):\n",
    "        try: return detect(text)==\"en\"\n",
    "        except Exception: return False\n",
    "    if len(text)<160:\n",
    "        try: return detect(text)==\"en\"\n",
    "        except Exception: return False\n",
    "    return _english_confidence(text)>=0.55\n",
    "\n",
    "def _translate_piece_bedrock(piece,model_id,system_prompt=None):\n",
    "    body={\"max_tokens\":4000,\"temperature\":0.0}\n",
    "    if \"anthropic\" in model_id:\n",
    "        body={\"anthropic_version\":\"bedrock-2023-05-31\",\"max_tokens\":4000,\"temperature\":0.0,\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"Translate this to precise legal English. Keep headings, numbering, dates, entities verbatim. No commentary.\\n\\n\"+piece}]}]}\n",
    "    else:\n",
    "        prompt=\"Translate the following text into precise legal English. Preserve headings, numbering, and dates. No commentary.\\n\\n\"+piece\n",
    "        body={\"inputText\":prompt,\"textGenerationConfig\":{\"maxTokenCount\":4000,\"temperature\":0.0}}\n",
    "    try:\n",
    "        resp=bedrock.invoke_model(modelId=model_id,body=json.dumps(body))\n",
    "        data=json.loads(resp[\"body\"].read())\n",
    "        if \"anthropic\" in model_id: return data.get(\"content\",[{}])[0].get(\"text\",\"\")\n",
    "        return data.get(\"outputText\",\"\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _chunk_for_translate(t,limit=4200):\n",
    "    out=[]; i=0; n=len(t)\n",
    "    seps=[\"\\n\\n\",\"。\\n\",\"。\\n\\n\",\"；\",\"；\\n\",\"；\\n\\n\",\"，\",\"。\\n—\\n\",\"\\n- \"]\n",
    "    while i<n:\n",
    "        j=min(i+limit,n)\n",
    "        k=-1\n",
    "        for sep in seps:\n",
    "            ks=t.rfind(sep,i,j)\n",
    "            if ks>k: k=ks+len(sep)\n",
    "        if k<i+200: k=j\n",
    "        piece=t[i:k].strip(); i=k\n",
    "        if piece: out.append(piece)\n",
    "    return out\n",
    "\n",
    "def _force_english(text):\n",
    "    if not text: return text\n",
    "    if _is_english(text): return text\n",
    "    chunks=_chunk_for_translate(text,limit=4000)\n",
    "    out=[]\n",
    "    for piece in chunks:\n",
    "        ok=False\n",
    "        try:\n",
    "            r=translate.translate_text(Text=piece,SourceLanguageCode=\"auto\",TargetLanguageCode=\"en\")\n",
    "            cand=r.get(\"TranslatedText\",\"\") or \"\"\n",
    "            if cand.strip() and _is_english(cand): out.append(cand); ok=True\n",
    "        except Exception: pass\n",
    "        if not ok:\n",
    "            cand=_translate_piece_bedrock(piece,MODEL_TRANSLATE_FALLBACK_1)\n",
    "            if cand.strip() and _is_english(cand): out.append(cand); ok=True\n",
    "        if not ok:\n",
    "            cand=_translate_piece_bedrock(piece,MODEL_TRANSLATE_FALLBACK_2)\n",
    "            if cand.strip():\n",
    "                out.append(cand if _is_english(cand) else _translate_piece_bedrock(cand,MODEL_TRANSLATE_FALLBACK_1))\n",
    "    final=\"\\n\".join([c for c in out if c]).strip()\n",
    "    return final if final else text\n",
    "\n",
    "def _translate(text):\n",
    "    t=(text or \"\").strip()\n",
    "    if not t: return t\n",
    "    if _is_english(t): return t\n",
    "    out=_force_english(t)\n",
    "    if not _is_english(out): out=_force_english(out)\n",
    "    return out\n",
    "\n",
    "def _prune_operative(text_en):\n",
    "    x=PARLIAMENTARY.sub(\"\",text_en)\n",
    "    x=RECITALS.sub(\"\",x)\n",
    "    x=TAIL.sub(\"\",x)\n",
    "    x=re.sub(r\"\\bTable of Contents\\b.*?(?=(^|\\n)\\s*Article\\s+(1|premier|1er)\\b)\", \"\", x, flags=re.IGNORECASE|re.DOTALL)\n",
    "    return x.strip()\n",
    "\n",
    "def _chunk_iter(text,limit=MAX_CHUNK_CHARS):\n",
    "    i=0; n=len(text)\n",
    "    seps=[\"\\n\\n\",\"\\n- \",\"; \",\". \"]\n",
    "    while i<n:\n",
    "        j=min(i+limit,n)\n",
    "        k=-1\n",
    "        for sep in seps:\n",
    "            ks=text.rfind(sep,i,j)\n",
    "            if ks>k: k=ks+len(sep)\n",
    "        if k<i+200: k=j\n",
    "        piece=text[i:k].strip(); i=k\n",
    "        if piece: yield piece\n",
    "\n",
    "def _to_ymd(s):\n",
    "    try: return parse_date(s,fuzzy=True,dayfirst=False).date().isoformat()\n",
    "    except Exception: return \"\"\n",
    "\n",
    "def _parse_cjk_date(s):\n",
    "    m=CJK_DATE_RX.search(s)\n",
    "    if not m: return \"\"\n",
    "    y=int(m.group(1)); mth=int(m.group(2)); d=int(m.group(3))\n",
    "    try: return date(y,mth,d).isoformat()\n",
    "    except Exception: return \"\"\n",
    "\n",
    "def _parse_jp_era(s):\n",
    "    m=re.search(r\"(令和|平成|昭和|大正|明治)\\s*([元\\d]+)\\s*年\\s*([0-9]{1,2})\\s*月\\s*([0-9]{1,2})\\s*日\",s)\n",
    "    if not m: return \"\"\n",
    "    era=m.group(1); year=m.group(2); month=int(m.group(3)); day=int(m.group(4))\n",
    "    base=dict(JP_ERA)[era]; y=1 if year==\"元\" else int(year)\n",
    "    try: return date(base+y-1,month,day).isoformat()\n",
    "    except Exception: return \"\"\n",
    "\n",
    "def _doc_date(raw_text,soup,fname):\n",
    "    cands=[]\n",
    "    try: cands.extend(el.get_text(\" \",strip=True) for el in soup.select(\"#lawTitleNo,#lawTitle,.oj-hd-date,.oj-doc-ti,.date,.document-date,.pubdate,.issued,.enacted,.approved,#lawTitleNo\"))\n",
    "    except Exception: pass\n",
    "    head=\" \".join(cands)\n",
    "    jp=_parse_jp_era(head) or _parse_jp_era(raw_text[:120000])\n",
    "    if jp: return jp\n",
    "    cjk=_parse_cjk_date(head) or _parse_cjk_date(raw_text[:120000])\n",
    "    if cjk: return cjk\n",
    "    for m in DATE_RX.findall(head):\n",
    "        d=_to_ymd(m[0])\n",
    "        if d: return d\n",
    "    for m in DATE_RX.findall(raw_text[:120000]):\n",
    "        d=_to_ymd(m[0])\n",
    "        if d: return d\n",
    "    m=re.search(r\"(\\d{4})[-_](\\d{1,2})[-_](\\d{1,2})\",Path(fname).name)\n",
    "    if m:\n",
    "        try: return datetime(int(m.group(1)),int(m.group(2)),int(m.group(3))).date().isoformat()\n",
    "        except Exception: pass\n",
    "    m=re.search(r\"\\b(19|20)\\d{2}\\b\",Path(fname).name)\n",
    "    if m:\n",
    "        try: return datetime(int(m.group(0)),6,30).date().isoformat()\n",
    "        except Exception: pass\n",
    "    try:\n",
    "        ts=Path(fname).stat().st_mtime\n",
    "        return datetime.fromtimestamp(ts,tz=timezone.utc).date().isoformat()\n",
    "    except Exception:\n",
    "        return datetime.now(timezone.utc).date().isoformat()\n",
    "\n",
    "def _ensure_ascii_lower(s: str):\n",
    "    s=(s or \"\").strip()\n",
    "    s=unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    s=re.sub(r\"[^A-Za-z0-9%/_\\-\\s]\", \"\", s)\n",
    "    s=re.sub(r\"\\s+\",\" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def _kw_from_text(text_en, max_k=40):\n",
    "    try:\n",
    "        vec=TfidfVectorizer(stop_words=list(STOPWORDS_EN),ngram_range=(1,2),min_df=1,token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z%/_-]+\\b\")\n",
    "        X=vec.fit_transform([text_en]); scores=X.toarray()[0]; terms=vec.get_feature_names_out()\n",
    "        pairs=sorted(zip(terms,scores),key=lambda x:-x[1])\n",
    "        bad={\"article\",\"articles\",\"annex\",\"section\",\"chapter\",\"law\",\"act\",\"directive\",\"regulation\",\"regulations\",\"union\",\"paragraph\",\"recital\",\"subparagraph\",\"subtitle\",\"title\"}\n",
    "        out=[]\n",
    "        for t,_ in pairs:\n",
    "            tl=_ensure_ascii_lower(t)\n",
    "            if tl in bad or tl in STOPWORDS_EN: continue\n",
    "            if re.fullmatch(r\"\\d+(\\.\\d+)?\",tl): continue\n",
    "            if len(tl)<=2: continue\n",
    "            if tl not in out: out.append(tl)\n",
    "            if len(out)>=max_k: break\n",
    "        return out\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _llm_extract_chunk(chunk_en):\n",
    "    prompt=(\"Return a strict JSON object with keys exactly:\\n\"\n",
    "            \"jurisdiction (string), sector (array of strings), activity (array of strings), regulatory_theme (array of strings), impact_type (array of strings), regulator (array of strings), company_country (array of strings), effective_date (YYYY-MM-DD or \\\"\\\").\\n\"\n",
    "            \"Use only this text, be concise, lowercase labels except jurisdictions/regulators/countries (title case), deduplicate.\\n\\nTEXT:\\n\"+chunk_en[:MAX_CHUNK_CHARS])\n",
    "    body={\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":prompt}]}],\"max_tokens\":3000,\"temperature\":0.0}\n",
    "    try:\n",
    "        resp=bedrock.invoke_model(modelId=MODEL_EXTRACT,body=json.dumps(body))\n",
    "        parsed=json.loads(resp[\"body\"].read())\n",
    "        text_out=parsed.get(\"content\",[{}])[0].get(\"text\",\"{}\")\n",
    "        data=json.loads(text_out)\n",
    "        def normlist(v):\n",
    "            if isinstance(v,list): return [str(x).strip() for x in v if str(x).strip()]\n",
    "            if isinstance(v,str) and v.strip(): return [v.strip()]\n",
    "            return []\n",
    "        return {\n",
    "            \"jurisdiction\":str(data.get(\"jurisdiction\",\"\")).strip(),\n",
    "            \"sector\":normlist(data.get(\"sector\",[])),\n",
    "            \"activity\":normlist(data.get(\"activity\",[])),\n",
    "            \"regulatory_theme\":normlist(data.get(\"regulatory_theme\",[])),\n",
    "            \"impact_type\":normlist(data.get(\"impact_type\",[])),\n",
    "            \"regulator\":normlist(data.get(\"regulator\",[])),\n",
    "            \"company_country\":normlist(data.get(\"company_country\",[])),\n",
    "            \"effective_date\":str(data.get(\"effective_date\",\"\")).strip()\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"jurisdiction\":\"\",\"sector\":[],\"activity\":[],\"regulatory_theme\":[],\"impact_type\":[],\"regulator\":[],\"company_country\":[],\"effective_date\":\"\"}\n",
    "\n",
    "def _llm_extract_doc(title, text_en):\n",
    "    prompt=(\"From the following title and document, infer a single JSON object with keys exactly:\\n\"\n",
    "            \"jurisdiction (string), sector (array), activity (array), regulatory_theme (array), impact_type (array), regulator (array), company_country (array), default_effective_date (YYYY-MM-DD or \\\"\\\").\\n\\n\"\n",
    "            \"TITLE:\\n\"+(title or \"\")+\"\\n\\nDOCUMENT:\\n\"+(text_en[:MAX_DOC_CHARS] or \"\"))\n",
    "    body={\"messages\":[{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":prompt}]}],\"max_tokens\":6000,\"temperature\":0.0}\n",
    "    try:\n",
    "        resp=bedrock.invoke_model(modelId=MODEL_EXTRACT_DOC,body=json.dumps(body))\n",
    "        parsed=json.loads(resp[\"body\"].read())\n",
    "        text_out=parsed.get(\"content\",[{}])[0].get(\"text\",\"{}\")\n",
    "        data=json.loads(text_out)\n",
    "        def normlist(v):\n",
    "            if isinstance(v,list): return [str(x).strip() for x in v if str(x).strip()]\n",
    "            if isinstance(v,str) and v.strip(): return [v.strip()]\n",
    "            return []\n",
    "        return {\n",
    "            \"jurisdiction\":str(data.get(\"jurisdiction\",\"\")).strip(),\n",
    "            \"sector\":normlist(data.get(\"sector\",[])),\n",
    "            \"activity\":normlist(data.get(\"activity\",[])),\n",
    "            \"regulatory_theme\":normlist(data.get(\"regulatory_theme\",[])),\n",
    "            \"impact_type\":normlist(data.get(\"impact_type\",[])),\n",
    "            \"regulator\":normlist(data.get(\"regulator\",[])),\n",
    "            \"company_country\":normlist(data.get(\"company_country\",[])),\n",
    "            \"default_effective_date\":str(data.get(\"default_effective_date\",\"\")).strip()\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"jurisdiction\":\"\",\"sector\":[],\"activity\":[],\"regulatory_theme\":[],\"impact_type\":[],\"regulator\":[],\"company_country\":[],\"default_effective_date\":\"\"}\n",
    "\n",
    "def _norm_titlecase(items): return sorted(list(dict.fromkeys([_to_title(x) for x in items if x.strip()])))\n",
    "def _norm_lower(items): return sorted(list(dict.fromkeys([_ensure_ascii_lower(x) for x in items if _ensure_ascii_lower(x)])))\n",
    "\n",
    "def _to_title(s):\n",
    "    s=_ensure_ascii_lower(s)\n",
    "    return \" \".join([w.capitalize() for w in s.split()])\n",
    "\n",
    "def _aggregate_fields(doc_backfill, chunk_fields_list, corpus_text_en, doc_date_guess):\n",
    "    juris=doc_backfill.get(\"jurisdiction\",\"\").strip() or \"\"\n",
    "    sectors=set(doc_backfill.get(\"sector\",[]))\n",
    "    activities=set(doc_backfill.get(\"activity\",[]))\n",
    "    themes=set(doc_backfill.get(\"regulatory_theme\",[]))\n",
    "    impacts=set(doc_backfill.get(\"impact_type\",[]))\n",
    "    regulators=set(doc_backfill.get(\"regulator\",[]))\n",
    "    countries=set(doc_backfill.get(\"company_country\",[]))\n",
    "    eff=doc_backfill.get(\"default_effective_date\",\"\").strip() or \"\"\n",
    "\n",
    "    for cf in chunk_fields_list:\n",
    "        if not juris and cf.get(\"jurisdiction\",\"\"): juris=cf[\"jurisdiction\"]\n",
    "        sectors.update(cf.get(\"sector\",[]))\n",
    "        activities.update(cf.get(\"activity\",[]))\n",
    "        themes.update(cf.get(\"regulatory_theme\",[]))\n",
    "        impacts.update(cf.get(\"impact_type\",[]))\n",
    "        regulators.update(cf.get(\"regulator\",[]))\n",
    "        countries.update(cf.get(\"company_country\",[]))\n",
    "        if not eff and cf.get(\"effective_date\",\"\"): eff=cf[\"effective_date\"]\n",
    "\n",
    "    if not eff:\n",
    "        head=corpus_text_en[:8000]\n",
    "        m=DATE_RX.search(head)\n",
    "        if m: eff=_to_ymd(m[0]) or eff\n",
    "        if not eff: eff=doc_date_guess\n",
    "\n",
    "    juris = _to_title(juris) if juris else \"Global\"\n",
    "    regulators=_norm_titlecase(regulators)\n",
    "    countries=_norm_titlecase(countries) if countries else ([juris] if juris!=\"Global\" else [\"Global\"])\n",
    "    sectors=_norm_lower(sectors) or [\"general\"]\n",
    "    activities=_norm_lower(activities) or [\"general\"]\n",
    "    themes=_norm_lower(themes) or [\"general\"]\n",
    "    impacts=_norm_lower(impacts) or [\"obligation\"]\n",
    "    eff = eff or \"\"\n",
    "\n",
    "    kws=_kw_from_text(corpus_text_en, max_k=50)\n",
    "\n",
    "    return {\n",
    "        \"jurisdiction\": juris,\n",
    "        \"sector\": sectors,\n",
    "        \"activity\": activities,\n",
    "        \"regulatory_theme\": themes,\n",
    "        \"impact_type\": impacts,\n",
    "        \"effective_date\": eff,\n",
    "        \"regulator\": regulators or [\"General Regulator\"],\n",
    "        \"keywords\": kws,\n",
    "        \"company_country\": countries\n",
    "    }\n",
    "\n",
    "def _serialize_row(row):\n",
    "    def J(x): return json.dumps(x, ensure_ascii=False)\n",
    "    return {\n",
    "        \"jurisdiction\": row[\"jurisdiction\"],\n",
    "        \"sector\": J(sorted(row[\"sector\"])),\n",
    "        \"activity\": J(sorted(row[\"activity\"])),\n",
    "        \"regulatory_theme\": J(sorted(row[\"regulatory_theme\"])),\n",
    "        \"impact_type\": J(sorted(row[\"impact_type\"])),\n",
    "        \"effective_date\": row[\"effective_date\"],\n",
    "        \"regulator\": J(sorted(row[\"regulator\"])),\n",
    "        \"keywords\": J(row[\"keywords\"]),\n",
    "        \"company_country\": J(sorted(row[\"company_country\"]))\n",
    "    }\n",
    "\n",
    "def _save_per_input(row,input_path):\n",
    "    Path(OUT_DIR).mkdir(parents=True,exist_ok=True)\n",
    "    stem=input_path.stem\n",
    "    csv_path=f\"{OUT_DIR}/{stem}.csv\"\n",
    "    df=pd.DataFrame([_serialize_row(row)],columns=[\"jurisdiction\",\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\",\"effective_date\",\"regulator\",\"keywords\",\"company_country\"])\n",
    "    df.to_csv(csv_path,index=False)\n",
    "    print(f\"[SAVE] {csv_path}\")\n",
    "\n",
    "def _save_all(rows):\n",
    "    Path(OUT_DIR).mkdir(parents=True,exist_ok=True)\n",
    "    csv_path=f\"{OUT_DIR}/Regulatory_Extraction_ALL.csv\"\n",
    "    df=pd.DataFrame([_serialize_row(r) for r in rows],columns=[\"jurisdiction\",\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\",\"effective_date\",\"regulator\",\"keywords\",\"company_country\"])\n",
    "    df.to_csv(csv_path,index=False)\n",
    "    print(f\"[SAVE] {csv_path}\")\n",
    "\n",
    "def process_file(path: Path):\n",
    "    raw=_read(path)\n",
    "    raw_txt,soup=_html_to_text(raw)\n",
    "    title=_extract_main_title(soup, path.stem)\n",
    "    en=_translate(raw_txt)\n",
    "    if not _is_english(en): en=_force_english(en)\n",
    "    operative=_prune_operative(en)\n",
    "    doc_date_guess=_doc_date(raw_txt,soup,str(path))\n",
    "    back=_llm_extract_doc(title, operative)\n",
    "\n",
    "    chunk_fields=[]\n",
    "    for chunk in _chunk_iter(operative, limit=MAX_CHUNK_CHARS):\n",
    "        c_en=_translate(chunk)\n",
    "        if not _is_english(c_en): c_en=_force_english(c_en)\n",
    "        f=_llm_extract_chunk(c_en)\n",
    "        chunk_fields.append(f)\n",
    "\n",
    "    row=_aggregate_fields(back, chunk_fields, operative, back.get(\"default_effective_date\") or doc_date_guess)\n",
    "    return row\n",
    "\n",
    "def process_all_documents():\n",
    "    root=_resolve_input_dir()\n",
    "    files=_list_files(root,recursive=RECURSIVE)\n",
    "    if not files:\n",
    "        print(\"[WARN] no inputs\"); return pd.DataFrame()\n",
    "    all_rows=[]\n",
    "    for f in files:\n",
    "        row=process_file(f)\n",
    "        _save_per_input(row,f)\n",
    "        all_rows.append(row)\n",
    "        print(f\"[FILE] {Path(f).name}: 1 row\")\n",
    "    _save_all(all_rows)\n",
    "    df=pd.DataFrame([_serialize_row(r) for r in all_rows],columns=[\"jurisdiction\",\"sector\",\"activity\",\"regulatory_theme\",\"impact_type\",\"effective_date\",\"regulator\",\"keywords\",\"company_country\"])\n",
    "    print(f\"[TOTAL] {len(df)} rows\")\n",
    "    return df\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    df=process_all_documents()\n",
    "    print(df.head() if not df.empty else \"No results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1328c-f0db-462b-be8b-ed57fafece82",
   "metadata": {},
   "source": [
    "# Load and translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "execution_state": "idle",
   "id": "a879a226-37c0-4826-aa2b-b74f7a30940c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T01:48:36.616195Z",
     "iopub.status.busy": "2025-11-02T01:48:36.615937Z",
     "iopub.status.idle": "2025-11-02T02:20:54.157594Z",
     "shell.execute_reply": "2025-11-02T02:20:54.156953Z",
     "shell.execute_reply.started": "2025-11-02T01:48:36.616176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files in directives folder\n",
      "Processing: 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL.html\n",
      "  Translating chunk 1/28\n",
      "  Translating chunk 2/28\n",
      "  Translating chunk 3/28\n",
      "  Translating chunk 4/28\n",
      "  Translating chunk 5/28\n",
      "  Translating chunk 6/28\n",
      "  Translating chunk 7/28\n",
      "  Translating chunk 8/28\n",
      "  Translating chunk 9/28\n",
      "  Translating chunk 10/28\n",
      "  Translating chunk 11/28\n",
      "  Translating chunk 12/28\n",
      "  Translating chunk 13/28\n",
      "  Translating chunk 14/28\n",
      "  Translating chunk 15/28\n",
      "  Translating chunk 16/28\n",
      "  Translating chunk 17/28\n",
      "  Translating chunk 18/28\n",
      "  Translating chunk 19/28\n",
      "  Translating chunk 20/28\n",
      "  Translating chunk 21/28\n",
      "  Translating chunk 22/28\n",
      "  Translating chunk 23/28\n",
      "  Translating chunk 24/28\n",
      "  Translating chunk 25/28\n",
      "  Translating chunk 26/28\n",
      "  Translating chunk 27/28\n",
      "  Translating chunk 28/28\n",
      "Saved: 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL.txt\n",
      "Processing: 3.H.R.5376 - Inflation Reduction Act of 2022.xml\n",
      "  Translating chunk 1/200\n",
      "  Translating chunk 2/200\n",
      "  Translating chunk 3/200\n",
      "  Translating chunk 4/200\n",
      "  Translating chunk 5/200\n",
      "  Translating chunk 6/200\n",
      "  Translating chunk 7/200\n",
      "  Translating chunk 8/200\n",
      "  Translating chunk 9/200\n",
      "  Translating chunk 10/200\n",
      "  Translating chunk 11/200\n",
      "  Translating chunk 12/200\n",
      "  Translating chunk 13/200\n",
      "  Translating chunk 14/200\n",
      "  Translating chunk 15/200\n",
      "  Translating chunk 16/200\n",
      "  Translating chunk 17/200\n",
      "  Translating chunk 18/200\n",
      "  Translating chunk 19/200\n",
      "  Translating chunk 20/200\n",
      "  Translating chunk 21/200\n",
      "  Translating chunk 22/200\n",
      "  Translating chunk 23/200\n",
      "  Translating chunk 24/200\n",
      "  Translating chunk 25/200\n",
      "  Translating chunk 26/200\n",
      "  Translating chunk 27/200\n",
      "  Translating chunk 28/200\n",
      "  Translating chunk 29/200\n",
      "  Translating chunk 30/200\n",
      "  Translating chunk 31/200\n",
      "  Translating chunk 32/200\n",
      "  Translating chunk 33/200\n",
      "  Translating chunk 34/200\n",
      "  Translating chunk 35/200\n",
      "  Translating chunk 36/200\n",
      "  Translating chunk 37/200\n",
      "  Translating chunk 38/200\n",
      "  Translating chunk 39/200\n",
      "  Translating chunk 40/200\n",
      "  Translating chunk 41/200\n",
      "  Translating chunk 42/200\n",
      "  Translating chunk 43/200\n",
      "  Translating chunk 44/200\n",
      "  Translating chunk 45/200\n",
      "  Translating chunk 46/200\n",
      "  Translating chunk 47/200\n",
      "  Translating chunk 48/200\n",
      "  Translating chunk 49/200\n",
      "  Translating chunk 50/200\n",
      "  Translating chunk 51/200\n",
      "  Translating chunk 52/200\n",
      "  Translating chunk 53/200\n",
      "  Translating chunk 54/200\n",
      "  Translating chunk 55/200\n",
      "  Translating chunk 56/200\n",
      "  Translating chunk 57/200\n",
      "  Translating chunk 58/200\n",
      "  Translating chunk 59/200\n",
      "  Translating chunk 60/200\n",
      "  Translating chunk 61/200\n",
      "  Translating chunk 62/200\n",
      "  Translating chunk 63/200\n",
      "  Translating chunk 64/200\n",
      "  Translating chunk 65/200\n",
      "  Translating chunk 66/200\n",
      "  Translating chunk 67/200\n",
      "  Translating chunk 68/200\n",
      "  Translating chunk 69/200\n",
      "  Translating chunk 70/200\n",
      "  Translating chunk 71/200\n",
      "  Translating chunk 72/200\n",
      "  Translating chunk 73/200\n",
      "  Translating chunk 74/200\n",
      "  Translating chunk 75/200\n",
      "  Translating chunk 76/200\n",
      "  Translating chunk 77/200\n",
      "  Translating chunk 78/200\n",
      "  Translating chunk 79/200\n",
      "  Translating chunk 80/200\n",
      "  Translating chunk 81/200\n",
      "  Translating chunk 82/200\n",
      "  Translating chunk 83/200\n",
      "  Translating chunk 84/200\n",
      "  Translating chunk 85/200\n",
      "  Translating chunk 86/200\n",
      "  Translating chunk 87/200\n",
      "  Translating chunk 88/200\n",
      "  Translating chunk 89/200\n",
      "  Translating chunk 90/200\n",
      "  Translating chunk 91/200\n",
      "  Translating chunk 92/200\n",
      "  Translating chunk 93/200\n",
      "  Translating chunk 94/200\n",
      "  Translating chunk 95/200\n",
      "  Translating chunk 96/200\n",
      "  Translating chunk 97/200\n",
      "  Translating chunk 98/200\n",
      "  Translating chunk 99/200\n",
      "  Translating chunk 100/200\n",
      "  Translating chunk 101/200\n",
      "  Translating chunk 102/200\n",
      "  Translating chunk 103/200\n",
      "  Translating chunk 104/200\n",
      "  Translating chunk 105/200\n",
      "  Translating chunk 106/200\n",
      "  Translating chunk 107/200\n",
      "  Translating chunk 108/200\n",
      "  Translating chunk 109/200\n",
      "  Translating chunk 110/200\n",
      "  Translating chunk 111/200\n",
      "  Translating chunk 112/200\n",
      "  Translating chunk 113/200\n",
      "  Translating chunk 114/200\n",
      "  Translating chunk 115/200\n",
      "  Translating chunk 116/200\n",
      "  Translating chunk 117/200\n",
      "  Translating chunk 118/200\n",
      "  Translating chunk 119/200\n",
      "  Translating chunk 120/200\n",
      "  Translating chunk 121/200\n",
      "  Translating chunk 122/200\n",
      "  Translating chunk 123/200\n",
      "  Translating chunk 124/200\n",
      "  Translating chunk 125/200\n",
      "  Translating chunk 126/200\n",
      "  Translating chunk 127/200\n",
      "  Translating chunk 128/200\n",
      "  Translating chunk 129/200\n",
      "  Translating chunk 130/200\n",
      "  Translating chunk 131/200\n",
      "  Translating chunk 132/200\n",
      "  Translating chunk 133/200\n",
      "  Translating chunk 134/200\n",
      "  Translating chunk 135/200\n",
      "  Translating chunk 136/200\n",
      "  Translating chunk 137/200\n",
      "  Translating chunk 138/200\n",
      "  Translating chunk 139/200\n",
      "  Translating chunk 140/200\n",
      "  Translating chunk 141/200\n",
      "  Translating chunk 142/200\n",
      "  Translating chunk 143/200\n",
      "  Translating chunk 144/200\n",
      "  Translating chunk 145/200\n",
      "  Translating chunk 146/200\n",
      "  Translating chunk 147/200\n",
      "  Translating chunk 148/200\n",
      "  Translating chunk 149/200\n",
      "  Translating chunk 150/200\n",
      "  Translating chunk 151/200\n",
      "  Translating chunk 152/200\n",
      "  Translating chunk 153/200\n",
      "  Translating chunk 154/200\n",
      "  Translating chunk 155/200\n",
      "  Translating chunk 156/200\n",
      "  Translating chunk 157/200\n",
      "  Translating chunk 158/200\n",
      "  Translating chunk 159/200\n",
      "  Translating chunk 160/200\n",
      "  Translating chunk 161/200\n",
      "  Translating chunk 162/200\n",
      "  Translating chunk 163/200\n",
      "  Translating chunk 164/200\n",
      "  Translating chunk 165/200\n",
      "  Translating chunk 166/200\n",
      "  Translating chunk 167/200\n",
      "  Translating chunk 168/200\n",
      "  Translating chunk 169/200\n",
      "  Translating chunk 170/200\n",
      "  Translating chunk 171/200\n",
      "  Translating chunk 172/200\n",
      "  Translating chunk 173/200\n",
      "  Translating chunk 174/200\n",
      "  Translating chunk 175/200\n",
      "  Translating chunk 176/200\n",
      "  Translating chunk 177/200\n",
      "  Translating chunk 178/200\n",
      "  Translating chunk 179/200\n",
      "  Translating chunk 180/200\n",
      "  Translating chunk 181/200\n",
      "  Translating chunk 182/200\n",
      "  Translating chunk 183/200\n",
      "  Translating chunk 184/200\n",
      "  Translating chunk 185/200\n",
      "  Translating chunk 186/200\n",
      "  Translating chunk 187/200\n",
      "  Translating chunk 188/200\n",
      "  Translating chunk 189/200\n",
      "  Translating chunk 190/200\n",
      "  Translating chunk 191/200\n",
      "  Translating chunk 192/200\n",
      "  Translating chunk 193/200\n",
      "  Translating chunk 194/200\n",
      "  Translating chunk 195/200\n",
      "  Translating chunk 196/200\n",
      "  Translating chunk 197/200\n",
      "  Translating chunk 198/200\n",
      "  Translating chunk 199/200\n",
      "  Translating chunk 200/200\n",
      "Saved: 3.H.R.5376 - Inflation Reduction Act of 2022.txt\n",
      "Processing: 4.REGULATION (EU) 20241689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL.html\n",
      "  Translating chunk 1/158\n",
      "  Translating chunk 2/158\n",
      "  Translating chunk 3/158\n",
      "  Translating chunk 4/158\n",
      "  Translating chunk 5/158\n",
      "  Translating chunk 6/158\n",
      "  Translating chunk 7/158\n",
      "  Translating chunk 8/158\n",
      "  Translating chunk 9/158\n",
      "  Translating chunk 10/158\n",
      "  Translating chunk 11/158\n",
      "  Translating chunk 12/158\n",
      "  Translating chunk 13/158\n",
      "  Translating chunk 14/158\n",
      "  Translating chunk 15/158\n",
      "  Translating chunk 16/158\n",
      "  Translating chunk 17/158\n",
      "  Translating chunk 18/158\n",
      "  Translating chunk 19/158\n",
      "  Translating chunk 20/158\n",
      "  Translating chunk 21/158\n",
      "  Translating chunk 22/158\n",
      "  Translating chunk 23/158\n",
      "  Translating chunk 24/158\n",
      "  Translating chunk 25/158\n",
      "  Translating chunk 26/158\n",
      "  Translating chunk 27/158\n",
      "  Translating chunk 28/158\n",
      "  Translating chunk 29/158\n",
      "  Translating chunk 30/158\n",
      "  Translating chunk 31/158\n",
      "  Translating chunk 32/158\n",
      "  Translating chunk 33/158\n",
      "  Translating chunk 34/158\n",
      "  Translating chunk 35/158\n",
      "  Translating chunk 36/158\n",
      "  Translating chunk 37/158\n",
      "  Translating chunk 38/158\n",
      "  Translating chunk 39/158\n",
      "  Translating chunk 40/158\n",
      "  Translating chunk 41/158\n",
      "  Translating chunk 42/158\n",
      "  Translating chunk 43/158\n",
      "  Translating chunk 44/158\n",
      "  Translating chunk 45/158\n",
      "  Translating chunk 46/158\n",
      "  Translating chunk 47/158\n",
      "  Translating chunk 48/158\n",
      "  Translating chunk 49/158\n",
      "  Translating chunk 50/158\n",
      "  Translating chunk 51/158\n",
      "  Translating chunk 52/158\n",
      "  Translating chunk 53/158\n",
      "  Translating chunk 54/158\n",
      "  Translating chunk 55/158\n",
      "  Translating chunk 56/158\n",
      "  Translating chunk 57/158\n",
      "  Translating chunk 58/158\n",
      "  Translating chunk 59/158\n",
      "  Translating chunk 60/158\n",
      "  Translating chunk 61/158\n",
      "  Translating chunk 62/158\n",
      "  Translating chunk 63/158\n",
      "  Translating chunk 64/158\n",
      "  Translating chunk 65/158\n",
      "  Translating chunk 66/158\n",
      "  Translating chunk 67/158\n",
      "  Translating chunk 68/158\n",
      "  Translating chunk 69/158\n",
      "  Translating chunk 70/158\n",
      "  Translating chunk 71/158\n",
      "  Translating chunk 72/158\n",
      "  Translating chunk 73/158\n",
      "  Translating chunk 74/158\n",
      "  Translating chunk 75/158\n",
      "  Translating chunk 76/158\n",
      "  Translating chunk 77/158\n",
      "  Translating chunk 78/158\n",
      "  Translating chunk 79/158\n",
      "  Translating chunk 80/158\n",
      "  Translating chunk 81/158\n",
      "  Translating chunk 82/158\n",
      "  Translating chunk 83/158\n",
      "  Translating chunk 84/158\n",
      "  Translating chunk 85/158\n",
      "  Translating chunk 86/158\n",
      "  Translating chunk 87/158\n",
      "  Translating chunk 88/158\n",
      "  Translating chunk 89/158\n",
      "  Translating chunk 90/158\n",
      "  Translating chunk 91/158\n",
      "  Translating chunk 92/158\n",
      "  Translating chunk 93/158\n",
      "  Translating chunk 94/158\n",
      "  Translating chunk 95/158\n",
      "  Translating chunk 96/158\n",
      "  Translating chunk 97/158\n",
      "  Translating chunk 98/158\n",
      "  Translating chunk 99/158\n",
      "  Translating chunk 100/158\n",
      "  Translating chunk 101/158\n",
      "  Translating chunk 102/158\n",
      "  Translating chunk 103/158\n",
      "  Translating chunk 104/158\n",
      "  Translating chunk 105/158\n",
      "  Translating chunk 106/158\n",
      "  Translating chunk 107/158\n",
      "  Translating chunk 108/158\n",
      "  Translating chunk 109/158\n",
      "  Translating chunk 110/158\n",
      "  Translating chunk 111/158\n",
      "  Translating chunk 112/158\n",
      "  Translating chunk 113/158\n",
      "  Translating chunk 114/158\n",
      "  Translating chunk 115/158\n",
      "  Translating chunk 116/158\n",
      "  Translating chunk 117/158\n",
      "  Translating chunk 118/158\n",
      "  Translating chunk 119/158\n",
      "  Translating chunk 120/158\n",
      "  Translating chunk 121/158\n",
      "  Translating chunk 122/158\n",
      "  Translating chunk 123/158\n",
      "  Translating chunk 124/158\n",
      "  Translating chunk 125/158\n",
      "  Translating chunk 126/158\n",
      "  Translating chunk 127/158\n",
      "  Translating chunk 128/158\n",
      "  Translating chunk 129/158\n",
      "  Translating chunk 130/158\n",
      "  Translating chunk 131/158\n",
      "  Translating chunk 132/158\n",
      "  Translating chunk 133/158\n",
      "  Translating chunk 134/158\n",
      "  Translating chunk 135/158\n",
      "  Translating chunk 136/158\n",
      "  Translating chunk 137/158\n",
      "  Translating chunk 138/158\n",
      "  Translating chunk 139/158\n",
      "  Translating chunk 140/158\n",
      "  Translating chunk 141/158\n",
      "  Translating chunk 142/158\n",
      "  Translating chunk 143/158\n",
      "  Translating chunk 144/158\n",
      "  Translating chunk 145/158\n",
      "  Translating chunk 146/158\n",
      "  Translating chunk 147/158\n",
      "  Translating chunk 148/158\n",
      "  Translating chunk 149/158\n",
      "  Translating chunk 150/158\n",
      "  Translating chunk 151/158\n",
      "  Translating chunk 152/158\n",
      "  Translating chunk 153/158\n",
      "  Translating chunk 154/158\n",
      "  Translating chunk 155/158\n",
      "  Translating chunk 156/158\n",
      "  Translating chunk 157/158\n",
      "  Translating chunk 158/158\n",
      "Saved: 4.REGULATION (EU) 20241689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL.txt\n",
      "Processing: 5.中华人民共和国能源法__中国政府网.html\n",
      "  Translating chunk 1/10\n",
      "  Translating chunk 2/10\n",
      "  Translating chunk 3/10\n",
      "  Translating chunk 4/10\n",
      "  Translating chunk 5/10\n",
      "  Translating chunk 6/10\n",
      "  Translating chunk 7/10\n",
      "  Translating chunk 8/10\n",
      "  Translating chunk 9/10\n",
      "  Translating chunk 10/10\n",
      "Saved: 5.中华人民共和国能源法__中国政府网.txt\n",
      "Processing: 6.人工知能関連技術の研究開発及び活用の推進に関する法律.html\n",
      "  Translating chunk 1/2\n",
      "  Translating chunk 2/2\n",
      "Saved: 6.人工知能関連技術の研究開発及び活用の推進に関する法律.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "AWS_REGION = \"us-west-2\"\n",
    "OUT_DIR = \"out/translate\"\n",
    "MAX_CHUNK_CHARS = 4000\n",
    "\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION, config=Config(read_timeout=60, retries={\"max_attempts\": 3}))\n",
    "\n",
    "PROFILE_IDS = {\n",
    "    \"anthropic_haiku_4_5\": \"global.anthropic.claude-haiku-4-5-20251001-v1:0\",\n",
    "    \"anthropic_sonnet_4_5\": \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    \"anthropic_sonnet_4\": \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "}\n",
    "\n",
    "def get_files():\n",
    "    directives_path = Path(\"./directives\")\n",
    "    if not directives_path.exists():\n",
    "        print(\"directives folder not found\")\n",
    "        return []\n",
    "    files = [f for f in directives_path.iterdir() if f.is_file() and f.suffix.lower() in {\".html\", \".xml\"}]\n",
    "    return files\n",
    "\n",
    "def html_to_text(raw):\n",
    "    soup = BeautifulSoup(raw, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "        tag.extract()\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "def chunk_text(text, limit=MAX_CHUNK_CHARS):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        end = min(i + limit, len(text))\n",
    "        if end < len(text):\n",
    "            for sep in [\". \", \".\\n\", \"! \", \"? \"]:\n",
    "                last_sep = text.rfind(sep, i, end)\n",
    "                if last_sep > i + 200:\n",
    "                    end = last_sep + len(sep)\n",
    "                    break\n",
    "        chunks.append(text[i:end].strip())\n",
    "        i = end\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "def invoke_anthropic_profile(profile_id, user_text, max_tokens=4000, temperature=0.0):\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_text}]}],\n",
    "    }\n",
    "    resp = bedrock.invoke_model(modelId=profile_id, body=json.dumps(body))\n",
    "    data = json.loads(resp[\"body\"].read())\n",
    "    return (data.get(\"content\", [{}])[0].get(\"text\") or \"\").strip()\n",
    "\n",
    "def translate_chunk(text):\n",
    "    try:\n",
    "        profile_id = PROFILE_IDS[\"anthropic_haiku_4_5\"]\n",
    "        prompt = f\"Translate this text to precise legal English. Preserve headings, numbering, and dates. No commentary.\\n\\n{text}\"\n",
    "        out = invoke_anthropic_profile(profile_id, prompt)\n",
    "        if out:\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        print(f\"Anthropic Haiku failed: {e}\")\n",
    "    try:\n",
    "        profile_id = PROFILE_IDS[\"anthropic_sonnet_4_5\"]\n",
    "        prompt = f\"Translate this text to precise legal English. Preserve headings, numbering, and dates. No commentary.\\n\\n{text}\"\n",
    "        out = invoke_anthropic_profile(profile_id, prompt)\n",
    "        if out:\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        print(f\"Anthropic Sonnet failed: {e}\")\n",
    "    return text\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing: {file_path.name}\")\n",
    "    raw = file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    text = html_to_text(raw)\n",
    "    chunks = chunk_text(text)\n",
    "    translated_chunks = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"  Translating chunk {i+1}/{len(chunks)}\")\n",
    "        translated = translate_chunk(chunk)\n",
    "        translated_chunks.append(translated)\n",
    "    final_text = \"\\n\\n\".join(translated_chunks)\n",
    "    out_dir = Path(OUT_DIR)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = out_dir / f\"{file_path.stem}.txt\"\n",
    "    out_file.write_text(final_text, encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_file.name}\")\n",
    "\n",
    "def main():\n",
    "    files = get_files()\n",
    "    print(f\"Found {len(files)} files in directives folder\")\n",
    "    for file_path in files:\n",
    "        process_file(file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c8f0a-b24f-4043-95b4-c9724c478b5d",
   "metadata": {},
   "source": [
    "# Translation with English recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "e52cb7cd-0191-4359-8d54-afdf4214a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "AWS_REGION = \"us-west-2\"\n",
    "OUT_DIR = \"out/translate\"\n",
    "MAX_CHUNK_CHARS = 4000\n",
    "\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION, config=Config(read_timeout=60, retries={\"max_attempts\": 3}))\n",
    "\n",
    "PROFILE_IDS = {\n",
    "    \"anthropic_haiku_4_5\": \"global.anthropic.claude-haiku-4-5-20251001-v1:0\",\n",
    "    \"anthropic_sonnet_4_5\": \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    \"anthropic_sonnet_4\": \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "}\n",
    "\n",
    "def log(msg):\n",
    "    print(f\"[LOG] {msg}\")\n",
    "\n",
    "def get_files():\n",
    "    p = Path(\"./directives\")\n",
    "    if not p.exists():\n",
    "        log(\"directives folder not found\")\n",
    "        return []\n",
    "    files = [f for f in p.iterdir() if f.is_file() and f.suffix.lower() in {\".html\", \".xml\"}]\n",
    "    log(f\"Detected {len(files)} eligible files in directives folder\")\n",
    "    return files\n",
    "\n",
    "def html_to_text(raw):\n",
    "    soup = BeautifulSoup(raw, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "        tag.extract()\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    log(f\"Extracted {len(text)} characters of text from HTML\")\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, limit=MAX_CHUNK_CHARS):\n",
    "    chunks, i = [], 0\n",
    "    while i < len(text):\n",
    "        end = min(i + limit, len(text))\n",
    "        if end < len(text):\n",
    "            for sep in [\". \", \".\\n\", \"! \", \"? \"]:\n",
    "                k = text.rfind(sep, i, end)\n",
    "                if k > i + 200:\n",
    "                    end = k + len(sep)\n",
    "                    break\n",
    "        chunks.append(text[i:end].strip())\n",
    "        i = end\n",
    "    log(f\"Split text into {len(chunks)} chunks of up to {limit} characters\")\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "def invoke_anthropic_profile(profile_id, user_text, max_tokens=4000, temperature=0.0):\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_text}]}],\n",
    "    }\n",
    "    resp = bedrock.invoke_model(modelId=profile_id, body=json.dumps(body))\n",
    "    data = json.loads(resp[\"body\"].read())\n",
    "    return (data.get(\"content\", [{}])[0].get(\"text\") or \"\").strip()\n",
    "\n",
    "def llm_is_english(sample_text):\n",
    "    log(\"Checking language of text with LLM fallback...\")\n",
    "    q = \"Answer with exactly 'en' if the text is English, otherwise 'non-en'. Text:\\n\\n\" + sample_text[:2000]\n",
    "    try:\n",
    "        out = invoke_anthropic_profile(PROFILE_IDS[\"anthropic_haiku_4_5\"], q, max_tokens=5)\n",
    "        return out.strip().lower().startswith(\"en\")\n",
    "    except Exception as e:\n",
    "        log(f\"LLM language check (Haiku) failed: {e}\")\n",
    "        try:\n",
    "            out = invoke_anthropic_profile(PROFILE_IDS[\"anthropic_sonnet_4_5\"], q, max_tokens=5)\n",
    "            return out.strip().lower().startswith(\"en\")\n",
    "        except Exception as e2:\n",
    "            log(f\"LLM language check (Sonnet) failed: {e2}\")\n",
    "            return False\n",
    "\n",
    "def corpus_is_english(text):\n",
    "    t = (text or \"\").strip()\n",
    "    if not t:\n",
    "        return True\n",
    "    try:\n",
    "        lang = detect(t[:10000])\n",
    "        log(f\"Detected language via langdetect: {lang}\")\n",
    "        if str(lang).lower() == \"en\":\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        log(f\"langdetect failed: {e}\")\n",
    "    first_chunk = chunk_text(t, limit=MAX_CHUNK_CHARS)[:1]\n",
    "    if first_chunk:\n",
    "        result = llm_is_english(first_chunk[0])\n",
    "        log(f\"LLM language detection result: {'English' if result else 'Non-English'}\")\n",
    "        return result\n",
    "    return False\n",
    "\n",
    "def translate_chunk(text):\n",
    "    try:\n",
    "        pid = PROFILE_IDS[\"anthropic_haiku_4_5\"]\n",
    "        prompt = f\"Translate this text to precise legal English. Preserve headings, numbering, and dates. No commentary.\\n\\n{text}\"\n",
    "        out = invoke_anthropic_profile(pid, prompt)\n",
    "        if out:\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        log(f\"Anthropic Haiku translation failed: {e}\")\n",
    "    try:\n",
    "        pid = PROFILE_IDS[\"anthropic_sonnet_4_5\"]\n",
    "        prompt = f\"Translate this text to precise legal English. Preserve headings, numbering, and dates. No commentary.\\n\\n{text}\"\n",
    "        out = invoke_anthropic_profile(pid, prompt)\n",
    "        if out:\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        log(f\"Anthropic Sonnet translation failed: {e}\")\n",
    "    return text\n",
    "\n",
    "def process_file(file_path):\n",
    "    log(f\"Processing: {file_path.name}\")\n",
    "    raw = file_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    text = html_to_text(raw)\n",
    "    if corpus_is_english(text):\n",
    "        log(\"Document is already in English. Skipping translation.\")\n",
    "        final_text = text\n",
    "    else:\n",
    "        log(\"Document is NOT in English. Starting translation process...\")\n",
    "        chunks = chunk_text(text)\n",
    "        translated_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            log(f\"Translating chunk {i+1}/{len(chunks)}\")\n",
    "            translated_chunks.append(translate_chunk(chunk))\n",
    "        final_text = \"\\n\\n\".join(translated_chunks)\n",
    "        log(\"Translation completed successfully.\")\n",
    "    out_dir = Path(OUT_DIR)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = out_dir / f\"{file_path.stem}.txt\"\n",
    "    out_file.write_text(final_text, encoding=\"utf-8\")\n",
    "    log(f\"Saved translated text to {out_file.name}\")\n",
    "\n",
    "def main():\n",
    "    files = get_files()\n",
    "    log(f\"Found {len(files)} files to process.\")\n",
    "    for file_path in files:\n",
    "        process_file(file_path)\n",
    "    log(\"All files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc4ef9-bca3-4ed3-9868-e916490391a8",
   "metadata": {},
   "source": [
    "# Chunk Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "execution_state": "idle",
   "id": "921335db-a7f9-4d4d-b7e7-71935ce20777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T02:28:34.548663Z",
     "iopub.status.busy": "2025-11-02T02:28:34.548391Z",
     "iopub.status.idle": "2025-11-02T02:28:37.059282Z",
     "shell.execute_reply": "2025-11-02T02:28:37.058472Z",
     "shell.execute_reply.started": "2025-11-02T02:28:34.548641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "execution_state": "idle",
   "id": "f66d06ef-7fe5-4dc3-9f71-c7f1b97a8c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T02:28:40.772374Z",
     "iopub.status.busy": "2025-11-02T02:28:40.772089Z",
     "iopub.status.idle": "2025-11-02T02:28:41.964609Z",
     "shell.execute_reply": "2025-11-02T02:28:41.963748Z",
     "shell.execute_reply.started": "2025-11-02T02:28:40.772348Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    tokens = [t for t in tokens if any(ch.isalnum() for ch in t)]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def process_translations(in_dir=\"out/translate\", out_dir=\"out/processed\"):\n",
    "    in_path = Path(in_dir)\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    for f in in_path.glob(\"*.txt\"):\n",
    "        raw = f.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        cleaned = clean_text(raw)\n",
    "        (out_path / f.name).write_text(cleaned, encoding=\"utf-8\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_translations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e7f7cf-a8f3-471b-a88f-4cb9e612b5b4",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "487e53f5-21ea-4772-bbff-9f7c71619d18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T16:28:38.562461Z",
     "iopub.status.busy": "2025-11-02T16:28:38.562183Z",
     "iopub.status.idle": "2025-11-02T17:31:56.347879Z",
     "shell.execute_reply": "2025-11-02T17:31:56.346223Z",
     "shell.execute_reply.started": "2025-11-02T16:28:38.562439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Found 5 files in out/processed\n",
      "[LOG] Start processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL\n",
      "[LOG] Date chosen (header) for 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 2019-11-27\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 1/23 (0.0%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 1/23 (4.3%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 2/23 (4.3%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 2/23 (8.7%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 3/23 (8.7%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 3/23 (13.0%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 4/23 (13.0%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 4/23 (17.4%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 5/23 (17.4%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 5/23 (21.7%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 6/23 (21.7%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 6/23 (26.1%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 7/23 (26.1%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 7/23 (30.4%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 8/23 (30.4%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 8/23 (34.8%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 9/23 (34.8%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 9/23 (39.1%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 10/23 (39.1%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 10/23 (43.5%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 11/23 (43.5%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 11/23 (47.8%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 12/23 (47.8%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 12/23 (52.2%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 13/23 (52.2%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 13/23 (56.5%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 14/23 (56.5%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 14/23 (60.9%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 15/23 (60.9%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 15/23 (65.2%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 16/23 (65.2%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 16/23 (69.6%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 17/23 (69.6%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 17/23 (73.9%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 18/23 (73.9%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 18/23 (78.3%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 19/23 (78.3%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 19/23 (82.6%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 20/23 (82.6%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 20/23 (87.0%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 21/23 (87.0%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 21/23 (91.3%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 22/23 (91.3%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 22/23 (95.7%)\n",
      "[LOG] Processing 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: chunk 23/23 (95.7%)\n",
      "[LOG] Progress 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 23/23 (100.0%)\n",
      "[LOG] Final date for 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL: 2019-11-27\n",
      "[LOG] Completed 1.DIRECTIVE (UE) 20192161 DU PARLEMENT EUROPÉEN ET DU CONSEIL (20.0%)\n",
      "[LOG] Start processing 3.H.R.5376 - Inflation Reduction Act of 2022\n",
      "[LOG] No header date found for 3.H.R.5376 - Inflation Reduction Act of 2022\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 1/177 (0.0%)\n",
      "[LOG] Date chosen (model) for 3.H.R.5376 - Inflation Reduction Act of 2022: 2022-08-16 (conf 0.95)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 1/177 (0.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 2/177 (0.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 2/177 (1.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 3/177 (1.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 3/177 (1.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 4/177 (1.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 4/177 (2.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 5/177 (2.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 5/177 (2.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 6/177 (2.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 6/177 (3.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 7/177 (3.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 7/177 (4.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 8/177 (4.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 8/177 (4.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 9/177 (4.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 9/177 (5.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 10/177 (5.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 10/177 (5.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 11/177 (5.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 11/177 (6.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 12/177 (6.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 12/177 (6.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 13/177 (6.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 13/177 (7.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 14/177 (7.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 14/177 (7.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 15/177 (7.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 15/177 (8.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 16/177 (8.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 16/177 (9.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 17/177 (9.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 17/177 (9.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 18/177 (9.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 18/177 (10.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 19/177 (10.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 19/177 (10.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 20/177 (10.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 20/177 (11.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 21/177 (11.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 21/177 (11.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 22/177 (11.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 22/177 (12.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 23/177 (12.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 23/177 (13.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 24/177 (13.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 24/177 (13.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 25/177 (13.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 25/177 (14.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 26/177 (14.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 26/177 (14.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 27/177 (14.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 27/177 (15.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 28/177 (15.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 28/177 (15.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 29/177 (15.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 29/177 (16.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 30/177 (16.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 30/177 (16.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 31/177 (16.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 31/177 (17.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 32/177 (17.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 32/177 (18.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 33/177 (18.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 33/177 (18.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 34/177 (18.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 34/177 (19.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 35/177 (19.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 35/177 (19.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 36/177 (19.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 36/177 (20.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 37/177 (20.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 37/177 (20.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 38/177 (20.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 38/177 (21.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 39/177 (21.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 39/177 (22.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 40/177 (22.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 40/177 (22.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 41/177 (22.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 41/177 (23.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 42/177 (23.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 42/177 (23.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 43/177 (23.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 43/177 (24.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 44/177 (24.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 44/177 (24.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 45/177 (24.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 45/177 (25.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 46/177 (25.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 46/177 (26.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 47/177 (26.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 47/177 (26.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 48/177 (26.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 48/177 (27.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 49/177 (27.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 49/177 (27.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 50/177 (27.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 50/177 (28.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 51/177 (28.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 51/177 (28.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 52/177 (28.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 52/177 (29.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 53/177 (29.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 53/177 (29.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 54/177 (29.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 54/177 (30.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 55/177 (30.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 55/177 (31.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 56/177 (31.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 56/177 (31.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 57/177 (31.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 57/177 (32.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 58/177 (32.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 58/177 (32.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 59/177 (32.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 59/177 (33.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 60/177 (33.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 60/177 (33.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 61/177 (33.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 61/177 (34.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 62/177 (34.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 62/177 (35.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 63/177 (35.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 63/177 (35.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 64/177 (35.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 64/177 (36.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 65/177 (36.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 65/177 (36.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 66/177 (36.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 66/177 (37.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 67/177 (37.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 67/177 (37.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 68/177 (37.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 68/177 (38.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 69/177 (38.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 69/177 (39.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 70/177 (39.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 70/177 (39.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 71/177 (39.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 71/177 (40.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 72/177 (40.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 72/177 (40.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 73/177 (40.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 73/177 (41.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 74/177 (41.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 74/177 (41.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 75/177 (41.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 75/177 (42.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 76/177 (42.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 76/177 (42.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 77/177 (42.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 77/177 (43.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 78/177 (43.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 78/177 (44.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 79/177 (44.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 79/177 (44.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 80/177 (44.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 80/177 (45.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 81/177 (45.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 81/177 (45.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 82/177 (45.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 82/177 (46.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 83/177 (46.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 83/177 (46.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 84/177 (46.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 84/177 (47.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 85/177 (47.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 85/177 (48.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 86/177 (48.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 86/177 (48.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 87/177 (48.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 87/177 (49.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 88/177 (49.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 88/177 (49.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 89/177 (49.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 89/177 (50.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 90/177 (50.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 90/177 (50.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 91/177 (50.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 91/177 (51.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 92/177 (51.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 92/177 (52.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 93/177 (52.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 93/177 (52.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 94/177 (52.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 94/177 (53.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 95/177 (53.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 95/177 (53.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 96/177 (53.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 96/177 (54.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 97/177 (54.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 97/177 (54.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 98/177 (54.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 98/177 (55.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 99/177 (55.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 99/177 (55.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 100/177 (55.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 100/177 (56.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 101/177 (56.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 101/177 (57.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 102/177 (57.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 102/177 (57.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 103/177 (57.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 103/177 (58.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 104/177 (58.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 104/177 (58.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 105/177 (58.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 105/177 (59.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 106/177 (59.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 106/177 (59.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 107/177 (59.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 107/177 (60.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 108/177 (60.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 108/177 (61.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 109/177 (61.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 109/177 (61.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 110/177 (61.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 110/177 (62.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 111/177 (62.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 111/177 (62.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 112/177 (62.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 112/177 (63.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 113/177 (63.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 113/177 (63.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 114/177 (63.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 114/177 (64.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 115/177 (64.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 115/177 (65.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 116/177 (65.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 116/177 (65.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 117/177 (65.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 117/177 (66.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 118/177 (66.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 118/177 (66.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 119/177 (66.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 119/177 (67.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 120/177 (67.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 120/177 (67.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 121/177 (67.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 121/177 (68.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 122/177 (68.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 122/177 (68.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 123/177 (68.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 123/177 (69.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 124/177 (69.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 124/177 (70.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 125/177 (70.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 125/177 (70.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 126/177 (70.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 126/177 (71.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 127/177 (71.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 127/177 (71.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 128/177 (71.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 128/177 (72.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 129/177 (72.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 129/177 (72.9%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 130/177 (72.9%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 130/177 (73.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 131/177 (73.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 131/177 (74.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 132/177 (74.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 132/177 (74.6%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 133/177 (74.6%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 133/177 (75.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 134/177 (75.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 134/177 (75.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 135/177 (75.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 135/177 (76.3%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 136/177 (76.3%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 136/177 (76.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 137/177 (76.8%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 137/177 (77.4%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 138/177 (77.4%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 138/177 (78.0%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 139/177 (78.0%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 139/177 (78.5%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 140/177 (78.5%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 140/177 (79.1%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 141/177 (79.1%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 141/177 (79.7%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 142/177 (79.7%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 142/177 (80.2%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 143/177 (80.2%)\n",
      "[LOG] Progress 3.H.R.5376 - Inflation Reduction Act of 2022: 143/177 (80.8%)\n",
      "[LOG] Processing 3.H.R.5376 - Inflation Reduction Act of 2022: chunk 144/177 (80.8%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 241\u001b[0m\n\u001b[1;32m    238\u001b[0m         done\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m;log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlaw_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpct(done,total)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    239\u001b[0m     log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll files processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: process_all()\n",
      "Cell \u001b[0;32mIn[1], line 235\u001b[0m, in \u001b[0;36mprocess_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m law_id\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mstem;log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlaw_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m text\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mread_text(encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m,errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m state\u001b[38;5;241m=\u001b[39m\u001b[43mextract_from_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlaw_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m row\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaw_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:law_id,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate}\n\u001b[1;32m    237\u001b[0m write_csv_row(out_dir\u001b[38;5;241m/\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlaw_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,row)\n",
      "Cell \u001b[0;32mIn[1], line 204\u001b[0m, in \u001b[0;36mextract_from_chunks\u001b[0;34m(law_id, text)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate chosen (model) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlaw_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (conf \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m sprompt\u001b[38;5;241m=\u001b[39mbuild_state_prompt(law_id,json\u001b[38;5;241m.\u001b[39mdumps({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m:date_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]},ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),date_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m],date_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevidence_chunk\u001b[39m\u001b[38;5;124m\"\u001b[39m],date_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaw_header\u001b[39m\u001b[38;5;124m\"\u001b[39m],ch)\n\u001b[0;32m--> 204\u001b[0m supd\u001b[38;5;241m=\u001b[39m\u001b[43mcall_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43msprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexpect_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supd,\u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m supd\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m supd\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m!=\u001b[39mdate_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[1], line 151\u001b[0m, in \u001b[0;36mcall_json\u001b[0;34m(prompt, expect_object, max_tokens)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_json\u001b[39m(prompt,expect_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m [PROFILE_IDS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic_haiku_4_5\u001b[39m\u001b[38;5;124m\"\u001b[39m],PROFILE_IDS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic_sonnet_4_5\u001b[39m\u001b[38;5;124m\"\u001b[39m],PROFILE_IDS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic_sonnet_4\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[0;32m--> 151\u001b[0m         out\u001b[38;5;241m=\u001b[39m\u001b[43minvoke_anthropic_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m         obj\u001b[38;5;241m=\u001b[39msafe_load_json(out,expect_object\u001b[38;5;241m=\u001b[39mexpect_object)\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m, in \u001b[0;36minvoke_anthropic_profile\u001b[0;34m(profile_id, user_text, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, [{}])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:601\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m     )\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1056\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1052\u001b[0m     maybe_compress_request(\n\u001b[1;32m   1053\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[1;32m   1055\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m-> 1056\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1062\u001b[0m     http_response\u001b[38;5;241m=\u001b[39mhttp,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1066\u001b[0m )\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1080\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1084\u001b[0m             exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   1085\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1086\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:118\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    113\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m         operation_model,\n\u001b[1;32m    116\u001b[0m         request_dict,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:196\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m    195\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[0;32m--> 196\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[1;32m    200\u001b[0m     attempts,\n\u001b[1;32m    201\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     exception,\n\u001b[1;32m    205\u001b[0m ):\n\u001b[1;32m    206\u001b[0m     attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:238\u001b[0m, in \u001b[0;36mEndpoint._get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, operation_model, context):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# This will return a tuple of (success_response, exception)\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# and success_response is itself a tuple of\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# (http_response, parsed_dict).\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# If an exception occurs then the success_response is None.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# If no exception occurs then exception is None.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     kwargs_to_emit \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_response\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: context,\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m'\u001b[39m: exception,\n\u001b[1;32m    246\u001b[0m     }\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:278\u001b[0m, in \u001b[0;36mEndpoint._do_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    276\u001b[0m     http_response \u001b[38;5;241m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m http_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m         http_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:382\u001b[0m, in \u001b[0;36mEndpoint._send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_send\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/httpsession.py:464\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    461\u001b[0m     conn\u001b[38;5;241m.\u001b[39mproxy_headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m host\n\u001b[1;32m    463\u001b[0m request_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_target(request\u001b[38;5;241m.\u001b[39murl, proxy_url)\n\u001b[0;32m--> 464\u001b[0m urllib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m http_response \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mawsrequest\u001b[38;5;241m.\u001b[39mAWSResponse(\n\u001b[1;32m    477\u001b[0m     request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    478\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    479\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    480\u001b[0m     urllib_response,\n\u001b[1;32m    481\u001b[0m )\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m request\u001b[38;5;241m.\u001b[39mstream_output:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# Cause the raw stream to be exhausted immediately. We do it\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# this way instead of using preload_content because\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# preload_content will never buffer chunked responses\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "application/sagemaker-interactive-debugging": {
       "cell_id": "487e53f5-21ea-4772-bbff-9f7c71619d18",
       "debugging_info_folder": "/home/sagemaker-user/shared/.temp_sagemaker_unified_studio_debugging_info/487e53f5-21ea-4772-bbff-9f7c71619d18",
       "instruction_file": "/home/sagemaker-user/shared/.temp_sagemaker_unified_studio_debugging_info/ipython_debugging_sop.txt",
       "magic_command": "no_magic",
       "session_type": "python_3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json, time, re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "AWS_REGION = \"us-west-2\"\n",
    "IN_DIR = \"out/processed\"\n",
    "OUT_DIR = \"out/results\"\n",
    "MAX_CHUNK_CHARS = 4000\n",
    "\n",
    "PROFILE_IDS = {\n",
    "    \"anthropic_haiku_4_5\": \"global.anthropic.claude-haiku-4-5-20251001-v1:0\",\n",
    "    \"anthropic_sonnet_4_5\": \"global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    \"anthropic_sonnet_4\": \"global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "}\n",
    "\n",
    "bedrock = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION, config=Config(read_timeout=60, retries={\"max_attempts\": 3}))\n",
    "\n",
    "def log(msg): print(f\"[LOG] {msg}\")\n",
    "def pct(n, d): return 0 if d == 0 else round(100 * n / d, 1)\n",
    "\n",
    "def chunk_text(text, limit=MAX_CHUNK_CHARS):\n",
    "    chunks, i = [], 0\n",
    "    while i < len(text):\n",
    "        end = min(i + limit, len(text))\n",
    "        if end < len(text):\n",
    "            for sep in [\". \", \".\\n\", \"! \", \"? \"]:\n",
    "                k = text.rfind(sep, i, end)\n",
    "                if k > i + 200:\n",
    "                    end = k + len(sep)\n",
    "                    break\n",
    "        chunks.append(text[i:end].strip())\n",
    "        i = end\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "def invoke_anthropic_profile(profile_id, user_text, max_tokens=2000, temperature=0.0):\n",
    "    body = {\"anthropic_version\": \"bedrock-2023-05-31\",\"max_tokens\": max_tokens,\"temperature\": temperature,\"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_text}]}]}\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = bedrock.invoke_model(modelId=profile_id, body=json.dumps(body))\n",
    "            data = json.loads(resp[\"body\"].read())\n",
    "            return (data.get(\"content\", [{}])[0].get(\"text\") or \"\").strip()\n",
    "        except Exception as e:\n",
    "            log(f\"LLM call failed (attempt {attempt+1}): {e}\")\n",
    "            time.sleep(1 + attempt)\n",
    "    return \"\"\n",
    "\n",
    "def _normalize_json_text(s):\n",
    "    if not s: return \"\"\n",
    "    s = s.replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\").replace(\"\\u201C\",'\"').replace(\"\\u201D\",'\"').replace(\"\\u00AB\",'\"').replace(\"\\u00BB\",'\"').strip()\n",
    "    s = re.sub(r\"^```(?:json)?\", \"\", s, flags=re.IGNORECASE).strip()\n",
    "    s = re.sub(r\"```$\", \"\", s).strip()\n",
    "    m = re.search(r\"[\\{\\[]\", s)\n",
    "    return s[m.start():] if m else s\n",
    "\n",
    "def _find_balanced_json(s):\n",
    "    s = s.strip()\n",
    "    if not s: return \"\"\n",
    "    start = None\n",
    "    for i,ch in enumerate(s):\n",
    "        if ch in \"{[\":\n",
    "            start = i\n",
    "            break\n",
    "    if start is None: return \"\"\n",
    "    stack=[];in_str=False;esc=False\n",
    "    for j in range(start,len(s)):\n",
    "        ch=s[j]\n",
    "        if ch=='\"' and not esc: in_str=not in_str\n",
    "        esc=(ch=='\\\\' and not esc) if in_str else False\n",
    "        if in_str: continue\n",
    "        if ch in \"{[}\": \n",
    "            if ch in \"{[}\": \n",
    "                if ch in \"{[}\": \n",
    "                    pass\n",
    "        elif ch in \"}]\":\n",
    "            if not stack: return \"\"\n",
    "            top=stack.pop()\n",
    "            if (top==\"{\" and ch!=\"}\") or (top==\"[\" and ch!=\"]\"): return \"\"\n",
    "            if not stack: return s[start:j+1]\n",
    "    return \"\"\n",
    "\n",
    "def safe_load_json(raw, expect_object=True):\n",
    "    if not raw: return None\n",
    "    txt=_normalize_json_text(raw)\n",
    "    try:\n",
    "        obj=json.loads(txt)\n",
    "        if expect_object and not isinstance(obj,dict): return None\n",
    "        return obj\n",
    "    except Exception: pass\n",
    "    seg=_find_balanced_json(txt)\n",
    "    if not seg: return None\n",
    "    try:\n",
    "        obj=json.loads(seg)\n",
    "        if expect_object and not isinstance(obj,dict): return None\n",
    "        return obj\n",
    "    except Exception: return None\n",
    "\n",
    "def empty_state():\n",
    "    return {\"date\":None,\"jurisdiction_country\":[],\"sector\":[],\"activity\":[],\"regulatory_domain\":[],\"impact_type\":[],\"regulator_entity\":[]}\n",
    "\n",
    "def empty_date_info():\n",
    "    return {\"date\":None,\"specificity\":0,\"evidence_chunk\":\"\",\"locked\":False,\"law_header\":\"\"}\n",
    "\n",
    "def merge_state(state,update):\n",
    "    for k in [\"jurisdiction_country\",\"sector\",\"activity\",\"regulatory_domain\",\"impact_type\",\"regulator_entity\"]:\n",
    "        seen=set(state.get(k) or [])\n",
    "        for v in update.get(k,[]) or []:\n",
    "            v=(v or \"\").strip()\n",
    "            if v: seen.add(v)\n",
    "        state[k]=sorted(seen,key=lambda x:x.lower())\n",
    "    return state\n",
    "\n",
    "def build_state_prompt(law_id,prior_state_json,current_date,date_evidence,law_header,chunk_text):\n",
    "    return f\"\"\"Return ONLY a JSON object with keys: \"date\",\"jurisdiction_country\",\"sector\",\"activity\",\"regulatory_domain\",\"impact_type\",\"regulator_entity\".\n",
    "Rules:\n",
    "- law_id is {law_id} and must NOT appear in output.\n",
    "- Do not change \"date\". If you include \"date\", it MUST equal CURRENT_DATE or be a strictly more specific ISO refinement of the SAME year and month.\n",
    "- For list fields, add unique strings supported by this chunk; do not remove prior values.\n",
    "\n",
    "LAW_HEADER:\n",
    "{law_header}\n",
    "\n",
    "CURRENT_STATE:\n",
    "{prior_state_json}\n",
    "\n",
    "CURRENT_DATE_CONTEXT:\n",
    "date: {current_date or \"\"}\n",
    "evidence_chunk: {date_evidence or \"\"}\n",
    "\n",
    "CHUNK:\n",
    "{chunk_text}\"\"\"\n",
    "\n",
    "def build_date_probe_prompt(law_id,law_header,current_date,current_specificity,chunk_text):\n",
    "    return f\"\"\"Return ONLY JSON: {{\"date\":\"\",\"specificity\":0,\"is_stronger\":false,\"same_law\":false,\"confidence\":0.0,\"evidence\":\"\"}}.\n",
    "- Consider ONLY dates that refer to THIS law (not citations to other instruments).\n",
    "- same_law: true only if the chunk clearly ties the date to THIS law identified by law_id and header.\n",
    "- confidence: 0..1 for that judgment.\n",
    "- specificity: 3=YYYY-MM-DD, 2=YYYY-MM, 1=YYYY, 0=unknown.\n",
    "- is_stronger: true only if same_law is true AND the candidate is more specific than CURRENT_DATE and same year.\n",
    "\n",
    "law_id: {law_id}\n",
    "law_header: {law_header}\n",
    "\n",
    "CURRENT_DATE: {current_date or \"\"} (specificity={current_specificity})\n",
    "CHUNK:\n",
    "{chunk_text}\"\"\"\n",
    "\n",
    "def call_json(prompt,expect_object=True,max_tokens=800):\n",
    "    for pid in [PROFILE_IDS[\"anthropic_haiku_4_5\"],PROFILE_IDS[\"anthropic_sonnet_4_5\"],PROFILE_IDS[\"anthropic_sonnet_4\"]]:\n",
    "        out=invoke_anthropic_profile(pid,prompt,max_tokens=max_tokens)\n",
    "        obj=safe_load_json(out,expect_object=expect_object)\n",
    "        if obj is not None: return obj\n",
    "    return None\n",
    "\n",
    "MONTHS={\"january\":1,\"february\":2,\"march\":3,\"april\":4,\"may\":5,\"june\":6,\"july\":7,\"august\":8,\"september\":9,\"october\":10,\"november\":11,\"december\":12}\n",
    "HEADER_RE=re.compile(r\"(DIRECTIVE|REGULATION|DECISION)[^\\n]{0,300}?\\bOF\\b\\s+(\\d{1,2}\\s+[A-Za-z]+\\s+\\d{4})\",re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "def _to_iso_date(s):\n",
    "    s=s.strip()\n",
    "    if re.match(r\"^\\d{4}(-\\d{2}(-\\d{2})?)?$\",s):\n",
    "        y=int(s[:4])\n",
    "        if 1950<=y<=datetime.utcnow().year+1: return s\n",
    "        return \"\"\n",
    "    m=re.match(r\"^(\\d{1,2})\\s+([A-Za-z]+)\\s+(\\d{4})$\",s)\n",
    "    if m:\n",
    "        d=int(m.group(1)); mon=MONTHS.get(m.group(2).lower()); y=int(m.group(3))\n",
    "        if mon and 1<=d<=31 and 1950<=y<=datetime.utcnow().year+1: return f\"{y:04d}-{mon:02d}-{d:02d}\"\n",
    "    return \"\"\n",
    "\n",
    "def _header_date(text):\n",
    "    head=text[:2000].replace(\"\\u00A0\",\" \").replace(\"\\u202F\",\" \")\n",
    "    m=HEADER_RE.search(head)\n",
    "    if not m: return \"\"\n",
    "    return _to_iso_date(m.group(2)) or \"\"\n",
    "\n",
    "def extract_from_chunks(law_id,text):\n",
    "    state=empty_state();date_info=empty_date_info()\n",
    "    hd=_header_date(text)\n",
    "    if hd:\n",
    "        date_info[\"date\"]=hd;date_info[\"specificity\"]=3;date_info[\"evidence_chunk\"]=text[:2000];date_info[\"locked\"]=True\n",
    "        log(f\"Date chosen (header) for {law_id}: {hd}\")\n",
    "    else:\n",
    "        log(f\"No header date found for {law_id}\")\n",
    "    chunks=chunk_text(text)\n",
    "    if chunks: date_info[\"law_header\"]=chunks[0][:1000]\n",
    "    total=len(chunks)\n",
    "    for idx,ch in enumerate(chunks,1):\n",
    "        log(f\"Processing {law_id}: chunk {idx}/{total} ({pct(idx-1,total)}%)\")\n",
    "        if not date_info[\"locked\"]:\n",
    "            dprobe=call_json(build_date_probe_prompt(law_id,date_info[\"law_header\"],date_info[\"date\"],date_info[\"specificity\"],ch),expect_object=True,max_tokens=320)\n",
    "            if isinstance(dprobe,dict):\n",
    "                cand=_to_iso_date((dprobe.get(\"date\") or \"\").strip())\n",
    "                spec=int(dprobe.get(\"specificity\") or 0)\n",
    "                stronger=bool(dprobe.get(\"is_stronger\"));same_law=bool(dprobe.get(\"same_law\"));conf=float(dprobe.get(\"confidence\") or 0.0)\n",
    "                if cand and same_law and conf>=0.8:\n",
    "                    if date_info[\"date\"] and len(cand)==10 and len(date_info[\"date\"])<10 and cand.startswith(date_info[\"date\"][:7]):\n",
    "                        date_info[\"date\"]=cand;date_info[\"specificity\"]=spec;date_info[\"evidence_chunk\"]=ch if len(ch)<=2000 else ch[:2000];date_info[\"locked\"]=True\n",
    "                        log(f\"Date refined (model) for {law_id}: {cand} (conf {conf})\")\n",
    "                    elif not date_info[\"date\"]:\n",
    "                        date_info[\"date\"]=cand;date_info[\"specificity\"]=spec;date_info[\"evidence_chunk\"]=ch if len(ch)<=2000 else ch[:2000];date_info[\"locked\"]=True\n",
    "                        log(f\"Date chosen (model) for {law_id}: {cand} (conf {conf})\")\n",
    "        sprompt=build_state_prompt(law_id,json.dumps({**state,\"date\":date_info[\"date\"]},ensure_ascii=False),date_info[\"date\"],date_info[\"evidence_chunk\"],date_info[\"law_header\"],ch)\n",
    "        supd=call_json(sprompt,expect_object=True,max_tokens=800)\n",
    "        if isinstance(supd,dict):\n",
    "            if supd.get(\"date\") and supd.get(\"date\")!=date_info[\"date\"]:\n",
    "                log(f\"Ignored model date for {law_id}: {supd.get('date')} (kept {date_info['date']})\")\n",
    "            state=merge_state(state,supd)\n",
    "        log(f\"Progress {law_id}: {idx}/{total} ({pct(idx,total)}%)\")\n",
    "    if date_info[\"date\"]:\n",
    "        state[\"date\"]=date_info[\"date\"]\n",
    "        log(f\"Final date for {law_id}: {date_info['date']}\")\n",
    "    else:\n",
    "        log(f\"No date resolved for {law_id}\")\n",
    "    return state\n",
    "\n",
    "def write_csv_row(path,row):\n",
    "    path.parent.mkdir(parents=True,exist_ok=True)\n",
    "    headers=[\"law_id\",\"date\",\"jurisdiction_country\",\"sector\",\"activity\",\"regulatory_domain\",\"impact_type\",\"regulator_entity\"]\n",
    "    def join(v):\n",
    "        if v is None: return \"\"\n",
    "        if isinstance(v,list): return \";\".join(v)\n",
    "        return str(v)\n",
    "    with path.open(\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(headers)+\"\\n\")\n",
    "        f.write(\",\".join([row.get(\"law_id\",\"\"),row.get(\"date\",\"\") or \"\",join(row.get(\"jurisdiction_country\",[])),join(row.get(\"sector\",[])),join(row.get(\"activity\",[])),join(row.get(\"regulatory_domain\",[])),join(row.get(\"impact_type\",[])),join(row.get(\"regulator_entity\",[]))])+\"\\n\")\n",
    "\n",
    "def process_all():\n",
    "    in_dir=Path(IN_DIR);out_dir=Path(OUT_DIR);out_dir.mkdir(parents=True,exist_ok=True)\n",
    "    files=sorted(in_dir.glob(\"*.txt\"));total=len(files);done=0\n",
    "    log(f\"Found {total} files in {IN_DIR}\")\n",
    "    for f in files:\n",
    "        law_id=f.stem;log(f\"Start processing {law_id}\")\n",
    "        text=f.read_text(encoding=\"utf-8\",errors=\"ignore\")\n",
    "        state=extract_from_chunks(law_id,text)\n",
    "        row={\"law_id\":law_id,**state}\n",
    "        write_csv_row(out_dir/f\"{law_id}.csv\",row)\n",
    "        done+=1;log(f\"Completed {law_id} ({pct(done,total)}%)\")\n",
    "    log(\"All files processed.\")\n",
    "\n",
    "if __name__==\"__main__\": process_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9857c7-7bff-41bf-aa2c-5b030530d138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
